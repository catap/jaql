
;//------------------- TEST-CASE -----------------
//-- hdfs write/read expressions --

// write/read Items to SequenceFile: default adapter, default format, no converter
[1,2,3]
-> write({type: 'hdfs', location: 'jaqlTest/test1.dat'});

;//--------------------- RESULT ------------------

{
  "location": "jaqlTest/test1.dat",
  "type": "hdfs"
}

;//------------------- TEST-CASE -----------------


read({type: 'hdfs', location: 'jaqlTest/test1.dat'});

;//--------------------- RESULT ------------------

[
  1,
  2,
  3
]

;//------------------- TEST-CASE -----------------


[1,2,3]
-> write({type: 'hdfs', location: 'jaqlTest/test1alt.dat'})
-> read();

;//--------------------- RESULT ------------------

[
  1,
  2,
  3
]

;//------------------- TEST-CASE -----------------


// write/read Items to SequenceFile: specify FileAdapter, SequenceFileFormat, no converter
[1,2,3]
-> write({location: 'jaqlTest/test2.dat', 
         outoptions: {adapter: 'com.ibm.jaql.io.hadoop.DefaultHadoopOutputAdapter', 
                         format: 'org.apache.hadoop.mapred.SequenceFileOutputFormat',
                         configurator: 'com.ibm.jaql.io.hadoop.FileOutputConfigurator'}});

;//--------------------- RESULT ------------------

{
  "location": "jaqlTest/test2.dat",
  "outoptions": {
    "adapter": "com.ibm.jaql.io.hadoop.DefaultHadoopOutputAdapter",
    "configurator": "com.ibm.jaql.io.hadoop.FileOutputConfigurator",
    "format": "org.apache.hadoop.mapred.SequenceFileOutputFormat"
  }
}

;//------------------- TEST-CASE -----------------


read({location: 'jaqlTest/test2.dat', 
        inoptions: {adapter: 'com.ibm.jaql.io.hadoop.DefaultHadoopInputAdapter', 
                  format: 'org.apache.hadoop.mapred.SequenceFileInputFormat',
                  configurator: 'com.ibm.jaql.io.hadoop.FileInputConfigurator'}});

;//--------------------- RESULT ------------------

[
  1,
  2,
  3
]

;//------------------- TEST-CASE -----------------

                  
[1,2,3]
-> write({location: 'jaqlTest/test2alt.dat', 
                outoptions: {adapter: 'com.ibm.jaql.io.hadoop.DefaultHadoopOutputAdapter', 
                             format: 'org.apache.hadoop.mapred.SequenceFileOutputFormat',
                             configurator: 'com.ibm.jaql.io.hadoop.FileOutputConfigurator'},
                inoptions:  {adapter: 'com.ibm.jaql.io.hadoop.DefaultHadoopInputAdapter', 
                             format: 'org.apache.hadoop.mapred.SequenceFileInputFormat',
                             configurator: 'com.ibm.jaql.io.hadoop.FileInputConfigurator'}})
-> read();

;//--------------------- RESULT ------------------

[
  1,
  2,
  3
]

;//------------------- TEST-CASE -----------------

                        
// write/read Items to Text in a SequenceFile: specify ConverterAdapter, SequenceFileFormat, Item <-> Text (JSON)
// This assumes that the default, FileOutput(Input)Format can be passed a converter.
[{one: '1'}, {two: '2'}, {three: '3'}]
-> write({type: 'hdfs', location: 'jaqlTest/test3.dat', 
         outoptions: {converter: 'com.foobar.store.ToJSONSeqConverter',
                      configurator: 'com.foobar.store.TextFileOutputConfigurator'}});

;//--------------------- RESULT ------------------

{
  "location": "jaqlTest/test3.dat",
  "outoptions": {
    "configurator": "com.foobar.store.TextFileOutputConfigurator",
    "converter": "com.foobar.store.ToJSONSeqConverter"
  },
  "type": "hdfs"
}

;//------------------- TEST-CASE -----------------

              
read({type: 'hdfs', location: 'jaqlTest/test3.dat', 
        inoptions: {converter: 'com.foobar.store.FromJSONSeqConverter'}});

;//--------------------- RESULT ------------------

[
  {
    "one": "1"
  },
  {
    "two": "2"
  },
  {
    "three": "3"
  }
]

;//------------------- TEST-CASE -----------------


[{one: '1'}, {two: '2'}, {three: '3'}]
-> write({type: 'hdfs', location: 'jaqlTest/test3alt.dat', 
               outoptions: {converter: 'com.foobar.store.ToJSONSeqConverter',
                            configurator: 'com.foobar.store.TextFileOutputConfigurator'},
               inoptions: {converter: 'com.foobar.store.FromJSONSeqConverter'}})
-> read();

;//--------------------- RESULT ------------------

[
  {
    "one": "1"
  },
  {
    "two": "2"
  },
  {
    "three": "3"
  }
]

;//------------------- TEST-CASE -----------------


// write/read Items to Text in a TextFile: specify ConverterAdapter, TextFormat, Item <-> Text (JSON)
[{one: '1'}, {two: '2'}, {three: '3'}]
-> write({type: 'hdfs', location: 'jaqlTest/test4.txt', 
         outoptions: {format: 'org.apache.hadoop.mapred.TextOutputFormat',
                      converter: 'com.foobar.store.ToJSONTxtConverter',
                      configurator: 'com.foobar.store.TextFileOutputConfigurator'}});

;//--------------------- RESULT ------------------

{
  "location": "jaqlTest/test4.txt",
  "outoptions": {
    "configurator": "com.foobar.store.TextFileOutputConfigurator",
    "converter": "com.foobar.store.ToJSONTxtConverter",
    "format": "org.apache.hadoop.mapred.TextOutputFormat"
  },
  "type": "hdfs"
}

;//------------------- TEST-CASE -----------------

              
read({type: 'hdfs', location: 'jaqlTest/test4.txt',
	    inoptions: {format: 'org.apache.hadoop.mapred.TextInputFormat',
			  		converter: 'com.foobar.store.FromJSONTxtConverter'}});

;//--------------------- RESULT ------------------

[
  {
    "one": "1"
  },
  {
    "two": "2"
  },
  {
    "three": "3"
  }
]

;//------------------- TEST-CASE -----------------


[{one: '1'}, {two: '2'}, {three: '3'}]
-> write({type: 'hdfs', location: 'jaqlTest/test4alt.txt', 
                outoptions: {format: 'org.apache.hadoop.mapred.TextOutputFormat',
                             converter: 'com.foobar.store.ToJSONTxtConverter',
                             configurator: 'com.foobar.store.TextFileOutputConfigurator'},
                inoptions: {format: 'org.apache.hadoop.mapred.TextInputFormat',
			  		        converter: 'com.foobar.store.FromJSONTxtConverter'}})
-> read();

;//--------------------- RESULT ------------------

[
  {
    "one": "1"
  },
  {
    "two": "2"
  },
  {
    "three": "3"
  }
]

;//------------------- TEST-CASE -----------------


//-- local file read/write expressions

// write/read Items to/from a local file
[1,2,3]
-> write({location: 'mynewfile.txt', 
         outoptions: {adapter : 'com.ibm.jaql.io.stream.FileStreamOutputAdapter',
                      format  : 'com.ibm.jaql.io.stream.converter.JsonOutputStream'}});

;//--------------------- RESULT ------------------

{
  "location": "mynewfile.txt",
  "outoptions": {
    "adapter": "com.ibm.jaql.io.stream.FileStreamOutputAdapter",
    "format": "com.ibm.jaql.io.stream.converter.JsonOutputStream"
  }
}

;//------------------- TEST-CASE -----------------


read({location: 'mynewfile.txt', 
        inoptions: {adapter : 'com.ibm.jaql.io.stream.FileStreamInputAdapter',
                    format  : 'com.ibm.jaql.io.stream.converter.JsonInputStream'}});

;//--------------------- RESULT ------------------

[
  1,
  2,
  3
]

;//------------------- TEST-CASE -----------------


[1,2,3]
-> localWrite({type: 'local', location: 'myotherfile.txt'});

;//--------------------- RESULT ------------------

{
  "location": "myotherfile.txt",
  "type": "local"
}

;//------------------- TEST-CASE -----------------


read({type: 'local', location: 'myotherfile.txt'});

;//--------------------- RESULT ------------------

[
  1,
  2,
  3
]

;//------------------- TEST-CASE -----------------


//-- test map/reduce --

// input/output is hdfs sequence file of Items

// note, 'g' is quoted so that it can be used as a key for subsequent hbase tests
[
  { key: 0, g:"0", text: 'zero' },
  { key: 1, g:"1", text: 'one' },
  { key: 2, g:"0", text: 'two' },
  { key: 3, g:"1", text: 'three' },
  { key: 4, g:"0", text: 'four' },
  { key: 5, g:"1", text: 'five' },
  { key: 6, g:"0", text: 'six' },
  { key: 7, g:"1", text: 'seven' },
  { key: 8, g:"0", text: 'eight' },
]
-> write({ type: 'hdfs', location: 'jaqlTest/test6.dat'});

;//--------------------- RESULT ------------------

{
  "location": "jaqlTest/test6.dat",
  "type": "hdfs"
}

;//------------------- TEST-CASE -----------------


read({type: 'hdfs', location: 'jaqlTest/test6.dat'});

;//--------------------- RESULT ------------------

[
  {
    "g": "0",
    "key": 0,
    "text": "zero"
  },
  {
    "g": "1",
    "key": 1,
    "text": "one"
  },
  {
    "g": "0",
    "key": 2,
    "text": "two"
  },
  {
    "g": "1",
    "key": 3,
    "text": "three"
  },
  {
    "g": "0",
    "key": 4,
    "text": "four"
  },
  {
    "g": "1",
    "key": 5,
    "text": "five"
  },
  {
    "g": "0",
    "key": 6,
    "text": "six"
  },
  {
    "g": "1",
    "key": 7,
    "text": "seven"
  },
  {
    "g": "0",
    "key": 8,
    "text": "eight"
  }
]

;//------------------- TEST-CASE -----------------


// input is a file with JSON text (one record per line), output is a SequenceFile of items
[
  { key: 0, g:0, text: 'zero' },
  { key: 1, g:1, text: 'one' },
  { key: 2, g:0, text: 'two' },
  { key: 3, g:1, text: 'three' },
  { key: 4, g:0, text: 'four' },
  { key: 5, g:1, text: 'five' },
  { key: 6, g:0, text: 'six' },
  { key: 7, g:1, text: 'seven' },
  { key: 8, g:0, text: 'eight' },
]
-> write({type: 'hdfs', location: 'jaqlTest/test8.txt', 
         outoptions: {format: 'org.apache.hadoop.mapred.TextOutputFormat',
                      converter: 'com.foobar.store.ToJSONTxtConverter',
                      configurator: 'com.foobar.store.TextFileOutputConfigurator'}});

;//--------------------- RESULT ------------------

{
  "location": "jaqlTest/test8.txt",
  "outoptions": {
    "configurator": "com.foobar.store.TextFileOutputConfigurator",
    "converter": "com.foobar.store.ToJSONTxtConverter",
    "format": "org.apache.hadoop.mapred.TextOutputFormat"
  },
  "type": "hdfs"
}

;//------------------- TEST-CASE -----------------


read({type: 'hdfs', location: 'jaqlTest/test8.txt',
	    inoptions: {format: 'org.apache.hadoop.mapred.TextInputFormat',
			  		converter: 'com.foobar.store.FromJSONTxtConverter'}});

;//--------------------- RESULT ------------------

[
  {
    "g": 0,
    "key": 0,
    "text": "zero"
  },
  {
    "g": 1,
    "key": 1,
    "text": "one"
  },
  {
    "g": 0,
    "key": 2,
    "text": "two"
  },
  {
    "g": 1,
    "key": 3,
    "text": "three"
  },
  {
    "g": 0,
    "key": 4,
    "text": "four"
  },
  {
    "g": 1,
    "key": 5,
    "text": "five"
  },
  {
    "g": 0,
    "key": 6,
    "text": "six"
  },
  {
    "g": 1,
    "key": 7,
    "text": "seven"
  },
  {
    "g": 0,
    "key": 8,
    "text": "eight"
  }
]

;//------------------- TEST-CASE -----------------

			  		    
mapReduce( {
    'input' : {type: 'hdfs', location: 'jaqlTest/test8.txt', 
               inoptions: {format: 'org.apache.hadoop.mapred.TextInputFormat',
			  		       converter: 'com.foobar.store.FromJSONTxtConverter'}},
    'map'   : fn($) ( $ -> transform [ $.g, 1 ]),
    'reduce': fn(key, values) [{ g: key, n: count(values) }],
    'output': {type: 'hdfs', location: 'jaqlTest/test8.dat'}
    })
-> read();

;//--------------------- RESULT ------------------

[
  {
    "g": 0,
    "n": 5
  },
  {
    "g": 1,
    "n": 4
  }
]

;//------------------- TEST-CASE -----------------


//-- test composite input adapter (see hbaseQueries.txt)

// write out an hdfs file
[
  { key: "0", g:0, text: 'zero' },
  { key: "1", g:1, text: 'one' },
  { key: "2", g:0, text: 'two' },
  { key: "3", g:1, text: 'three' },
  { key: "4", g:0, text: 'four' }
]
-> write({ type: 'hdfs', location: 'jaqlTest/test9.dat'});

;//--------------------- RESULT ------------------

{
  "location": "jaqlTest/test9.dat",
  "type": "hdfs"
}

;//------------------- TEST-CASE -----------------


//-- test higher level interfaces

[1,2,3] -> localWrite(file('test11a.dat'));

;//--------------------- RESULT ------------------

{
  "location": "test11a.dat",
  "type": "local"
}

;//------------------- TEST-CASE -----------------


read(file('test11a.dat'));

;//--------------------- RESULT ------------------

[
  1,
  2,
  3
]

;//------------------- TEST-CASE -----------------


[1,2,3] -> localWrite(file('test11b.dat'));

;//--------------------- RESULT ------------------

{
  "location": "test11b.dat",
  "type": "local"
}

;//------------------- TEST-CASE -----------------


read(file('test11b.dat'));

;//--------------------- RESULT ------------------

[
  1,
  2,
  3
]

;//------------------- TEST-CASE -----------------


[1,2,3] -> write(hdfs('jaqlTest/test12a.dat'));

;//--------------------- RESULT ------------------

{
  "location": "jaqlTest/test12a.dat",
  "type": "hdfs"
}

;//------------------- TEST-CASE -----------------


read(hdfs('jaqlTest/test12a.dat'));

;//--------------------- RESULT ------------------

[
  1,
  2,
  3
]

;//------------------- TEST-CASE -----------------


[1,2,3] -> write(hdfs('jaqlTest/test12b.dat'));

;//--------------------- RESULT ------------------

{
  "location": "jaqlTest/test12b.dat",
  "type": "hdfs"
}

;//------------------- TEST-CASE -----------------


read(hdfs('jaqlTest/test12b.dat'));

;//--------------------- RESULT ------------------

[
  1,
  2,
  3
]

;//------------------- TEST-CASE -----------------


r1 = read(file(DATADIR+"books.json"));
;//------------------- TEST-CASE -----------------


r1 -> write(file("b1"));

;//--------------------- RESULT ------------------

{
  "location": "b1",
  "type": "local"
}

;//------------------- TEST-CASE -----------------


read(file("b1")) == read(file(DATADIR+"books.json"));

;//--------------------- RESULT ------------------

true

;//------------------- TEST-CASE -----------------


r1 -> write(file("b2", {format: "com.ibm.jaql.io.stream.converter.JsonOutputStream"}));

;//--------------------- RESULT ------------------

{
  "location": "b2",
  "options": {
    "format": "com.ibm.jaql.io.stream.converter.JsonOutputStream"
  },
  "type": "local"
}

;//------------------- TEST-CASE -----------------


r2 = read(file("b2", {format: "com.ibm.jaql.io.stream.converter.JsonInputStream"}));
;//------------------- TEST-CASE -----------------


r1 == r2;

;//--------------------- RESULT ------------------

true

;//------------------- TEST-CASE -----------------


r1 -> write(hdfs("jaqlTest/b1"));

;//--------------------- RESULT ------------------

{
  "location": "jaqlTest/b1",
  "type": "hdfs"
}

;//------------------- TEST-CASE -----------------


r1 == read(hdfs("jaqlTest/b1"));

;//--------------------- RESULT ------------------

true

;//------------------- TEST-CASE -----------------


v = {someField: "some value"};
;//------------------- TEST-CASE -----------------


[v] -> write(file("b2", {asArray: false}));

;//--------------------- RESULT ------------------

{
  "location": "b2",
  "options": {
    "asArray": false
  },
  "type": "local"
}

;//------------------- TEST-CASE -----------------


read(file("b2", {asArray: false}))[0] == v;

;//--------------------- RESULT ------------------

true

;//------------------- TEST-CASE -----------------


hdfsShell("-copyFromLocal "+DATADIR+"test.json jaqlTest/test") * 0;

;//--------------------- RESULT ------------------

0

;//------------------- TEST-CASE -----------------


r = read(hdfs("jaqlTest/test", {format: "org.apache.hadoop.mapred.TextInputFormat", converter: "com.ibm.jaql.io.hadoop.converter.FromJsonTextConverter"}));
;//------------------- TEST-CASE -----------------


l = [
  {
    "a": "foo",
    "b": "bar"
  },
  {
    "a": 1,
    "b": 2,
    "c": 3
  }
];
;//------------------- TEST-CASE -----------------


l -> write(hdfs("jaqlTest/test2", {format: "org.apache.hadoop.mapred.TextOutputFormat", converter: "com.ibm.jaql.io.hadoop.converter.ToJsonTextConverter"}));

;//--------------------- RESULT ------------------

{
  "location": "jaqlTest/test2",
  "options": {
    "converter": "com.ibm.jaql.io.hadoop.converter.ToJsonTextConverter",
    "format": "org.apache.hadoop.mapred.TextOutputFormat"
  },
  "type": "hdfs"
}

;//------------------- TEST-CASE -----------------


r -> write(hdfs("jaqlTest/test2", {format: "org.apache.hadoop.mapred.TextOutputFormat", converter: "com.ibm.jaql.io.hadoop.converter.ToJsonTextConverter"}));

;//--------------------- RESULT ------------------

{
  "location": "jaqlTest/test2",
  "options": {
    "converter": "com.ibm.jaql.io.hadoop.converter.ToJsonTextConverter",
    "format": "org.apache.hadoop.mapred.TextOutputFormat"
  },
  "type": "hdfs"
}

;//------------------- TEST-CASE -----------------


r2 = read(hdfs("jaqlTest/test2", {format: "org.apache.hadoop.mapred.TextInputFormat", converter: "com.ibm.jaql.io.hadoop.converter.FromJsonTextConverter"}));
;//------------------- TEST-CASE -----------------


(r -> sort by [$]) == (r2 -> sort by [$]);

;//--------------------- RESULT ------------------

true

;//------------------- TEST-CASE -----------------


//artist = "The Police";
//url = "http://api.freebase.com/api/service/mqlread";
//query = {query: {album: [], name: artist, type: "/music/artist"}};
//httpGet(url, {query: serialize(query)}, {asArray: false})[0].result.album;
//read(http(url, {query: serialize(query)}))[0].result.album;

//-- test array read
read({type: 'array', inoptions: {array: [1,2,3]}});

;//--------------------- RESULT ------------------

[
  1,
  2,
  3
]

;//------------------- TEST-CASE -----------------


arrayRead([1,2,3]);

;//--------------------- RESULT ------------------

[
  1,
  2,
  3
]

;//------------------- TEST-CASE -----------------


mapReduce( {
    'input': {type: 'array', inoptions: {array: [1,2,3]}},
    'map'   : fn($) ( $ -> transform [ null, $ ] ),
    'output': {type: 'hdfs', location: 'jaqlTest/test14out.dat'}
  })
-> read() -> sort by [$];

;//--------------------- RESULT ------------------

[
  1,
  2,
  3
]

;//------------------- TEST-CASE -----------------


mapReduce( {
    'input': [ {type: 'array', inoptions: {array: [1,2,3]}},
               {type: 'array', inoptions: {array: [1,3,5]}} ],
    'map'   : [ fn($) ($ -> transform [ null, $ + 10 ]),
                fn($) ($ -> transform [ null, $ + 20 ]) ],
    'output': {type: 'hdfs', location: 'jaqlTest/test14out.dat'}
  })
-> read() -> sort by [$];

;//--------------------- RESULT ------------------

[
  11,
  12,
  13,
  21,
  23,
  25
]

;//------------------- TEST-CASE -----------------


//-- test rewrites --

// group by over hdfs read
read({type: 'hdfs', location: 'jaqlTest/test6.dat'})
-> group by a = ($.g)
    into { g:a, i:count($) }
-> sort by [$.g];

;//--------------------- RESULT ------------------

[
  {
    "g": "0",
    "i": 5
  },
  {
    "g": "1",
    "i": 4
  }
]

;//------------------- TEST-CASE -----------------


// for loop over hdfs read
read({type: 'hdfs', location: 'jaqlTest/test6.dat'})
-> transform $.key;

;//--------------------- RESULT ------------------

[
  0,
  1,
  2,
  3,
  4,
  5,
  6,
  7,
  8
]

;//------------------- TEST-CASE -----------------


;//------------------- TEST-DONE -----------------
