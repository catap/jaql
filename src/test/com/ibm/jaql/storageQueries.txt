//-- hdfs write/read expressions --

// write/read Items to SequenceFile: default adapter, default format, no converter
[1,2,3]
-> write({type: 'hdfs', location: 'jaqlTest/test1.dat'});

read({type: 'hdfs', location: 'jaqlTest/test1.dat'});

[1,2,3]
-> write({type: 'hdfs', location: 'jaqlTest/test1alt.dat'})
-> read();

// write/read Items to SequenceFile: specify FileAdapter, SequenceFileFormat, no converter
[1,2,3]
-> write({location: 'jaqlTest/test2.dat', 
         outoptions: {adapter: 'com.ibm.jaql.io.hadoop.DefaultHadoopOutputAdapter', 
                         format: 'org.apache.hadoop.mapred.SequenceFileOutputFormat',
                         configurator: 'com.ibm.jaql.io.hadoop.FileOutputConfigurator'}});

read({location: 'jaqlTest/test2.dat', 
        inoptions: {adapter: 'com.ibm.jaql.io.hadoop.DefaultHadoopInputAdapter', 
                  format: 'org.apache.hadoop.mapred.SequenceFileInputFormat',
                  configurator: 'com.ibm.jaql.io.hadoop.FileInputConfigurator'}});
                  
[1,2,3]
-> write({location: 'jaqlTest/test2alt.dat', 
                outoptions: {adapter: 'com.ibm.jaql.io.hadoop.DefaultHadoopOutputAdapter', 
                             format: 'org.apache.hadoop.mapred.SequenceFileOutputFormat',
                             configurator: 'com.ibm.jaql.io.hadoop.FileOutputConfigurator'},
                inoptions:  {adapter: 'com.ibm.jaql.io.hadoop.DefaultHadoopInputAdapter', 
                             format: 'org.apache.hadoop.mapred.SequenceFileInputFormat',
                             configurator: 'com.ibm.jaql.io.hadoop.FileInputConfigurator'}})
-> read();
                        
// write/read Items to Text in a SequenceFile: specify ConverterAdapter, SequenceFileFormat, Item <-> Text (JSON)
// This assumes that the default, FileOutput(Input)Format can be passed a converter.
[{one: '1'}, {two: '2'}, {three: '3'}]
-> write({type: 'hdfs', location: 'jaqlTest/test3.dat', 
         outoptions: {converter: 'com.foobar.store.ToJSONSeqConverter',
                      configurator: 'com.foobar.store.TextFileOutputConfigurator'}});
              
read({type: 'hdfs', location: 'jaqlTest/test3.dat', 
        inoptions: {converter: 'com.foobar.store.FromJSONSeqConverter'}});

[{one: '1'}, {two: '2'}, {three: '3'}]
-> write({type: 'hdfs', location: 'jaqlTest/test3alt.dat', 
               outoptions: {converter: 'com.foobar.store.ToJSONSeqConverter',
                            configurator: 'com.foobar.store.TextFileOutputConfigurator'},
               inoptions: {converter: 'com.foobar.store.FromJSONSeqConverter'}})
-> read();

// write/read Items to Text in a TextFile: specify ConverterAdapter, TextFormat, Item <-> Text (JSON)
[{one: '1'}, {two: '2'}, {three: '3'}]
-> write({type: 'hdfs', location: 'jaqlTest/test4.txt', 
         outoptions: {format: 'org.apache.hadoop.mapred.TextOutputFormat',
                      converter: 'com.foobar.store.ToJSONTxtConverter',
                      configurator: 'com.foobar.store.TextFileOutputConfigurator'}});
              
read({type: 'hdfs', location: 'jaqlTest/test4.txt',
	    inoptions: {format: 'org.apache.hadoop.mapred.TextInputFormat',
			  		converter: 'com.foobar.store.FromJSONTxtConverter'}});

[{one: '1'}, {two: '2'}, {three: '3'}]
-> write({type: 'hdfs', location: 'jaqlTest/test4alt.txt', 
                outoptions: {format: 'org.apache.hadoop.mapred.TextOutputFormat',
                             converter: 'com.foobar.store.ToJSONTxtConverter',
                             configurator: 'com.foobar.store.TextFileOutputConfigurator'},
                inoptions: {format: 'org.apache.hadoop.mapred.TextInputFormat',
			  		        converter: 'com.foobar.store.FromJSONTxtConverter'}})
-> read();

//-- local file read/write expressions

// write/read Items to/from a local file
[1,2,3]
-> write({location: 'mynewfile.txt', 
         outoptions: {adapter : 'com.ibm.jaql.io.stream.FileStreamOutputAdapter',
                      format  : 'com.ibm.jaql.io.stream.converter.JsonOutputStream'}});

read({location: 'mynewfile.txt', 
        inoptions: {adapter : 'com.ibm.jaql.io.stream.FileStreamInputAdapter',
                    format  : 'com.ibm.jaql.io.stream.converter.JsonInputStream'}});

[1,2,3]
-> localWrite({type: 'local', location: 'myotherfile.txt'});

read({type: 'local', location: 'myotherfile.txt'});

//-- test map/reduce --

// input/output is hdfs sequence file of Items

// note, 'g' is quoted so that it can be used as a key for subsequent hbase tests
[
  { key: 0, g:"0", text: 'zero' },
  { key: 1, g:"1", text: 'one' },
  { key: 2, g:"0", text: 'two' },
  { key: 3, g:"1", text: 'three' },
  { key: 4, g:"0", text: 'four' },
  { key: 5, g:"1", text: 'five' },
  { key: 6, g:"0", text: 'six' },
  { key: 7, g:"1", text: 'seven' },
  { key: 8, g:"0", text: 'eight' },
]
-> write({ type: 'hdfs', location: 'jaqlTest/test6.dat'});

read({type: 'hdfs', location: 'jaqlTest/test6.dat'});

// input is a file with JSON text (one record per line), output is a SequenceFile of items
[
  { key: 0, g:0, text: 'zero' },
  { key: 1, g:1, text: 'one' },
  { key: 2, g:0, text: 'two' },
  { key: 3, g:1, text: 'three' },
  { key: 4, g:0, text: 'four' },
  { key: 5, g:1, text: 'five' },
  { key: 6, g:0, text: 'six' },
  { key: 7, g:1, text: 'seven' },
  { key: 8, g:0, text: 'eight' },
]
-> write({type: 'hdfs', location: 'jaqlTest/test8.txt', 
         outoptions: {format: 'org.apache.hadoop.mapred.TextOutputFormat',
                      converter: 'com.foobar.store.ToJSONTxtConverter',
                      configurator: 'com.foobar.store.TextFileOutputConfigurator'}});

read({type: 'hdfs', location: 'jaqlTest/test8.txt',
	    inoptions: {format: 'org.apache.hadoop.mapred.TextInputFormat',
			  		converter: 'com.foobar.store.FromJSONTxtConverter'}});
			  		    
mapReduce( {
    'input' : {type: 'hdfs', location: 'jaqlTest/test8.txt', 
               inoptions: {format: 'org.apache.hadoop.mapred.TextInputFormat',
			  		       converter: 'com.foobar.store.FromJSONTxtConverter'}},
    'map'   : fn($) ( $ -> transform [ $.g, 1 ]),
    'reduce': fn(key, values) [{ g: key, n: count(values) }],
    'output': {type: 'hdfs', location: 'jaqlTest/test8.dat'}
    })
-> read();

//-- test composite input adapter (see hbaseQueries.txt)

// write out an hdfs file
[
  { key: "0", g:0, text: 'zero' },
  { key: "1", g:1, text: 'one' },
  { key: "2", g:0, text: 'two' },
  { key: "3", g:1, text: 'three' },
  { key: "4", g:0, text: 'four' }
]
-> write({ type: 'hdfs', location: 'jaqlTest/test9.dat'});

//-- test higher level interfaces

[1,2,3] -> localWrite(file('test11a.dat'));

read(file('test11a.dat'));

[1,2,3] -> localWrite(file('test11b.dat'));

read(file('test11b.dat'));

[1,2,3] -> write(hdfs('jaqlTest/test12a.dat'));

read(hdfs('jaqlTest/test12a.dat'));

[1,2,3] -> write(hdfs('jaqlTest/test12b.dat'));

read(hdfs('jaqlTest/test12b.dat'));

r1 = read(file(DATADIR+"books.json"));

r1 -> write(file("b1"));

read(file("b1")) == read(file(DATADIR+"books.json"));

r1 -> write(file("b2", {format: "com.ibm.jaql.io.stream.converter.JsonOutputStream"}));

r2 = read(file("b2", {format: "com.ibm.jaql.io.stream.converter.JsonInputStream"}));

r1 == r2;

r1 -> write(hdfs("jaqlTest/b1"));

r1 == read(hdfs("jaqlTest/b1"));

v = {someField: "some value"};

[v] -> write(file("b2", {asArray: false}));

read(file("b2", {asArray: false}))[0] == v;

hdfsShell("-copyFromLocal "+DATADIR+"test.json jaqlTest/test") * 0;

r = read(hdfs("jaqlTest/test", {format: "org.apache.hadoop.mapred.TextInputFormat", converter: "com.ibm.jaql.io.hadoop.converter.FromJsonTextConverter"}));

l = [
  {
    "a": "foo",
    "b": "bar"
  },
  {
    "a": 1,
    "b": 2,
    "c": 3
  }
];

l -> write(hdfs("jaqlTest/test2", {format: "org.apache.hadoop.mapred.TextOutputFormat", converter: "com.ibm.jaql.io.hadoop.converter.ToJsonTextConverter"}));

r -> write(hdfs("jaqlTest/test2", {format: "org.apache.hadoop.mapred.TextOutputFormat", converter: "com.ibm.jaql.io.hadoop.converter.ToJsonTextConverter"}));

r2 = read(hdfs("jaqlTest/test2", {format: "org.apache.hadoop.mapred.TextInputFormat", converter: "com.ibm.jaql.io.hadoop.converter.FromJsonTextConverter"}));

(r -> sort by [$]) == (r2 -> sort by [$]);

//artist = "The Police";
//url = "http://api.freebase.com/api/service/mqlread";
//query = {query: {album: [], name: artist, type: "/music/artist"}};
//httpGet(url, {query: serialize(query)}, {asArray: false})[0].result.album;
//read(http(url, {query: serialize(query)}))[0].result.album;

//-- test array read
read({type: 'array', inoptions: {array: [1,2,3]}});

arrayRead([1,2,3]);

mapReduce( {
    'input': {type: 'array', inoptions: {array: [1,2,3]}},
    'map'   : fn($) ( $ -> transform [ null, $ ] ),
    'output': {type: 'hdfs', location: 'jaqlTest/test14out.dat'}
  })
-> read() -> sort by [$];

mapReduce( {
    'input': [ {type: 'array', inoptions: {array: [1,2,3]}},
               {type: 'array', inoptions: {array: [1,3,5]}} ],
    'map'   : [ fn($) ($ -> transform [ null, $ + 10 ]),
                fn($) ($ -> transform [ null, $ + 20 ]) ],
    'output': {type: 'hdfs', location: 'jaqlTest/test14out.dat'}
  })
-> read() -> sort by [$];

//-- test rewrites --

// group by over hdfs read
read({type: 'hdfs', location: 'jaqlTest/test6.dat'})
-> group by a = ($.g)
    into { g:a, i:count($) }
-> sort by [$.g];

// for loop over hdfs read
read({type: 'hdfs', location: 'jaqlTest/test6.dat'})
-> transform $.key;

// test out sequence file with Text value, using non-test classes
read(hdfs('jaqlTest/test1.dat')) 
-> transform $ + 1 
-> write(hdfs('jaqlTest/test15out.dat', 
              {converter: 'com.ibm.jaql.io.hadoop.converter.ToJsonTextConverter', 
               configurator: 'com.ibm.jaql.io.hadoop.TextFileOutputConfigurator'}));
               
read(hdfs('jaqlTest/test15out.dat', { converter: 'com.ibm.jaql.io.hadoop.converter.FromJsonTextConverter' })); 