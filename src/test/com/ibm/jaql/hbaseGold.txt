// the data
$data = [
  { key: "0", g:0, text: 'zero' },
  { key: "1", g:0, text: 'one' },
  { key: "2", g:0, text: 'two' },
  { key: "3", text: 'three' },
  { key: "4", g:0, text: 'four' },
  { key: "5", g:1, text: 'five' },
  { key: "6", g:0, text: 'six' },
  { key: "7", g:1, text: 'seven' },
  { key: "8", g:0, text: 'eight' }
];##
"$data"


// write it out
$data -> hbaseWrite('test1');##
{
  "location": "test1",
  "type": "hbase"
}


// read it in
hbaseRead('test1');##
[
  {
    "g": 0,
    "key": "0",
    "text": "zero"
  },
  {
    "g": 0,
    "key": "1",
    "text": "one"
  },
  {
    "g": 0,
    "key": "2",
    "text": "two"
  },
  {
    "key": "3",
    "text": "three"
  },
  {
    "g": 0,
    "key": "4",
    "text": "four"
  },
  {
    "g": 1,
    "key": "5",
    "text": "five"
  },
  {
    "g": 0,
    "key": "6",
    "text": "six"
  },
  {
    "g": 1,
    "key": "7",
    "text": "seven"
  },
  {
    "g": 0,
    "key": "8",
    "text": "eight"
  }
]


// fetch a value from one record
hbaseFetch('test1', ['5'], ['text']);##
[
  {
    "key": "5",
    "text": "five"
  }
]


// fetch multiple values from one record
hbaseFetch('test1', ['5'], ['text', 'g']);##
[
  {
    "g": 1,
    "key": "5",
    "text": "five"
  }
]


// fetch entire record
hbaseFetch('test1', ['5']);##
[
  {
    "g": 1,
    "key": "5",
    "text": "five"
  }
]


// fetch a value from multiple records
hbaseFetch('test1', ['3', '7'], ['text']);##
[
  {
    "key": "3",
    "text": "three"
  },
  {
    "key": "7",
    "text": "seven"
  }
]


// fetch multiple values from multiple records
hbaseFetch('test1', ['3', '7'], ['text', 'g']);##
[
  {
    "key": "3",
    "text": "three"
  },
  {
    "g": 1,
    "key": "7",
    "text": "seven"
  }
]


// fetch multiple entire records
hbaseFetch('test1', ['3', '7']);##
[
  {
    "key": "3",
    "text": "three"
  },
  {
    "g": 1,
    "key": "7",
    "text": "seven"
  }
]


// delete a value from one record 
hbaseDelete('test1', ['5'], ['g']);##
true


// fetch the deleted value
hbaseFetch('test1', ['5'], ['g']);##
[]


// fetch a record with a deleted value
hbaseFetch('test1', ['5']);##
[
  {
    "key": "5",
    "text": "five"
  }
]


// delete a value from multiple records
hbaseDelete('test1', ['2', '8'], ['text']);##
true


// fetch a deleted value
hbaseFetch('test1', ['2'], ['text']);##
[]


// fetch a record with a deleted value
hbaseFetch('test1', ['2']);##
[
  {
    "g": 0,
    "key": "2"
  }
]


// Example 3. Write to an HBase table named 'webcrawl'. (from exampleQueries)
[
    {key: "www.cnn.com", page:'...', rank: 0.9,
     inlinks:[
       {link: 'www.news.com', anchor: 'newsite'},
       {link: 'www.jscript.com', anchor: 'look here'}]},
    {key: "www.json.org", page:'...', rank: 0.8}
]
-> hbaseWrite('webcrawl');##
{
  "location": "webcrawl",
  "type": "hbase"
}

  
// Read it back... (from exampleQueries)
hbaseRead('webcrawl');##
[
  {
    "inlinks": [
      {
        "anchor": "newsite",
        "link": "www.news.com"
      },
      {
        "anchor": "look here",
        "link": "www.jscript.com"
      }
    ],
    "key": "www.cnn.com",
    "page": "...",
    "rank": 0.9
  },
  {
    "key": "www.json.org",
    "page": "...",
    "rank": 0.8
  }
]


// the data (from exampleQueries)

$books = [
    {publisher: 'Scholastic',
     author: 'J. K. Rowling',
     title: 'Deathly Hallows',
     year: 2007},

    {publisher: 'Scholastic',
     author: 'J. K. Rowling',
     title: 'Chamber of Secrets',
     year: 1999, 
     reviews: [
       {rating: 10, user: 'joe', review: 'The best ...'},
       {rating: 6, user: 'mary', review: 'Average ...'}]},

    {publisher: 'Scholastic',
     author: 'J. K. Rowling',
     title: 'Sorcerers Stone',
     year: 1998},

    {publisher: 'Scholastic',
     author: 'R. L. Stine',
     title: 'Monster Blood IV',
     year: 1997, 
     reviews: [
       {rating: 8, user: 'rob', review: 'High on my list...'}, 
       {rating: 2, user: 'mike', review: 'Not worth the paper ...', 
        discussion:
          [{user: 'ben', text: 'This is too harsh...'}, 
           {user: 'jill', text: 'I agree ...'}]}]},

    {publisher: 'Grosset',
     author: 'Carolyn Keene',
     title: 'The Secret of Kane',
     year: 1930}
  ];##
"$books"

  
// (from exampleQueries)
$books
-> write({type:'hdfs', location:'example.jql', 
          outoptions:{format    : 'org.apache.hadoop.mapred.TextOutputFormat',
                      converter : 'com.ibm.jaql.io.hadoop.converter.ToJsonTextConverter'}});##
{
  "location": "example.jql",
  "outoptions": {
    "adapter": "com.ibm.jaql.io.hadoop.DefaultHadoopOutputAdapter",
    "configurator": "com.ibm.jaql.io.hadoop.FileOutputConfigurator",
    "converter": "com.ibm.jaql.io.hadoop.converter.ToJsonTextConverter",
    "format": "org.apache.hadoop.mapred.TextOutputFormat"
  },
  "type": "hdfs"
}

                              
// (from exampleQueries)
registerAdapter({type     :	'myHDFSFile',
                 inoptions:	{adapter      : 'com.ibm.jaql.io.hadoop.DefaultHadoopInputAdapter', 
                             format       : 'org.apache.hadoop.mapred.TextInputFormat', 
                             converter    : 'com.ibm.jaql.io.hadoop.converter.FromJsonTextConverter',
                             configurator : 'com.ibm.jaql.io.hadoop.FileInputConfigurator'}});##
{
  "inoptions": {
    "adapter": "com.ibm.jaql.io.hadoop.DefaultHadoopInputAdapter",
    "configurator": "com.ibm.jaql.io.hadoop.FileInputConfigurator",
    "converter": "com.ibm.jaql.io.hadoop.converter.FromJsonTextConverter",
    "format": "org.apache.hadoop.mapred.TextInputFormat"
  },
  "type": "myHDFSFile"
}


// (from exampleQueries)       
[] -> hbaseWrite('example');##
{
  "location": "example",
  "type": "hbase"
}


// (from exampleQueries)
read({type:'myHDFSFile', location:'example.jql'})
-> transform {key: $.publisher, ($.title): $.year}
-> hbaseWrite('example');##
{
  "location": "example",
  "type": "hbase"
}


//-- hbase write/read expressions --

// write/read Items to an HBase Table as Items: default adapter, default format, no converter (from storageQueries)
[{key: '1', one: '1'}, {key: '2', two: '2'}, {key: '3', three: '3'}]
-> write({type: 'hbase', location: 'jaqlTesttest5'});##
{
  "location": "jaqlTesttest5",
  "type": "hbase"
}


read({type: 'hbase', location: 'jaqlTesttest5'});##
[
  {
    "key": "1",
    "one": "1"
  },
  {
    "key": "2",
    "two": "2"
  },
  {
    "key": "3",
    "three": "3"
  }
]


[{key: '1', one: '1'}, {key: '2', two: '2'}, {key: '3', three: '3'}]
-> write({type: 'hbase', location: 'jaqlTesttest5alt1'})
-> read();##
[
  {
    "key": "1",
    "one": "1"
  },
  {
    "key": "2",
    "two": "2"
  },
  {
    "key": "3",
    "three": "3"
  }
]


// can do the same thing if you pass in the right options to HadoopRead
[{key: '1', one: '1'}, {key: '2', two: '2'}, {key: '3', three: '3'}]
-> write({location: 'jaqlTesttest5alt2', 
         outoptions: {adapter: 'com.ibm.jaql.io.hadoop.DefaultHadoopOutputAdapter', 
                      format: 'com.ibm.jaql.io.hbase.JaqlTableOutputFormat',
                      configurator: 'com.ibm.jaql.io.hbase.TableOutputConfigurator'}});##
{
  "location": "jaqlTesttest5alt2",
  "outoptions": {
    "adapter": "com.ibm.jaql.io.hadoop.DefaultHadoopOutputAdapter",
    "configurator": "com.ibm.jaql.io.hbase.TableOutputConfigurator",
    "format": "com.ibm.jaql.io.hbase.JaqlTableOutputFormat"
  }
}

                            
read({location: 'jaqlTesttest5alt2',
	    inoptions: {adapter: 'com.ibm.jaql.io.hadoop.DefaultHadoopInputAdapter',
			  	    format: 'com.ibm.jaql.io.hbase.JaqlTableInputFormat',
			  	    configurator: 'com.ibm.jaql.io.hbase.TableInputConfigurator'}});##
[
  {
    "key": "1",
    "one": "1"
  },
  {
    "key": "2",
    "two": "2"
  },
  {
    "key": "3",
    "three": "3"
  }
]


[{key: '1', one: '1'}, {key: '2', two: '2'}, {key: '3', three: '3'}]
-> write({location: 'jaqlTesttest5alt3', 
                outoptions: {adapter: 'com.ibm.jaql.io.hadoop.DefaultHadoopOutputAdapter', 
                             format: 'com.ibm.jaql.io.hbase.JaqlTableOutputFormat',
                             configurator: 'com.ibm.jaql.io.hbase.TableOutputConfigurator'},
                inoptions: {adapter: 'com.ibm.jaql.io.hadoop.DefaultHadoopInputAdapter',
			  	            format: 'com.ibm.jaql.io.hbase.JaqlTableInputFormat',
			  	            configurator: 'com.ibm.jaql.io.hbase.TableInputConfigurator'}})
-> read();##
[
  {
    "key": "1",
    "one": "1"
  },
  {
    "key": "2",
    "two": "2"
  },
  {
    "key": "3",
    "three": "3"
  }
]

              
// input is hdfs sequence file of Items, output is hbase table (note that 'g' renamed to 'key') (from storageQueries.txt)
[] -> write({type: 'hbase', location: 'jaqlTesttest6out'});##
{
  "location": "jaqlTesttest6out",
  "type": "hbase"
}


// note, 'g' is quoted so that it can be used as a key for subsequent hbase tests (from storageQueries.txt)
[
  { key: 0, g:"0", text: 'zero' },
  { key: 1, g:"1", text: 'one' },
  { key: 2, g:"0", text: 'two' },
  { key: 3, g:"1", text: 'three' },
  { key: 4, g:"0", text: 'four' },
  { key: 5, g:"1", text: 'five' },
  { key: 6, g:"0", text: 'six' },
  { key: 7, g:"1", text: 'seven' },
  { key: 8, g:"0", text: 'eight' },
]
-> write({ type: 'hdfs', location: 'jaqlTest/test6.dat'});##
{
  "location": "jaqlTest/test6.dat",
  "type": "hdfs"
}


mapReduce( {
   'input' : {type: 'hdfs', location: 'jaqlTest/test6.dat'},
   'map'   : fn($) ( $ -> transform [ $.g, 1 ] ),
   'reduce': fn($key, $values) [{ key: $key, n: count($values) }],
   'output': {type: 'hbase', location: 'jaqlTesttest6out'}
   })
-> read();##
[
  {
    "key": "0",
    "n": 5
  },
  {
    "key": "1",
    "n": 4
  }
]

 
// input is hbase table, output is sequence file of Items
[
  { key: "0", g:0, text: 'zero' },
  { key: "1", g:1, text: 'one' },
  { key: "2", g:0, text: 'two' },
  { key: "3", g:1, text: 'three' },
  { key: "4", g:0, text: 'four' },
  { key: "5", g:1, text: 'five' },
  { key: "6", g:0, text: 'six' },
  { key: "7", g:1, text: 'seven' },
  { key: "8", g:0, text: 'eight' },
]
-> write({ type: 'hbase', location: 'jaqlTesttest7'});##
{
  "location": "jaqlTesttest7",
  "type": "hbase"
}


mapReduce( {
    'input' : {type: 'hbase', location: 'jaqlTesttest7'},
    'map'   : fn($) ( $ -> transform [ $.g, 1 ] ),
    'reduce': fn($key, $values) [{ g: $key, n: count($values) }],
    'output': {type: 'hdfs', location: 'jaqlTest/test7.dat'}
    })
-> read();##
[
  {
    "g": 0,
    "n": 5
  },
  {
    "g": 1,
    "n": 4
  }
]

 
//-- test composite input adapter (from storageQueries.txt)

// write out an hdfs file
[
  { key: "0", g:0, text: 'zero' },
  { key: "1", g:1, text: 'one' },
  { key: "2", g:0, text: 'two' },
  { key: "3", g:1, text: 'three' },
  { key: "4", g:0, text: 'four' }
]
-> write({ type: 'hdfs', location: 'jaqlTest/test9.dat'});##
{
  "location": "jaqlTest/test9.dat",
  "type": "hdfs"
}


// write out an hbase table
[
  { key: "5", g:1, text: 'five' },
  { key: "6", g:0, text: 'six' },
  { key: "7", g:1, text: 'seven' },
  { key: "8", g:0, text: 'eight' },
]
-> write({ type: 'hbase', location: 'jaqlTesttest9'});##
{
  "location": "jaqlTesttest9",
  "type": "hbase"
}


read([{type: 'hdfs', location: 'jaqlTest/test9.dat'}, {type: 'hbase', location: 'jaqlTesttest9'}]);##
[
  {
    "g": 0,
    "key": "0",
    "text": "zero"
  },
  {
    "g": 1,
    "key": "1",
    "text": "one"
  },
  {
    "g": 0,
    "key": "2",
    "text": "two"
  },
  {
    "g": 1,
    "key": "3",
    "text": "three"
  },
  {
    "g": 0,
    "key": "4",
    "text": "four"
  },
  {
    "g": 1,
    "key": "5",
    "text": "five"
  },
  {
    "g": 0,
    "key": "6",
    "text": "six"
  },
  {
    "g": 1,
    "key": "7",
    "text": "seven"
  },
  {
    "g": 0,
    "key": "8",
    "text": "eight"
  }
]


mapReduce( {
    'input': [ [{type: 'hdfs', location: 'jaqlTest/test9.dat'}, {type: 'hbase', location: 'jaqlTesttest9'}] ],
     'map'   : fn($) ( $ -> transform [ $.g, 1 ] ),
     'reduce': fn($key, $values) [{ g: $key, n: count($values) }],
     'output': {type: 'hdfs', location: 'jaqlTest/test9out.dat'}
   })
-> read();##
[
  {
    "g": 0,
    "n": 5
  },
  {
    "g": 1,
    "n": 4
  }
]


//-- test co-group (from storageQueries.txt)

mapReduce( {input: [ {type: 'hdfs', location: 'jaqlTest/test9.dat'}, 
                            {type: 'hbase', location: 'jaqlTesttest9'}],
                   map: [ fn($) ( $ -> transform [ $.g, 1 ] ),
                          fn($) ( $ -> transform [ $.g, 1 ] ) ],
                   reduce: fn($key, $aVals, $bVals) [ { g: $key, as: count($aVals), bs: count($bVals) } ],
                   output: {type: 'hdfs', location: 'jaqlTest/test10out.dat'}})
-> read();##
[
  {
    "as": 3,
    "bs": 2,
    "g": 0
  },
  {
    "as": 2,
    "bs": 2,
    "g": 1
  }
]


// (from storageQueries.txt)
[{key: '0', a: 'foo'}, {key: '1', b: 'bar'}]
-> write({type:'hbase', location:'jaqlTesttest13a.dat'});##
{
  "location": "jaqlTesttest13a.dat",
  "type": "hbase"
}


// (from storageQueries.txt)
read({type:'hbase', location:'jaqlTesttest13a.dat'});##
[
  {
    "a": "foo",
    "key": "0"
  },
  {
    "b": "bar",
    "key": "1"
  }
]


// (from storageQueries.txt)                   
[{key: '0', a: 'foo'}, {key: '1', b: 'bar'}]
-> hbaseWrite('jaqlTesttest13b.dat');##
{
  "location": "jaqlTesttest13b.dat",
  "type": "hbase"
}


// (from storageQueries.txt)
hbaseRead('jaqlTesttest13b.dat');##
[
  {
    "a": "foo",
    "key": "0"
  },
  {
    "b": "bar",
    "key": "1"
  }
]


// group by over hbase read (from storageQueries.txt)
read({type: 'hbase', location: 'jaqlTesttest7'}) 
-> group by $a = ($.g)
    into { g:$a, i:count($) };##
[
  {
    "g": 0,
    "i": 5
  },
  {
    "g": 1,
    "i": 4
  }
]


// for loop over hbase read (from storageQueries.txt)
read({type: 'hbase', location: 'jaqlTesttest7'})
-> transform $.key;##
[
  "0",
  "1",
  "2",
  "3",
  "4",
  "5",
  "6",
  "7",
  "8"
]


// co-group (from storageQueries.txt)
group
  read({type: 'hdfs', location: 'jaqlTest/test9.dat'}) 
     by $g = ($.g) as $i,
  read({type: 'hbase', location: 'jaqlTesttest9'}) 
     by $g = ($.g) as $j
 into { g: $g, as: count($i), bs: count($j) };##
[
  {
    "as": 3,
    "bs": 2,
    "g": 0
  },
  {
    "as": 2,
    "bs": 2,
    "g": 1
  }
]

