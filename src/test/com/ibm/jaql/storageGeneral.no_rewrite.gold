
;//------------------- TEST-CASE -----------------
//0. Prepare test data
$atoms = [1,2,3];
;//------------------- TEST-CASE -----------------

$atoms2 = [1,"love",null,
           3,,
           'Great Wall'];
;//------------------- TEST-CASE -----------------

$jsons = [
  { key: 0, g:"0", text: 'zero' },
  { key: 1, g:"1", text: 'one' },
  { key: 2, g:"0", text: 'two' },
  { key: 3, g:"1", text: 'three' },
  { key: 4, g:"0", text: 'four' },
  { key: 5, g:"1", text: 'five' },
  { key: 6, g:"0", text: 'six' },
  { key: 7, g:"1", text: 'seven' },
  { key: 8, g:"0", text: 'eight' }
];
;//------------------- TEST-CASE -----------------

$jsons2 = [
  { key: 0, 
    g:"0", 
    text: 'zero' },
  { key: 1, 
    g:"1", text: 'one' },
  { key: 2, g:"0", 
                   text: 'two' },
  { key: 3, g:"1"}
];
;//------------------- TEST-CASE -----------------


//1. HDFS Write & Read Syntax testing
//1.1 exercise hdfs write syntax
$atoms-> write({type: 'hdfs'});

;//--------------------- RESULT ------------------



;//----- EXCEPTION -----

// java.lang.IllegalArgumentException

;//------------------- TEST-CASE -----------------

;//------------------- TEST-CASE -----------------

$atoms-> write({location: 'jaqlTest/test1.dat'});

;//--------------------- RESULT ------------------



;//----- EXCEPTION -----

// java.lang.RuntimeException

;//------------------- TEST-CASE -----------------

;//------------------- TEST-CASE -----------------

//defect 9271
//$atoms-> write({type: 'hdfs', location: 'jaqlTest\ test2.dat'});
$atoms-> write({type: 'hdfs', location: 'jaqlTest\test3.dat'});

;//--------------------- RESULT ------------------

{
  "location": "jaqlTest\test3.dat",
  "type": "hdfs"
}

;//------------------- TEST-CASE -----------------

$atoms-> write({type: 'hdfs', location: 'jaqlTest\\test3.dat'});

;//--------------------- RESULT ------------------

{
  "location": "jaqlTest\\test3.dat",
  "type": "hdfs"
}

;//------------------- TEST-CASE -----------------

$atoms-> write({type: 'hdfs', location: 'jaqlTest:test4.dat'});

;//--------------------- RESULT ------------------



;//----- EXCEPTION -----

// java.net.URISyntaxException

;//------------------- TEST-CASE -----------------

;//------------------- TEST-CASE -----------------

$atoms-> write({type: 'hdfs', location: 'jaqlTest&test5.dat'});

;//--------------------- RESULT ------------------

{
  "location": "jaqlTest&test5.dat",
  "type": "hdfs"
}

;//------------------- TEST-CASE -----------------

$atoms-> write({type: 'hdfs2', location: 'jaqlTest/test6.dat'});

;//--------------------- RESULT ------------------



;//----- EXCEPTION -----

// java.lang.RuntimeException

;//------------------- TEST-CASE -----------------

;//------------------- TEST-CASE -----------------

$atoms-> write({type: 'hdfs ', location: 'jaqlTest/test7.dat'});

;//--------------------- RESULT ------------------



;//----- EXCEPTION -----

// java.lang.RuntimeException

;//------------------- TEST-CASE -----------------

;//------------------- TEST-CASE -----------------

$atoms-> write({type: 'HDFS', location: 'jaqlTest/test8.dat'});

;//--------------------- RESULT ------------------



;//----- EXCEPTION -----

// java.lang.RuntimeException

;//------------------- TEST-CASE -----------------

;//------------------- TEST-CASE -----------------
  
$atoms-> write({type: 'hdfs',location: 'jaqlTest/test1.dat'});

;//--------------------- RESULT ------------------

{
  "location": "jaqlTest/test1.dat",
  "type": "hdfs"
}

;//------------------- TEST-CASE -----------------
 
//1.2 exercise hdfs read syntax
read({type: 'hdfs',location: 'jaqlTest/test1.dat'});

;//--------------------- RESULT ------------------

[
  1,
  2,
  3
]

;//------------------- TEST-CASE -----------------

read({type: 'hdfs',location: 'jaqlTest/nonexistentFile'});

;//--------------------- RESULT ------------------



;//----- EXCEPTION -----

// org.apache.hadoop.mapred.InvalidInputException

;//------------------- TEST-CASE -----------------

;//------------------- TEST-CASE -----------------
 
read({type: 'hdfs',location: 'nonexistent/test1.dat'});

;//--------------------- RESULT ------------------



;//----- EXCEPTION -----

// org.apache.hadoop.mapred.InvalidInputException

;//------------------- TEST-CASE -----------------

;//------------------- TEST-CASE -----------------
 
read({type: 'hdfs',location: ''});

;//--------------------- RESULT ------------------



;//----- EXCEPTION -----

// java.lang.NullPointerException

;//------------------- TEST-CASE -----------------

;//------------------- TEST-CASE -----------------
 
read({type: 'hdfs'});

;//--------------------- RESULT ------------------



;//----- EXCEPTION -----

// java.lang.NullPointerException

;//------------------- TEST-CASE -----------------

;//------------------- TEST-CASE -----------------
 
read({type: 'hdfs', location: 'jaqlTest\NonexistentFile'});

;//----- EXCEPTION -----

// antlr.TokenStreamRecognitionException

;//------------------- TEST-CASE -----------------

;//------------------- TEST-CASE -----------------

;//------------------- TEST-CASE -----------------
 
//1.3 exercise specified adapter option  
read({type: 'hdfs',location: 'jaqlTest/test1.dat'});

;//--------------------- RESULT ------------------

[
  1,
  2,
  3
]

;//------------------- TEST-CASE -----------------
     
//specify an input adapter for output option, an output adapter for input option                                           
$atoms-> write({type: 'hdfs', 
                location: 'jaqlTest/test1.dat',
                outoptions: {adapter: 'com.ibm.jaql.io.hadoop.DefaultHadoopInputAdapter'}
               });

;//--------------------- RESULT ------------------



;//----- EXCEPTION -----

// java.lang.ClassCastException

;//------------------- TEST-CASE -----------------

;//------------------- TEST-CASE -----------------
  
read({type: 'hdfs', 
      location: 'jaqlTest/test1.dat',
      inoptions: {adapter: 'com.ibm.jaql.io.hadoop.DefaultHadoopOutputAdapter'}
       });

;//--------------------- RESULT ------------------



;//----- EXCEPTION -----

// java.lang.ClassCastException

;//------------------- TEST-CASE -----------------

;//------------------- TEST-CASE -----------------
  
//specify an invalidate adapter
$atoms-> write({type: 'hdfs', 
                location: 'jaqlTest/test1.dat',
                outoptions: {adapter: 'com.ibm.jaql.io.hadoop.DefaultHadoopInputAdapter_NotExisting'}
               });

;//--------------------- RESULT ------------------



;//----- EXCEPTION -----

// java.lang.ClassNotFoundException

;//------------------- TEST-CASE -----------------

;//------------------- TEST-CASE -----------------

read({type: 'hdfs', 
      location: 'jaqlTest/test1.dat',
      inoptions: {adapter: 'com.ibm.jaql.io.hadoop.DefaultHadoopInputAdapter_NotExisting'}
       });

;//--------------------- RESULT ------------------



;//----- EXCEPTION -----

// java.lang.ClassNotFoundException

;//------------------- TEST-CASE -----------------

;//------------------- TEST-CASE -----------------

//specify an invalidate adapter keyword
$atoms-> write({type: 'hdfs', 
                location: 'jaqlTest/test1.dat',
                outoptions: {NonSense_adapter: 'com.ibm.jaql.io.hadoop.DefaultHadoopInputAdapter'}
               });

;//--------------------- RESULT ------------------

{
  "location": "jaqlTest/test1.dat",
  "outoptions": {
    "NonSense_adapter": "com.ibm.jaql.io.hadoop.DefaultHadoopInputAdapter"
  },
  "type": "hdfs"
}

;//------------------- TEST-CASE -----------------
  
read({type: 'hdfs', 
      location: 'jaqlTest/test1.dat',
      inoptions: {NonSense_adapter: 'com.ibm.jaql.io.hadoop.DefaultHadoopInputAdapter'}
       });

;//--------------------- RESULT ------------------

[
  1,
  2,
  3
]

;//------------------- TEST-CASE -----------------
  
//specify a local file adapt file for hdfs                                              
$atoms-> write({type: 'hdfs', 
                location: 'jaqlTest/test1.dat',
                outoptions: {adapter: 'com.ibm.jaql.io.stream.FileStreamOutputAdapter'}
               });

;//--------------------- RESULT ------------------



;//----- EXCEPTION -----

// java.lang.ClassCastException

;//------------------- TEST-CASE -----------------

;//------------------- TEST-CASE -----------------
 
read({type: 'hdfs', 
      location: 'jaqlTest/test1.dat',
      inoptions: {adapter: 'com.ibm.jaql.io.stream.FileStreamInputAdapter'}
       });

;//--------------------- RESULT ------------------



;//----- EXCEPTION -----

// java.lang.Exception

;//------------------- TEST-CASE -----------------

;//------------------- TEST-CASE -----------------
                                                                                                     
//1.4 exercise specified format option
//specify a null format out option
$atoms-> write({type: 'hdfs', 
                location: 'jaqlTest/test1.dat',
                outoptions: {format: ''}
               });

;//--------------------- RESULT ------------------



;//----- EXCEPTION -----

// java.lang.ClassNotFoundException

;//------------------- TEST-CASE -----------------

;//------------------- TEST-CASE -----------------
 
read({type: 'hdfs', 
      location: 'jaqlTest/test1.dat',
      inoptions: {format: ''}
       });

;//--------------------- RESULT ------------------



;//----- EXCEPTION -----

// java.lang.ClassNotFoundException

;//------------------- TEST-CASE -----------------

;//------------------- TEST-CASE -----------------
   
//specify right sequencefile output format
$atoms-> write({type: 'hdfs', 
                location: 'jaqlTest/test1.dat',
                outoptions: {format: 'org.apache.hadoop.mapred.SequenceFileOutputFormat'}
               });

;//--------------------- RESULT ------------------

{
  "location": "jaqlTest/test1.dat",
  "outoptions": {
    "format": "org.apache.hadoop.mapred.SequenceFileOutputFormat"
  },
  "type": "hdfs"
}

;//------------------- TEST-CASE -----------------
  
read({type: 'hdfs', 
      location: 'jaqlTest/test1.dat',
      inoptions: {format: 'org.apache.hadoop.mapred.SequenceFileInputFormat'}
       });

;//--------------------- RESULT ------------------

[
  1,
  2,
  3
]

;//------------------- TEST-CASE -----------------
 
//only specify text output format, no converter specified, defect 9273
$atoms-> write({type: 'hdfs', 
                location: 'jaqlTest/test1.dat',
                outoptions: {format: 'org.apache.hadoop.mapred.TextOutputFormat'}
               });

;//--------------------- RESULT ------------------

{
  "location": "jaqlTest/test1.dat",
  "outoptions": {
    "format": "org.apache.hadoop.mapred.TextOutputFormat"
  },
  "type": "hdfs"
}

;//------------------- TEST-CASE -----------------
  
read({type: 'hdfs', 
      location: 'jaqlTest/test1.dat',
      inoptions: {format: 'org.apache.hadoop.mapred.TextInputFormat'}});

;//--------------------- RESULT ------------------

[

;//----- EXCEPTION -----

// java.lang.ClassCastException

;//------------------- TEST-CASE -----------------

;//------------------- TEST-CASE -----------------
  
//1.5 exercise specified converter option
//specify a ToDelConverter converter
$atoms-> write({type: 'hdfs', 
                location: 'jaqlTest/test1.dat',
                outoptions: {converter: 'com.ibm.jaql.io.hadoop.converter.ToDelConverter'}
               });

;//--------------------- RESULT ------------------



;//----- EXCEPTION -----

// java.lang.NullPointerException

;//------------------- TEST-CASE -----------------

;//------------------- TEST-CASE -----------------

read({type: 'hdfs', 
      location: 'jaqlTest/test1.dat',
      inoptions: {converter: 'com.ibm.jaql.io.hadoop.converter.FromDelConverter'}});

;//--------------------- RESULT ------------------

[

;//----- EXCEPTION -----

// java.io.EOFException

;//------------------- TEST-CASE -----------------

;//------------------- TEST-CASE -----------------
   
//specify a ToJsonTextConverter
$atoms-> write({type: 'hdfs', 
                location: 'jaqlTest/test1.dat',
                outoptions: {converter: 'com.ibm.jaql.io.hadoop.converter.ToJsonTextConverter'}
               });

;//--------------------- RESULT ------------------



;//----- EXCEPTION -----

// java.io.IOException

;//------------------- TEST-CASE -----------------

;//------------------- TEST-CASE -----------------
 
read({type: 'hdfs', 
      location: 'jaqlTest/test1.dat',
      inoptions: {converter: 'com.ibm.jaql.io.hadoop.converter.FromJsonTextConverter'}});

;//--------------------- RESULT ------------------

[

;//----- EXCEPTION -----

// java.io.EOFException

;//------------------- TEST-CASE -----------------

;//------------------- TEST-CASE -----------------
                  
//1.6 exercise specified configurator option
//specify an input configurator instead of output configurator
$atoms-> write({type: 'hdfs', 
                location: 'jaqlTest/test1.dat',
                outoptions: {configurator: 'com.ibm.jaql.io.hadoop.FileInputConfigurator'}
               });

;//--------------------- RESULT ------------------



;//----- EXCEPTION -----

// java.io.IOException

;//------------------- TEST-CASE -----------------

;//------------------- TEST-CASE -----------------
  
read({type: 'hdfs', 
      location: 'jaqlTest/test1.dat',
      inoptions: {configurator: 'com.ibm.jaql.io.hadoop.FileOutputConfigurator'}});

;//--------------------- RESULT ------------------



;//----- EXCEPTION -----

// java.io.IOException

;//------------------- TEST-CASE -----------------

;//------------------- TEST-CASE -----------------
               
       
//2 HDFS Sequence file Write & Read
$atoms-> write({type: 'hdfs', location: 'jaqlTest/test1.dat'});

;//--------------------- RESULT ------------------

{
  "location": "jaqlTest/test1.dat",
  "type": "hdfs"
}

;//------------------- TEST-CASE -----------------

read({type: 'hdfs', location: 'jaqlTest/test1.dat'});

;//--------------------- RESULT ------------------

[
  1,
  2,
  3
]

;//------------------- TEST-CASE -----------------
 
$atoms2-> write({type: 'hdfs', location: 'jaqlTest/test2.dat'});

;//--------------------- RESULT ------------------

{
  "location": "jaqlTest/test2.dat",
  "type": "hdfs"
}

;//------------------- TEST-CASE -----------------

read({type: 'hdfs', location: 'jaqlTest/test2.dat'});

;//--------------------- RESULT ------------------

[
  1,
  "love",
  null,
  3,
  "Great Wall"
]

;//------------------- TEST-CASE -----------------
 
$jsons-> write({type: 'hdfs', location: 'jaqlTest/test3.dat'});

;//--------------------- RESULT ------------------

{
  "location": "jaqlTest/test3.dat",
  "type": "hdfs"
}

;//------------------- TEST-CASE -----------------

read({type: 'hdfs', location: 'jaqlTest/test3.dat'});

;//--------------------- RESULT ------------------

[
  {
    "g": "0",
    "key": 0,
    "text": "zero"
  },
  {
    "g": "1",
    "key": 1,
    "text": "one"
  },
  {
    "g": "0",
    "key": 2,
    "text": "two"
  },
  {
    "g": "1",
    "key": 3,
    "text": "three"
  },
  {
    "g": "0",
    "key": 4,
    "text": "four"
  },
  {
    "g": "1",
    "key": 5,
    "text": "five"
  },
  {
    "g": "0",
    "key": 6,
    "text": "six"
  },
  {
    "g": "1",
    "key": 7,
    "text": "seven"
  },
  {
    "g": "0",
    "key": 8,
    "text": "eight"
  }
]

;//------------------- TEST-CASE -----------------
 
$jsons2-> write({type: 'hdfs', location: 'jaqlTest/test4.dat'});

;//--------------------- RESULT ------------------

{
  "location": "jaqlTest/test4.dat",
  "type": "hdfs"
}

;//------------------- TEST-CASE -----------------

read({type: 'hdfs', location: 'jaqlTest/test4.dat'});

;//--------------------- RESULT ------------------

[
  {
    "g": "0",
    "key": 0,
    "text": "zero"
  },
  {
    "g": "1",
    "key": 1,
    "text": "one"
  },
  {
    "g": "0",
    "key": 2,
    "text": "two"
  },
  {
    "g": "1",
    "key": 3
  }
]

;//------------------- TEST-CASE -----------------
 

//3 HDFS Text file Write & Read
//3.1 only sepcify text output format, no any converter specified
$atoms-> write({type: 'hdfs', 
                location: 'jaqlTest/test1.dat',
                outoptions: {format: 'org.apache.hadoop.mapred.TextOutputFormat'}
               });

;//--------------------- RESULT ------------------

{
  "location": "jaqlTest/test1.dat",
  "outoptions": {
    "format": "org.apache.hadoop.mapred.TextOutputFormat"
  },
  "type": "hdfs"
}

;//------------------- TEST-CASE -----------------
 
read({type: 'hdfs', 
      location: 'jaqlTest/test1.dat',
      inoptions: {format: 'org.apache.hadoop.mapred.TextInputFormat'}
     });

;//--------------------- RESULT ------------------

[

;//----- EXCEPTION -----

// java.lang.ClassCastException

;//------------------- TEST-CASE -----------------

;//------------------- TEST-CASE -----------------
 
$jsons-> write({type: 'hdfs', 
                location: 'jaqlTest/test2.dat',
                outoptions: {format: 'org.apache.hadoop.mapred.TextOutputFormat'}
               });

;//--------------------- RESULT ------------------

{
  "location": "jaqlTest/test2.dat",
  "outoptions": {
    "format": "org.apache.hadoop.mapred.TextOutputFormat"
  },
  "type": "hdfs"
}

;//------------------- TEST-CASE -----------------
 
read({type: 'hdfs', 
      location: 'jaqlTest/test2.dat',
      inoptions: {format: 'org.apache.hadoop.mapred.TextInputFormat'}
     });

;//--------------------- RESULT ------------------

[

;//----- EXCEPTION -----

// java.lang.ClassCastException

;//------------------- TEST-CASE -----------------

;//------------------- TEST-CASE -----------------
 
//3.2 specify text output format and ToDel converter
$atoms-> write({type: 'hdfs', 
                location: 'jaqlTest/test1.dat',
                outoptions: {format: 'org.apache.hadoop.mapred.TextOutputFormat',
                             converter: 'com.ibm.jaql.io.hadoop.converter.ToDelConverter'}
               });

;//--------------------- RESULT ------------------

{
  "location": "jaqlTest/test1.dat",
  "outoptions": {
    "converter": "com.ibm.jaql.io.hadoop.converter.ToDelConverter",
    "format": "org.apache.hadoop.mapred.TextOutputFormat"
  },
  "type": "hdfs"
}

;//------------------- TEST-CASE -----------------
 
read({type: 'hdfs', 
      location: 'jaqlTest/test1.dat',
      inoptions: {format: 'org.apache.hadoop.mapred.TextInputFormat',
                   converter: 'com.ibm.jaql.io.hadoop.converter.FromDelConverter'}
     });

;//--------------------- RESULT ------------------

[
  [
    "1"
  ],
  [
    "2"
  ],
  [
    "3"
  ]
]

;//------------------- TEST-CASE -----------------
 
read({type: 'hdfs', 
      location: 'jaqlTest/test1.dat',
      inoptions: {format: 'org.apache.hadoop.mapred.TextInputFormat',
                   converter: 'com.ibm.jaql.io.hadoop.converter.FromJsonTextConverter'}
     });

;//--------------------- RESULT ------------------

[
  1,
  2,
  3
]

;//------------------- TEST-CASE -----------------
 
     
$atoms2-> write({type: 'hdfs', 
                location: 'jaqlTest/test2.dat',
                outoptions: {format: 'org.apache.hadoop.mapred.TextOutputFormat',
                             converter: 'com.ibm.jaql.io.hadoop.converter.ToDelConverter'}
               });

;//--------------------- RESULT ------------------

{
  "location": "jaqlTest/test2.dat",
  "outoptions": {
    "converter": "com.ibm.jaql.io.hadoop.converter.ToDelConverter",
    "format": "org.apache.hadoop.mapred.TextOutputFormat"
  },
  "type": "hdfs"
}

;//------------------- TEST-CASE -----------------
 
//Failure
read({type: 'hdfs', 
      location: 'jaqlTest/test2.dat',
      inoptions: {format: 'org.apache.hadoop.mapred.TextInputFormat',
                   converter: 'com.ibm.jaql.io.hadoop.converter.FromDelConverter'}
     });

;//--------------------- RESULT ------------------

[
  [
    "1"
  ],
  [
    "love"
  ]

;//----- EXCEPTION -----

// java.lang.AssertionError

;//------------------- TEST-CASE -----------------

;//------------------- TEST-CASE -----------------
 
read({type: 'hdfs', 
      location: 'jaqlTest/test2.dat',
      inoptions: {format: 'org.apache.hadoop.mapred.TextInputFormat',
                   converter: 'com.ibm.jaql.io.hadoop.converter.FromJsonTextConverter'}
     });

;//--------------------- RESULT ------------------

[
  1,
  "love"

;//----- EXCEPTION -----

// com.ibm.jaql.json.parser.ParseException

;//------------------- TEST-CASE -----------------

;//------------------- TEST-CASE -----------------
  
//Failure     
$jsons-> write({type: 'hdfs', 
                location: 'jaqlTest/test3.dat',
                outoptions: {format: 'org.apache.hadoop.mapred.TextOutputFormat',
                             converter: 'com.ibm.jaql.io.hadoop.converter.ToDelConverter'}
               });

;//--------------------- RESULT ------------------



;//----- EXCEPTION -----

// java.lang.IllegalArgumentException

;//------------------- TEST-CASE -----------------

;//------------------- TEST-CASE -----------------
 
read({type: 'hdfs', 
      location: 'jaqlTest/test3.dat',
      inoptions: {format: 'org.apache.hadoop.mapred.TextInputFormat',
                   converter: 'com.ibm.jaql.io.hadoop.converter.FromDelConverter'}
     });

;//--------------------- RESULT ------------------

[]

;//------------------- TEST-CASE -----------------
 
read({type: 'hdfs', 
      location: 'jaqlTest/test3.dat',
      inoptions: {format: 'org.apache.hadoop.mapred.TextInputFormat',
                   converter: 'com.ibm.jaql.io.hadoop.converter.FromJsonTextConverter'}
     });

;//--------------------- RESULT ------------------

[]

;//------------------- TEST-CASE -----------------
 

$jsons2-> write({type: 'hdfs', 
                location: 'jaqlTest/test4.dat',
                outoptions: {format: 'org.apache.hadoop.mapred.TextOutputFormat',
                             converter: 'com.ibm.jaql.io.hadoop.converter.ToDelConverter'}
               });

;//--------------------- RESULT ------------------



;//----- EXCEPTION -----

// java.lang.IllegalArgumentException

;//------------------- TEST-CASE -----------------

;//------------------- TEST-CASE -----------------
 
read({type: 'hdfs', 
      location: 'jaqlTest/test4.dat',
      inoptions: {format: 'org.apache.hadoop.mapred.TextInputFormat',
                   converter: 'com.ibm.jaql.io.hadoop.converter.FromDelConverter'}
     });

;//--------------------- RESULT ------------------

[]

;//------------------- TEST-CASE -----------------
 
read({type: 'hdfs', 
      location: 'jaqlTest/test4.dat',
      inoptions: {format: 'org.apache.hadoop.mapred.TextInputFormat',
                   converter: 'com.ibm.jaql.io.hadoop.converter.FromJsonTextConverter'}
     });

;//--------------------- RESULT ------------------

[]

;//------------------- TEST-CASE -----------------
      
//3.3 specify text output format and ToJson converter            
$atoms-> write({type: 'hdfs', 
                location: 'jaqlTest/test4.dat',
                outoptions: {format: 'org.apache.hadoop.mapred.TextOutputFormat',
                             converter: 'com.ibm.jaql.io.hadoop.converter.ToJsonTextConverter'}
               });

;//--------------------- RESULT ------------------

{
  "location": "jaqlTest/test4.dat",
  "outoptions": {
    "converter": "com.ibm.jaql.io.hadoop.converter.ToJsonTextConverter",
    "format": "org.apache.hadoop.mapred.TextOutputFormat"
  },
  "type": "hdfs"
}

;//------------------- TEST-CASE -----------------
  
read({type: 'hdfs', 
      location: 'jaqlTest/test4.dat',
      inoptions: {format: 'org.apache.hadoop.mapred.TextInputFormat',
                   converter: 'com.ibm.jaql.io.hadoop.converter.FromJsonTextConverter'}
     });

;//--------------------- RESULT ------------------

[
  1,
  2,
  3
]

;//------------------- TEST-CASE -----------------
  
read({type: 'hdfs', 
      location: 'jaqlTest/test4.dat',
      inoptions: {format: 'org.apache.hadoop.mapred.TextInputFormat',
                   converter: 'com.ibm.jaql.io.hadoop.converter.FromDelConverter'}
     });

;//--------------------- RESULT ------------------

[
  [
    "1"
  ],
  [
    "2"
  ],
  [
    "3"
  ]
]

;//------------------- TEST-CASE -----------------
       
$atoms2-> write({type: 'hdfs', 
                location: 'jaqlTest/test5.dat',
                outoptions: {format: 'org.apache.hadoop.mapred.TextOutputFormat',
                             converter: 'com.ibm.jaql.io.hadoop.converter.ToJsonTextConverter'}
               });

;//--------------------- RESULT ------------------

{
  "location": "jaqlTest/test5.dat",
  "outoptions": {
    "converter": "com.ibm.jaql.io.hadoop.converter.ToJsonTextConverter",
    "format": "org.apache.hadoop.mapred.TextOutputFormat"
  },
  "type": "hdfs"
}

;//------------------- TEST-CASE -----------------
  
read({type: 'hdfs', 
      location: 'jaqlTest/test5.dat',
      inoptions: {format: 'org.apache.hadoop.mapred.TextInputFormat',
                   converter: 'com.ibm.jaql.io.hadoop.converter.FromJsonTextConverter'}
     });

;//--------------------- RESULT ------------------

[
  1,
  "love",
  null,
  3,
  "Great Wall"
]

;//------------------- TEST-CASE -----------------

read({type: 'hdfs', 
      location: 'jaqlTest/test5.dat',
      inoptions: {format: 'org.apache.hadoop.mapred.TextInputFormat',
                   converter: 'com.ibm.jaql.io.hadoop.converter.FromDelConverter'}
     });

;//--------------------- RESULT ------------------

[
  [
    "1"
  ],
  [
    "love"
  ],
  [
    "null"
  ],
  [
    "3"
  ],
  [
    "Great Wall"
  ]
]

;//------------------- TEST-CASE -----------------
       
$jsons-> write({type: 'hdfs', 
                location: 'jaqlTest/test6.dat',
                outoptions: {format: 'org.apache.hadoop.mapred.TextOutputFormat',
                             converter: 'com.ibm.jaql.io.hadoop.converter.ToJsonTextConverter'}
               });

;//--------------------- RESULT ------------------

{
  "location": "jaqlTest/test6.dat",
  "outoptions": {
    "converter": "com.ibm.jaql.io.hadoop.converter.ToJsonTextConverter",
    "format": "org.apache.hadoop.mapred.TextOutputFormat"
  },
  "type": "hdfs"
}

;//------------------- TEST-CASE -----------------
  
read({type: 'hdfs', 
      location: 'jaqlTest/test6.dat',
      inoptions: {format: 'org.apache.hadoop.mapred.TextInputFormat',
                   converter: 'com.ibm.jaql.io.hadoop.converter.FromJsonTextConverter'}
     });

;//--------------------- RESULT ------------------

[
  {
    "g": "0",
    "key": 0,
    "text": "zero"
  },
  {
    "g": "1",
    "key": 1,
    "text": "one"
  },
  {
    "g": "0",
    "key": 2,
    "text": "two"
  },
  {
    "g": "1",
    "key": 3,
    "text": "three"
  },
  {
    "g": "0",
    "key": 4,
    "text": "four"
  },
  {
    "g": "1",
    "key": 5,
    "text": "five"
  },
  {
    "g": "0",
    "key": 6,
    "text": "six"
  },
  {
    "g": "1",
    "key": 7,
    "text": "seven"
  },
  {
    "g": "0",
    "key": 8,
    "text": "eight"
  }
]

;//------------------- TEST-CASE -----------------
 
read({type: 'hdfs', 
      location: 'jaqlTest/test6.dat',
      inoptions: {format: 'org.apache.hadoop.mapred.TextInputFormat',
                   converter: 'com.ibm.jaql.io.hadoop.converter.FromDelConverter'}
     });

;//--------------------- RESULT ------------------

[
  [
    "{  \"g\": \"0\"",
    "  \"key\": 0",
    "  \"text\": \"zero\"}"
  ],
  [
    "{  \"g\": \"1\"",
    "  \"key\": 1",
    "  \"text\": \"one\"}"
  ],
  [
    "{  \"g\": \"0\"",
    "  \"key\": 2",
    "  \"text\": \"two\"}"
  ],
  [
    "{  \"g\": \"1\"",
    "  \"key\": 3",
    "  \"text\": \"three\"}"
  ],
  [
    "{  \"g\": \"0\"",
    "  \"key\": 4",
    "  \"text\": \"four\"}"
  ],
  [
    "{  \"g\": \"1\"",
    "  \"key\": 5",
    "  \"text\": \"five\"}"
  ],
  [
    "{  \"g\": \"0\"",
    "  \"key\": 6",
    "  \"text\": \"six\"}"
  ],
  [
    "{  \"g\": \"1\"",
    "  \"key\": 7",
    "  \"text\": \"seven\"}"
  ],
  [
    "{  \"g\": \"0\"",
    "  \"key\": 8",
    "  \"text\": \"eight\"}"
  ]
]

;//------------------- TEST-CASE -----------------
   
$jsons2-> write({type: 'hdfs', 
                location: 'jaqlTest/test7.dat',
                outoptions: {format: 'org.apache.hadoop.mapred.TextOutputFormat',
                             converter: 'com.ibm.jaql.io.hadoop.converter.ToJsonTextConverter'}
               });

;//--------------------- RESULT ------------------

{
  "location": "jaqlTest/test7.dat",
  "outoptions": {
    "converter": "com.ibm.jaql.io.hadoop.converter.ToJsonTextConverter",
    "format": "org.apache.hadoop.mapred.TextOutputFormat"
  },
  "type": "hdfs"
}

;//------------------- TEST-CASE -----------------
  
read({type: 'hdfs', 
      location: 'jaqlTest/test7.dat',
      inoptions: {format: 'org.apache.hadoop.mapred.TextInputFormat',
                   converter: 'com.ibm.jaql.io.hadoop.converter.FromJsonTextConverter'}
     });

;//--------------------- RESULT ------------------

[
  {
    "g": "0",
    "key": 0,
    "text": "zero"
  },
  {
    "g": "1",
    "key": 1,
    "text": "one"
  },
  {
    "g": "0",
    "key": 2,
    "text": "two"
  },
  {
    "g": "1",
    "key": 3
  }
]

;//------------------- TEST-CASE -----------------
  
read({type: 'hdfs', 
      location: 'jaqlTest/test7.dat',
      inoptions: {format: 'org.apache.hadoop.mapred.TextInputFormat',
                   converter: 'com.ibm.jaql.io.hadoop.converter.FromDelConverter'}
     });

;//--------------------- RESULT ------------------

[
  [
    "{  \"g\": \"0\"",
    "  \"key\": 0",
    "  \"text\": \"zero\"}"
  ],
  [
    "{  \"g\": \"1\"",
    "  \"key\": 1",
    "  \"text\": \"one\"}"
  ],
  [
    "{  \"g\": \"0\"",
    "  \"key\": 2",
    "  \"text\": \"two\"}"
  ]

;//----- EXCEPTION -----

// java.lang.RuntimeException

;//------------------- TEST-CASE -----------------

;//------------------- TEST-CASE -----------------
                  
//3.4 specify text output format and incompatible stream converter           
$atoms-> write({type: 'hdfs', 
                location: 'jaqlTest/test1.dat',
                outoptions: {format: 'org.apache.hadoop.mapred.TextOutputFormat',
                             converter: 'com.ibm.jaql.io.stream.converter.JsonTextOutputStream'}
               });

;//--------------------- RESULT ------------------



;//----- EXCEPTION -----

// java.lang.ClassCastException

;//------------------- TEST-CASE -----------------

;//------------------- TEST-CASE -----------------

    
//4 Local file read & write
//4.1 specify Json text output format for write, and Json text input format for read
$atoms-> write({location: 'test1.dat',
                 outoptions: {adapter: 'com.ibm.jaql.io.stream.FileStreamOutputAdapter',
                              format: 'com.ibm.jaql.io.stream.converter.JsonTextOutputStream'}
               });

;//--------------------- RESULT ------------------

{
  "location": "test1.dat",
  "outoptions": {
    "adapter": "com.ibm.jaql.io.stream.FileStreamOutputAdapter",
    "format": "com.ibm.jaql.io.stream.converter.JsonTextOutputStream"
  }
}

;//------------------- TEST-CASE -----------------
  
read({type: 'hdfs', 
      location: 'test1.dat',
      inoptions: {adapter: 'com.ibm.jaql.io.stream.FileStreamInputAdapter',
                  format: 'com.ibm.jaql.io.stream.converter.JsonTextInputStream'}
     });

;//--------------------- RESULT ------------------

[
  1,
  2,
  3
]

;//------------------- TEST-CASE -----------------
   
$atoms2-> write({location: 'test2.dat',
                 outoptions: {adapter: 'com.ibm.jaql.io.stream.FileStreamOutputAdapter',
                              format: 'com.ibm.jaql.io.stream.converter.JsonTextOutputStream'}
               });

;//--------------------- RESULT ------------------

{
  "location": "test2.dat",
  "outoptions": {
    "adapter": "com.ibm.jaql.io.stream.FileStreamOutputAdapter",
    "format": "com.ibm.jaql.io.stream.converter.JsonTextOutputStream"
  }
}

;//------------------- TEST-CASE -----------------
  
read({type: 'hdfs', 
      location: 'test2.dat',
      inoptions: {adapter: 'com.ibm.jaql.io.stream.FileStreamInputAdapter',
                  format: 'com.ibm.jaql.io.stream.converter.JsonTextInputStream'}
     });

;//--------------------- RESULT ------------------

[
  1,
  "love"
]

;//------------------- TEST-CASE -----------------
  
$jsons-> write({location: 'test3.dat',
                 outoptions: {adapter: 'com.ibm.jaql.io.stream.FileStreamOutputAdapter',
                              format: 'com.ibm.jaql.io.stream.converter.JsonTextOutputStream'}
               });

;//--------------------- RESULT ------------------

{
  "location": "test3.dat",
  "outoptions": {
    "adapter": "com.ibm.jaql.io.stream.FileStreamOutputAdapter",
    "format": "com.ibm.jaql.io.stream.converter.JsonTextOutputStream"
  }
}

;//------------------- TEST-CASE -----------------
  
read({type: 'hdfs', 
      location: 'test3.dat',
      inoptions: {adapter: 'com.ibm.jaql.io.stream.FileStreamInputAdapter',
                  format: 'com.ibm.jaql.io.stream.converter.JsonTextInputStream'}
     });

;//--------------------- RESULT ------------------

[
  {
    "g": "0",
    "key": 0,
    "text": "zero"
  },
  {
    "g": "1",
    "key": 1,
    "text": "one"
  },
  {
    "g": "0",
    "key": 2,
    "text": "two"
  },
  {
    "g": "1",
    "key": 3,
    "text": "three"
  },
  {
    "g": "0",
    "key": 4,
    "text": "four"
  },
  {
    "g": "1",
    "key": 5,
    "text": "five"
  },
  {
    "g": "0",
    "key": 6,
    "text": "six"
  },
  {
    "g": "1",
    "key": 7,
    "text": "seven"
  },
  {
    "g": "0",
    "key": 8,
    "text": "eight"
  }
]

;//------------------- TEST-CASE -----------------
  
$jsons2-> write({location: 'test4.dat',
                 outoptions: {adapter: 'com.ibm.jaql.io.stream.FileStreamOutputAdapter',
                              format: 'com.ibm.jaql.io.stream.converter.JsonTextOutputStream'}
               });

;//--------------------- RESULT ------------------

{
  "location": "test4.dat",
  "outoptions": {
    "adapter": "com.ibm.jaql.io.stream.FileStreamOutputAdapter",
    "format": "com.ibm.jaql.io.stream.converter.JsonTextOutputStream"
  }
}

;//------------------- TEST-CASE -----------------
  
read({type: 'hdfs', 
      location: 'test4.dat',
      inoptions: {adapter: 'com.ibm.jaql.io.stream.FileStreamInputAdapter',
                  format: 'com.ibm.jaql.io.stream.converter.JsonTextInputStream'}
     });

;//--------------------- RESULT ------------------

[
  {
    "g": "0",
    "key": 0,
    "text": "zero"
  },
  {
    "g": "1",
    "key": 1,
    "text": "one"
  },
  {
    "g": "0",
    "key": 2,
    "text": "two"
  },
  {
    "g": "1",
    "key": 3
  }
]

;//------------------- TEST-CASE -----------------
       
//4.2 specify Json text output format for write, and Json binary input format for read    
$atoms-> write({location: 'test1.dat',
                 outoptions: {adapter: 'com.ibm.jaql.io.stream.FileStreamOutputAdapter',
                              format: 'com.ibm.jaql.io.stream.converter.JsonTextOutputStream'}
               });

;//--------------------- RESULT ------------------

{
  "location": "test1.dat",
  "outoptions": {
    "adapter": "com.ibm.jaql.io.stream.FileStreamOutputAdapter",
    "format": "com.ibm.jaql.io.stream.converter.JsonTextOutputStream"
  }
}

;//------------------- TEST-CASE -----------------
  
read({type: 'hdfs', 
      location: 'test1.dat',
      inoptions: {adapter: 'com.ibm.jaql.io.stream.FileStreamInputAdapter',
                  format: 'com.ibm.jaql.io.stream.converter.JsonInputStream'}
     });

;//--------------------- RESULT ------------------

[

;//----- EXCEPTION -----

// java.lang.ArrayIndexOutOfBoundsException

;//------------------- TEST-CASE -----------------

;//------------------- TEST-CASE -----------------
  
$jsons-> write({location: 'test2.dat',
                 outoptions: {adapter: 'com.ibm.jaql.io.stream.FileStreamOutputAdapter',
                              format: 'com.ibm.jaql.io.stream.converter.JsonTextOutputStream'}
               });

;//--------------------- RESULT ------------------

{
  "location": "test2.dat",
  "outoptions": {
    "adapter": "com.ibm.jaql.io.stream.FileStreamOutputAdapter",
    "format": "com.ibm.jaql.io.stream.converter.JsonTextOutputStream"
  }
}

;//------------------- TEST-CASE -----------------
  
read({type: 'hdfs', 
      location: 'test2.dat',
      inoptions: {adapter: 'com.ibm.jaql.io.stream.FileStreamInputAdapter',
                  format: 'com.ibm.jaql.io.stream.converter.JsonInputStream'}
     });

;//--------------------- RESULT ------------------

[

;//----- EXCEPTION -----

// java.lang.ArrayIndexOutOfBoundsException

;//------------------- TEST-CASE -----------------

;//------------------- TEST-CASE -----------------
   
//4.3 specify Json binary output format for write, and Json binary input format for read
$atoms-> write({location: 'test1.dat',
                 outoptions: {adapter: 'com.ibm.jaql.io.stream.FileStreamOutputAdapter',
                              format: 'com.ibm.jaql.io.stream.converter.JsonOutputStream'}
               });

;//--------------------- RESULT ------------------

{
  "location": "test1.dat",
  "outoptions": {
    "adapter": "com.ibm.jaql.io.stream.FileStreamOutputAdapter",
    "format": "com.ibm.jaql.io.stream.converter.JsonOutputStream"
  }
}

;//------------------- TEST-CASE -----------------
  
read({type: 'hdfs', 
      location: 'test1.dat',
      inoptions: {adapter: 'com.ibm.jaql.io.stream.FileStreamInputAdapter',
                  format: 'com.ibm.jaql.io.stream.converter.JsonInputStream'}
     });

;//--------------------- RESULT ------------------

[
  1,
  2,
  3
]

;//------------------- TEST-CASE -----------------

$atoms2-> write({location: 'test2.dat',
                 outoptions: {adapter: 'com.ibm.jaql.io.stream.FileStreamOutputAdapter',
                              format: 'com.ibm.jaql.io.stream.converter.JsonOutputStream'}
               });

;//--------------------- RESULT ------------------

{
  "location": "test2.dat",
  "outoptions": {
    "adapter": "com.ibm.jaql.io.stream.FileStreamOutputAdapter",
    "format": "com.ibm.jaql.io.stream.converter.JsonOutputStream"
  }
}

;//------------------- TEST-CASE -----------------
  
read({type: 'hdfs', 
      location: 'test2.dat',
      inoptions: {adapter: 'com.ibm.jaql.io.stream.FileStreamInputAdapter',
                  format: 'com.ibm.jaql.io.stream.converter.JsonInputStream'}
     });

;//--------------------- RESULT ------------------

[
  1,
  "love"
]

;//------------------- TEST-CASE -----------------

$jsons-> write({location: 'test3.dat',
                 outoptions: {adapter: 'com.ibm.jaql.io.stream.FileStreamOutputAdapter',
                              format: 'com.ibm.jaql.io.stream.converter.JsonOutputStream'}
               });

;//--------------------- RESULT ------------------

{
  "location": "test3.dat",
  "outoptions": {
    "adapter": "com.ibm.jaql.io.stream.FileStreamOutputAdapter",
    "format": "com.ibm.jaql.io.stream.converter.JsonOutputStream"
  }
}

;//------------------- TEST-CASE -----------------
  
read({type: 'hdfs', 
      location: 'test3.dat',
      inoptions: {adapter: 'com.ibm.jaql.io.stream.FileStreamInputAdapter',
                  format: 'com.ibm.jaql.io.stream.converter.JsonInputStream'}
     });

;//--------------------- RESULT ------------------

[
  {
    "g": "0",
    "key": 0,
    "text": "zero"
  },
  {
    "g": "1",
    "key": 1,
    "text": "one"
  },
  {
    "g": "0",
    "key": 2,
    "text": "two"
  },
  {
    "g": "1",
    "key": 3,
    "text": "three"
  },
  {
    "g": "0",
    "key": 4,
    "text": "four"
  },
  {
    "g": "1",
    "key": 5,
    "text": "five"
  },
  {
    "g": "0",
    "key": 6,
    "text": "six"
  },
  {
    "g": "1",
    "key": 7,
    "text": "seven"
  },
  {
    "g": "0",
    "key": 8,
    "text": "eight"
  }
]

;//------------------- TEST-CASE -----------------
  
$jsons2-> write({location: 'test4.dat',
                 outoptions: {adapter: 'com.ibm.jaql.io.stream.FileStreamOutputAdapter',
                              format: 'com.ibm.jaql.io.stream.converter.JsonOutputStream'}
               });

;//--------------------- RESULT ------------------

{
  "location": "test4.dat",
  "outoptions": {
    "adapter": "com.ibm.jaql.io.stream.FileStreamOutputAdapter",
    "format": "com.ibm.jaql.io.stream.converter.JsonOutputStream"
  }
}

;//------------------- TEST-CASE -----------------
  
read({type: 'hdfs', 
      location: 'test4.dat',
      inoptions: {adapter: 'com.ibm.jaql.io.stream.FileStreamInputAdapter',
                  format: 'com.ibm.jaql.io.stream.converter.JsonInputStream'}
     });

;//--------------------- RESULT ------------------

[
  {
    "g": "0",
    "key": 0,
    "text": "zero"
  },
  {
    "g": "1",
    "key": 1,
    "text": "one"
  },
  {
    "g": "0",
    "key": 2,
    "text": "two"
  },
  {
    "g": "1",
    "key": 3
  }
]

;//------------------- TEST-CASE -----------------
   
//4.4 specify Json binary output format for write, and Json text input format for read      
$atoms-> write({location: 'test1.dat',
                 outoptions: {adapter: 'com.ibm.jaql.io.stream.FileStreamOutputAdapter',
                              format: 'com.ibm.jaql.io.stream.converter.JsonOutputStream'}
               });

;//--------------------- RESULT ------------------

{
  "location": "test1.dat",
  "outoptions": {
    "adapter": "com.ibm.jaql.io.stream.FileStreamOutputAdapter",
    "format": "com.ibm.jaql.io.stream.converter.JsonOutputStream"
  }
}

;//------------------- TEST-CASE -----------------
  
read({type: 'hdfs', 
      location: 'test1.dat',
      inoptions: {adapter: 'com.ibm.jaql.io.stream.FileStreamInputAdapter',
                  format: 'com.ibm.jaql.io.stream.converter.JsonTextInputStream'}
     });

;//--------------------- RESULT ------------------

[

;//----- EXCEPTION -----

// com.ibm.jaql.json.parser.TokenMgrError

;//------------------- TEST-CASE -----------------

;//------------------- TEST-CASE -----------------
     
$jsons-> write({location: 'test2.dat',
                 outoptions: {adapter: 'com.ibm.jaql.io.stream.FileStreamOutputAdapter',
                              format: 'com.ibm.jaql.io.stream.converter.JsonOutputStream'}
               });

;//--------------------- RESULT ------------------

{
  "location": "test2.dat",
  "outoptions": {
    "adapter": "com.ibm.jaql.io.stream.FileStreamOutputAdapter",
    "format": "com.ibm.jaql.io.stream.converter.JsonOutputStream"
  }
}

;//------------------- TEST-CASE -----------------
  
read({type: 'hdfs', 
      location: 'test2.dat',
      inoptions: {adapter: 'com.ibm.jaql.io.stream.FileStreamInputAdapter',
                  format: 'com.ibm.jaql.io.stream.converter.JsonTextInputStream'}
     });

;//--------------------- RESULT ------------------

[

;//----- EXCEPTION -----

// com.ibm.jaql.json.parser.TokenMgrError

;//------------------- TEST-CASE -----------------

;//------------------- TEST-CASE -----------------
  

//5. Map-Reduce
//5.1 input is a file with JSON text (one record per line), output is a SequenceFile of items
$jsons-> write({type: 'hdfs', location: 'jaqlTest/test1.txt', 
         outoptions: {format: 'org.apache.hadoop.mapred.TextOutputFormat',
                      converter: 'com.foobar.store.ToJSONTxtConverter',
                      configurator: 'com.foobar.store.TextFileOutputConfigurator'}});

;//--------------------- RESULT ------------------

{
  "location": "jaqlTest/test1.txt",
  "outoptions": {
    "configurator": "com.foobar.store.TextFileOutputConfigurator",
    "converter": "com.foobar.store.ToJSONTxtConverter",
    "format": "org.apache.hadoop.mapred.TextOutputFormat"
  },
  "type": "hdfs"
}

;//------------------- TEST-CASE -----------------
	    
mapReduce( {
    'input' : {type: 'hdfs', location: 'jaqlTest/test1.txt', 
               inoptions: {format: 'org.apache.hadoop.mapred.TextInputFormat',
			  		   converter: 'com.foobar.store.FromJSONTxtConverter'}},
    'map'   : fn($) ( $ -> transform [ $.g, 1 ]),
    'reduce': fn(key, values) [{ g: key, n: count(values) }],
    'output': {type: 'hdfs', location: 'jaqlTest/test1.dat'}
    })
-> read();

;//--------------------- RESULT ------------------

[
  {
    "g": "0",
    "n": 5
  },
  {
    "g": "1",
    "n": 4
  }
]

;//------------------- TEST-CASE -----------------

$jsons2-> write({type: 'hdfs', location: 'jaqlTest/test2.txt', 
         outoptions: {format: 'org.apache.hadoop.mapred.TextOutputFormat',
                      converter: 'com.foobar.store.ToJSONTxtConverter',
                      configurator: 'com.foobar.store.TextFileOutputConfigurator'}});

;//--------------------- RESULT ------------------

{
  "location": "jaqlTest/test2.txt",
  "outoptions": {
    "configurator": "com.foobar.store.TextFileOutputConfigurator",
    "converter": "com.foobar.store.ToJSONTxtConverter",
    "format": "org.apache.hadoop.mapred.TextOutputFormat"
  },
  "type": "hdfs"
}

;//------------------- TEST-CASE -----------------
	    
mapReduce( {
    'input' : {type: 'hdfs', location: 'jaqlTest/test2.txt', 
               inoptions: {format: 'org.apache.hadoop.mapred.TextInputFormat',
			  		    converter: 'com.foobar.store.FromJSONTxtConverter'}},
    'map'   : fn($) ( $ -> transform [ $.g, 1 ]),
    'reduce': fn(key, values) [{ g: key, n: count(values) }],
    'output': {type: 'hdfs', location: 'jaqlTest/test2.dat'}
    })
-> read();

;//--------------------- RESULT ------------------

[
  {
    "g": "0",
    "n": 2
  },
  {
    "g": "1",
    "n": 2
  }
]

;//------------------- TEST-CASE -----------------

$atoms-> write({type: 'hdfs', location: 'jaqlTest/test3.txt', 
         outoptions: {format: 'org.apache.hadoop.mapred.TextOutputFormat',
                      converter: 'com.foobar.store.ToJSONTxtConverter',
                      configurator: 'com.foobar.store.TextFileOutputConfigurator'}});

;//--------------------- RESULT ------------------

{
  "location": "jaqlTest/test3.txt",
  "outoptions": {
    "configurator": "com.foobar.store.TextFileOutputConfigurator",
    "converter": "com.foobar.store.ToJSONTxtConverter",
    "format": "org.apache.hadoop.mapred.TextOutputFormat"
  },
  "type": "hdfs"
}

;//------------------- TEST-CASE -----------------
	    
mapReduce( {
    'input' : {type: 'hdfs', location: 'jaqlTest/test3.txt', 
               inoptions: {format: 'org.apache.hadoop.mapred.TextInputFormat',
			  		    converter: 'com.foobar.store.FromJSONTxtConverter'}},
    'map'   : fn($) ( $ -> transform [ $, $*2 ]),
    'reduce': fn(key, values) [{ g: key, n: key+sum(values) }],
    'output': {type: 'hdfs', location: 'jaqlTest/test3.dat'}
    })
-> read();

;//--------------------- RESULT ------------------

[
  {
    "g": 1,
    "n": 3
  },
  {
    "g": 2,
    "n": 6
  },
  {
    "g": 3,
    "n": 9
  }
]

;//------------------- TEST-CASE -----------------

//5.2 input is a SequenceFile , output is also a SequenceFile of items
$jsons-> write({type: 'hdfs', location: 'jaqlTest/test3.dat'});

;//--------------------- RESULT ------------------

{
  "location": "jaqlTest/test3.dat",
  "type": "hdfs"
}

;//------------------- TEST-CASE -----------------

mapReduce( {
    'input' : {type: 'hdfs', location: 'jaqlTest/test3.dat'},
    'map'   : fn($) ( $ -> transform [ $.g, 1 ]),
    'reduce': fn(key, values) [{ g: key, n: count(values) }],
    'output': {type: 'hdfs', location: 'jaqlTest/test33.dat'}
    })
-> read();

;//--------------------- RESULT ------------------

[
  {
    "g": "0",
    "n": 5
  },
  {
    "g": "1",
    "n": 4
  }
]

;//------------------- TEST-CASE -----------------

$jsons2-> write({type: 'hdfs', location: 'jaqlTest/test4.dat'});

;//--------------------- RESULT ------------------

{
  "location": "jaqlTest/test4.dat",
  "type": "hdfs"
}

;//------------------- TEST-CASE -----------------

mapReduce( {
    'input' : {type: 'hdfs', location: 'jaqlTest/test4.dat'},
    'map'   : fn($) ( $ -> transform [ $.g, 1 ]),
    'reduce': fn(key, values) [{ g: key, n: count(values) }],
    'output': {type: 'hdfs', location: 'jaqlTest/test44.dat'}
    })
-> read();

;//--------------------- RESULT ------------------

[
  {
    "g": "0",
    "n": 2
  },
  {
    "g": "1",
    "n": 2
  }
]

;//------------------- TEST-CASE -----------------

$atoms-> write({type: 'hdfs', location: 'jaqlTest/test5.txt'});

;//--------------------- RESULT ------------------

{
  "location": "jaqlTest/test5.txt",
  "type": "hdfs"
}

;//------------------- TEST-CASE -----------------
	    
mapReduce( {
    'input' : {type: 'hdfs', location: 'jaqlTest/test5.txt'},
    'map'   : fn($) ( $ -> transform [ $, $*2 ]),
    'reduce': fn(key, values) [{ g: key, n: key+sum(values) }],
    'output': {type: 'hdfs', location: 'jaqlTest/test5.dat'}
    })
-> read();

;//--------------------- RESULT ------------------

[
  {
    "g": 1,
    "n": 3
  },
  {
    "g": 2,
    "n": 6
  },
  {
    "g": 3,
    "n": 9
  }
]

;//------------------- TEST-CASE -----------------

;//------------------- TEST-DONE -----------------
