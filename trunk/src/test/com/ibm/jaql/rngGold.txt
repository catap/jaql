registerRNG('r1', fn() 17);##
"r1"


registerRNG('r2', fn() 5);##
"r2"


for( $i in 1 to 5 )
  [ sampleRNG('r1') ];##
[
  -4937981208836185383,
  -5582529378488325032,
  1530270151771565451,
  -3389839389802268617,
  818775917343865025
]

  
for( $i in 1 to 10 )
  [ sampleRNG('r2') ];##
[
  -4971030886054769832,
  1628080142987304160,
  9018696937790626762,
  8519392794254051655,
  8275179822514474066,
  -5576223184683215057,
  5123902406589080641,
  -4427941369665934060,
  4044347177351347406,
  -1468102548290959518
]

  
hdfsWrite('test', [1,2,3,4,5]);##
{
  "location": "test",
  "type": "hdfs"
}


registerRNG('r3', fn() 17);##
"r3"


// test map-reduce using the same seed per-split
for( $i in hdfsRead('test') )
  [ sampleRNG('r3') ];##
[
  -4937981208836185383,
  -5582529378488325032,
  1530270151771565451,
  -3389839389802268617,
  818775917343865025
]

  
registerRNG('r4', fn() 5);##
"r4"
  

// test map-reduce using the same seed per-split
for( $i in hdfsRead('test') )
  [ sampleRNG('r4') ];##
[
  -4971030886054769832,
  1628080142987304160,
  9018696937790626762,
  8519392794254051655,
  8275179822514474066
]

  
// test map-reduce and reading a variable from the jobConf
for( $i in hdfsRead('test', {seed: 17, adapter: 'com.acme.extensions.data.SeedingHadoopAdapter', 
					                                 format: 'org.apache.hadoop.mapred.SequenceFileInputFormat',
					                                 configurator : 'com.ibm.jaql.io.hadoop.FileInputConfigurator'}) )
  [ readConf("seed", 17) ];##
[
  "-4937981208836185383",
  "-4937981208836185383",
  "-4937981208836185383",
  "-4937981208836185383",
  "-4937981208836185383"
]

  
// seed based on the value in the jobConf named "seed"
registerRNG('r5', fn() readConf("seed", 17));##
"r5"


// test map-reduce using a variable seed per-split. this example has one split
// so expect repeated value
for( $i in hdfsRead('test', {seed: 17, adapter: 'com.acme.extensions.data.SeedingHadoopAdapter', 
                                      format: 'org.apache.hadoop.mapred.SequenceFileInputFormat',
					                                 configurator : 'com.ibm.jaql.io.hadoop.FileInputConfigurator'}) )
  [ sampleRNG('r5') ];##
[
  532182518298471973,
  1794509825941162386,
  3246151381995469705,
  9074336781408775578,
  5272732885952040446
]

  
// test seeding adapter with composite split
$a = for( $i in hdfsRead('test', {seed: 17, adapter: 'com.acme.extensions.data.SeedingHadoopAdapter'}) ) [{k:$i}];##
"$a"


hdfsWrite('test2', [ {k:2}, {k:2}, {k:3} ]);##
{
  "location": "test2",
  "type": "hdfs"
}


join( $i in $a on $i.k,
      $j in hdfsRead('test2') on $j.k )
  [ $i ];##
[
  {
    "k": 2
  },
  {
    "k": 2
  },
  {
    "k": 3
  }
]

