
;//------------------- TEST-CASE -----------------
//===========================================================================
// probeLongList
//===========================================================================

// Useful function library that is built on probeLongList:
// TODO: make a module!

//---------------------------------------------------------------------------
// Return p in probe where probeKeyFn(p) in longVals.
// longVals will be loaded into memory (typically in each map task).
//
// probe is [T], T is any
// probeKeyFn is fn(T) returns long?
// longVals is [long?]
// returns [T]
//---------------------------------------------------------------------------
inMemoryLongInList = fn( [any...] probe, function probeKeyFn, [long...] longVals)
(
  probe
    -> transform [probeKeyFn($), $] // get the key form the probe (better be a long)
    -> probeLongList( longVals ) // return [ [long key, any value, position] ]
    -> filter $[2] >= 0 // only matching items
    -> transform $[1] // return the original probe value
);
;//------------------- TEST-CASE -----------------


//---------------------------------------------------------------------------
// Return p in probe where longHash(probeKeyFn(p)) in { longHash(i) : i in inList }
//
// The hash values will be temp'd on disk and then loaded into memory
// (typically reloaded into memory on each map task).
//
// probe is [T], T is any
// probeKeyFn is fn(T) returns any
// inList is [any]
// returns [T]
//---------------------------------------------------------------------------
fuzzyInList = fn(probe, probeKeyFn, inList) (
  // Temp the hash codes so we don't recompute them on each mapper.
  hashFd = inList -> transform longHash($) -> write(HadoopTemp()),
  probe -> inMemoryLongInList( fn(p) longHash(probeKeyFn(p)), read(hashFd) )
);
;//------------------- TEST-CASE -----------------


//---------------------------------------------------------------------------
// Return p in probe where probeKeyFn(p) in read(inListFd)
// 
// Uses fuzzyInList to find a superset of the probe values that might qualify.
// Then in a second pass, joins these candidates with the full inList to
// eliminate any false-positives.
//
// probe is [T], T is any
// probeKeyFn is fn(T) returns any
// inListFd is a file descriptor
//    read(inListFd) is [any]
// returns [T]
//---------------------------------------------------------------------------
twoPassInListFromFile = fn(probe, probeKeyFn, inListFd) (
  candidates = probe -> fuzzyInList(probeKeyFn, read(inListFd)),
  // we use cogroup instead of join just in case inListFd contains duplicates
  group candidates     by k = probeKeyFn($) as cs,
        read(inListFd) by     $ as i
    expand if(exists(i)) cs // just return the matching probe records
);
;//------------------- TEST-CASE -----------------


//---------------------------------------------------------------------------
// Return p in probe where probeKeyFn(p) in inList
// 
// Writes the inList to a temp and then runs twoPassInListFromFile.
//
// probe is [T], T is any
// probeKeyFn is fn(T) returns any
// inList is [any]
// returns [T]
//---------------------------------------------------------------------------
twoPassInList = fn(probe, probeKeyFn, inList) (
  // Temp the inList because we don't want to recompute it.
  // TODO: would be nice to avoid temp when inList is simply a read; add writeIfNotWritten()
  inListFd = inList -> write(HadoopTemp()),
  twoPassInListFromFile( probe, probeKeyFn, inListFd )
);
;//------------------- TEST-CASE -----------------


//---------------------------------------------------------------------------
// Join probe and build using a two-pass semi-join strategy that
// requires enough memory to hold the hash code for every distinct build key.
//
// Equivalent to:
// join p in probe,
//      b in read(buildFd)
//  where probeKeyFn(p) == buildKeyFn(b)
//  into projectFn(p,b)
//
// Temps the hash codes to disk and (typically) loads them all in every map task.
// Typically creates the following map/reduce jobs:
//   1. map-only job to generate the hash of the build keys.
//       This could be avoided if re-running the buildKeyFn + hash in every mapper
//       is cheaper than writing the temp and reading just the keys.
//       E.g., when the table is really skinny or in a column store (and we get
//       projection push-down).
//       To do this, we need to use inMemoryLongInList in place of fuzzyInList.
//   2. map-reduce job
//      map: will generate candidate probe values that will likely match + some false positives
//      reduce: join the probe values with the build values.
// 
// probe is [T], T is any
// probeKeyFn is fn(T) returns any  should be cheap and deterministic because called twice
// buildFd is file descriptor
//   read(buildFd) is [U], U is any
// buildKeyFn is fn(U) return any   should be cheap and deterministic because called twice
// projectFn is fn(T,U) returns V, V is any
// returns [V]
//---------------------------------------------------------------------------
twoPassJoinFromFile = fn(probe, probeKeyFn, buildFd, buildKeyFn, projectFn) (
  // TODO: add sortDistinct and optimized sortDistinctLong to avoid resorting magic set
  magicSet = read(buildFd) -> transform buildKeyFn($),
  candidates = probe -> fuzzyInList(probeKeyFn, magicSet),
  join p in candidates,
       b in read(buildFd)
    where probeKeyFn(p) == buildKeyFn(b)
    into projectFn(p,b)
);
;//------------------- TEST-CASE -----------------


//---------------------------------------------------------------------------
// Join probe and build using a two-pass semi-join strategy that
// requires enough memory to hold the hash code for every distinct build key.
//
// Temps the build to make two passes then uses twoPassJoinFromFile
//
// TODO: We could save one pass over the build file if we tee'd the hash
// codes while writing the build temp. It means either we cannot always reuse
// previous functions or we need to get clever in rewrites. eg,
//   A -> map1 -> B -> map2 -> C
// could be
//   A -> map1 -> tee --> B
//                    \-> map2 -> C
// 
// Equivalent to:
// join p in probe,
//      b in read(buildFd)
//  where probeKeyFn(p) == buildKeyFn(b)
//  into projectFn(p,b)
//---------------------------------------------------------------------------
twoPassJoin = fn(probe, probeKeyFn, build, buildKeyFn, projectFn) (
  buildFd = build -> write(HadoopTemp()), // TODO: writeIfNotWritten
  twoPassJoinFromFile(probe, probeKeyFn, buildFd, buildKeyFn, projectFn)
);
;//------------------- TEST-CASE -----------------


//---------------------------------------------------------------------------
// Join probe and build using a two-pass semi-join strategy that
// requires enough memory to hold the every distinct (long) build key.
//
// Unlike twoPassJoinFromFile, they keys must be long values, and it
// does not have any false positives from the first pass.
//
// Equivalent to:
// join p in probe,
//      b in read(buildFd)
//  where probeKeyFn(p) == buildKeyFn(b)
//  into projectFn(p,b)
//
// Temps the build keys to disk and (typically) loads them all in every map task.
// Typically creates the following map/reduce jobs:
//   1. map-only job to generate the build keys
//       This could be avoided if re-running the buildKeyFn in every mapper
//       is cheaper than writing the temp and reading just the keys.
//       E.g., when the table is really skinny or in a column store (and we get
//       projection push-down).
//       To do this, simply remove the write of the magicSet.
//   2. map-reduce job
//      map: will generate candidate probe values that will definitely match
//      reduce: join the probe values with the build values.
// 
// probe is [T], T is any
// probeKeyFn is fn(T) returns long?  should be cheap and deterministic because called twice
// buildFd is file descriptor
//   read(buildFd) is [U], U is any
// buildKeyFn is fn(U) return long?   should be cheap and deterministic because called twice
// projectFn is fn(T,U) returns V, V is any
// returns [V]
//---------------------------------------------------------------------------
twoPassJoinLongFromFile = fn(probe, probeKeyFn, buildFd, buildKeyFn, projectFn) (
  // TODO: add sortDistinct and optimized sortDistinctLong to avoid resorting magic set
  magicSetFd = read(buildFd) -> transform buildKeyFn($) -> write(HadoopTemp()),
  semijoin = probe -> inMemoryLongInList(probeKeyFn, read(magicSetFd)),
  join p in semijoin,
       b in read(buildFd)
    where probeKeyFn(p) == buildKeyFn(b)
    into projectFn(p,b)
);
;//------------------- TEST-CASE -----------------


//---------------------------------------------------------------------------
// Join probe and build using a two-pass semi-join strategy that
// requires enough memory to hold the distinct (long) build keys.
//
// Unlike twoPassJoin, they keys must be long values, and it
// does not have any false positives from the first pass.
//
// Temps the build to make two passes then uses twoPassJoinLongFromFile
// 
// Equivalent to:
// join p in probe,
//      b in read(buildFd)
//  where probeKeyFn(p) == buildKeyFn(b)
//  into projectFn(p,b)
//---------------------------------------------------------------------------
twoPassLongJoin = fn(probe, probeKeyFn, build, buildKeyFn, projectFn) (
  buildFd = build -> write(HadoopTemp()), // TODO: writeIfNotWritten
  twoPassJoinLongFromFile(probe, probeKeyFn, buildFd, buildKeyFn, projectFn)
);
;//------------------- TEST-CASE -----------------


// Tests

dataFd = hdfs('T');
;//------------------- TEST-CASE -----------------

inFd = hdfs('I');
;//------------------- TEST-CASE -----------------


// some fact-table-like data
range(1,20) 
  -> expand [ { fkey: $*2, value: $+100 },
              { fkey: $*2, value: $+200 } ]
  -> write(dataFd)
;

;//--------------------- RESULT ------------------

{
  "location": "T",
  "type": "hdfs"
}

;//------------------- TEST-CASE -----------------


// some big (pretending for testing) integer in-list
// Ideally the in list is distinct and sorted, otherwise that 
// work is repeated in each map-task.
range(1,20) 
  -> transform $*3
  // -> distinct()  << already unique
  // -> sort by [$] << already sorted  // TODO: sortDistinct()
  -> write(inFd)
;

;//--------------------- RESULT ------------------

{
  "location": "I",
  "type": "hdfs"
}

;//------------------- TEST-CASE -----------------


// Test the basic probeLongList functionality
read(dataFd)
 -> transform [$.fkey, $.value]
 -> probeLongList( read(inFd) )
;

;//--------------------- RESULT ------------------

[
  [
    2,
    101,
    -1
  ],
  [
    2,
    201,
    -1
  ],
  [
    4,
    102,
    -2
  ],
  [
    4,
    202,
    -2
  ],
  [
    6,
    103,
    1
  ],
  [
    6,
    203,
    1
  ],
  [
    8,
    104,
    -3
  ],
  [
    8,
    204,
    -3
  ],
  [
    10,
    105,
    -4
  ],
  [
    10,
    205,
    -4
  ],
  [
    12,
    106,
    3
  ],
  [
    12,
    206,
    3
  ],
  [
    14,
    107,
    -5
  ],
  [
    14,
    207,
    -5
  ],
  [
    16,
    108,
    -6
  ],
  [
    16,
    208,
    -6
  ],
  [
    18,
    109,
    5
  ],
  [
    18,
    209,
    5
  ],
  [
    20,
    110,
    -7
  ],
  [
    20,
    210,
    -7
  ],
  [
    22,
    111,
    -8
  ],
  [
    22,
    211,
    -8
  ],
  [
    24,
    112,
    7
  ],
  [
    24,
    212,
    7
  ],
  [
    26,
    113,
    -9
  ],
  [
    26,
    213,
    -9
  ],
  [
    28,
    114,
    -10
  ],
  [
    28,
    214,
    -10
  ],
  [
    30,
    115,
    9
  ],
  [
    30,
    215,
    9
  ],
  [
    32,
    116,
    -11
  ],
  [
    32,
    216,
    -11
  ],
  [
    34,
    117,
    -12
  ],
  [
    34,
    217,
    -12
  ],
  [
    36,
    118,
    11
  ],
  [
    36,
    218,
    11
  ],
  [
    38,
    119,
    -13
  ],
  [
    38,
    219,
    -13
  ],
  [
    40,
    120,
    -14
  ],
  [
    40,
    220,
    -14
  ]
]

;//------------------- TEST-CASE -----------------


// Test the basic probeLongList functionality
// This shows in-list processing.
read(dataFd)
 -> transform [$.fkey, $.value]
 -> probeLongList( read(inFd) )
 -> filter $[2] >= 0 // only matched items
 -> transform $[0]
;

;//--------------------- RESULT ------------------

[
  6,
  6,
  12,
  12,
  18,
  18,
  24,
  24,
  30,
  30,
  36,
  36
]

;//------------------- TEST-CASE -----------------


// Use the wrapper to hide transforms and filter
read(dataFd)
 -> inMemoryLongInList( fn($) $.fkey, read(inFd) )
;

;//--------------------- RESULT ------------------

[
  {
    "fkey": 6,
    "value": 103
  },
  {
    "fkey": 6,
    "value": 203
  },
  {
    "fkey": 12,
    "value": 106
  },
  {
    "fkey": 12,
    "value": 206
  },
  {
    "fkey": 18,
    "value": 109
  },
  {
    "fkey": 18,
    "value": 209
  },
  {
    "fkey": 24,
    "value": 112
  },
  {
    "fkey": 24,
    "value": 212
  },
  {
    "fkey": 30,
    "value": 115
  },
  {
    "fkey": 30,
    "value": 215
  },
  {
    "fkey": 36,
    "value": 118
  },
  {
    "fkey": 36,
    "value": 218
  }
]

;//------------------- TEST-CASE -----------------


// Try fuzzyInList: use string keys (which get hashed)
// We don't get any collisions... If our hash function is truly random,
// we wouldn't expect a collision until we had something like 2^32 keys.
read(dataFd)
  -> transform { fkey: strcat('string',$.fkey), $.value }
  -> fuzzyInList( fn($) $.fkey, read(inFd) -> transform strcat('string',$) )
;

;//--------------------- RESULT ------------------

[
  {
    "fkey": "string6",
    "value": 103
  },
  {
    "fkey": "string6",
    "value": 203
  },
  {
    "fkey": "string12",
    "value": 106
  },
  {
    "fkey": "string12",
    "value": 206
  },
  {
    "fkey": "string18",
    "value": 109
  },
  {
    "fkey": "string18",
    "value": 209
  },
  {
    "fkey": "string24",
    "value": 112
  },
  {
    "fkey": "string24",
    "value": 212
  },
  {
    "fkey": "string30",
    "value": 115
  },
  {
    "fkey": "string30",
    "value": 215
  },
  {
    "fkey": "string36",
    "value": 118
  },
  {
    "fkey": "string36",
    "value": 218
  }
]

;//------------------- TEST-CASE -----------------


// If we want to be sure we don't have any false-positives due to hash collisions
// we can use a second pass.
// We'll sort the results just to make sure the tests are happy.
stringInFd = hdfs('S');
;//------------------- TEST-CASE -----------------

read(inFd) 
  -> transform strcat('string',$) 
  -> write(stringInFd)
;

;//--------------------- RESULT ------------------

{
  "location": "S",
  "type": "hdfs"
}

;//------------------- TEST-CASE -----------------

read(dataFd)
  -> transform { fkey: strcat('string',$.fkey), $.value }
  -> twoPassInListFromFile( fn($) $.fkey, stringInFd )
  -> sort by [$]
;

;//--------------------- RESULT ------------------

[
  {
    "fkey": "string12",
    "value": 106
  },
  {
    "fkey": "string12",
    "value": 206
  },
  {
    "fkey": "string18",
    "value": 109
  },
  {
    "fkey": "string18",
    "value": 209
  },
  {
    "fkey": "string24",
    "value": 112
  },
  {
    "fkey": "string24",
    "value": 212
  },
  {
    "fkey": "string30",
    "value": 115
  },
  {
    "fkey": "string30",
    "value": 215
  },
  {
    "fkey": "string36",
    "value": 118
  },
  {
    "fkey": "string36",
    "value": 218
  },
  {
    "fkey": "string6",
    "value": 103
  },
  {
    "fkey": "string6",
    "value": 203
  }
]

;//------------------- TEST-CASE -----------------


// We can get the in list saved into a temp automagically
read(dataFd)
  -> transform { fkey: strcat('string',$.fkey), $.value }
  -> twoPassInList( fn($) $.fkey, read(inFd) -> transform strcat('string',$) )
  -> sort by [$]
;

;//--------------------- RESULT ------------------

[
  {
    "fkey": "string12",
    "value": 106
  },
  {
    "fkey": "string12",
    "value": 206
  },
  {
    "fkey": "string18",
    "value": 109
  },
  {
    "fkey": "string18",
    "value": 209
  },
  {
    "fkey": "string24",
    "value": 112
  },
  {
    "fkey": "string24",
    "value": 212
  },
  {
    "fkey": "string30",
    "value": 115
  },
  {
    "fkey": "string30",
    "value": 215
  },
  {
    "fkey": "string36",
    "value": 118
  },
  {
    "fkey": "string36",
    "value": 218
  },
  {
    "fkey": "string6",
    "value": 103
  },
  {
    "fkey": "string6",
    "value": 203
  }
]

;//------------------- TEST-CASE -----------------


// Now some joins

// some dimension-table-like data with string key
dimFd = hdfs('S');
;//------------------- TEST-CASE -----------------

read(inFd)
  -> transform { key: strcat('string',$), value: $+300 }
  -> write(dimFd)
;

;//--------------------- RESULT ------------------

{
  "location": "S",
  "type": "hdfs"
}

;//------------------- TEST-CASE -----------------


// join dimension-table in a file
read(dataFd)
  -> transform { fkey: strcat('string',$.fkey), $.value }
  -> twoPassJoinFromFile( fn(f) f.fkey, dimFd, fn(d) d.key, fn(f,d) { f, d } )
  -> sort by [$]
;

;//--------------------- RESULT ------------------

[
  {
    "d": {
      "key": "string12",
      "value": 312
    },
    "f": {
      "fkey": "string12",
      "value": 106
    }
  },
  {
    "d": {
      "key": "string12",
      "value": 312
    },
    "f": {
      "fkey": "string12",
      "value": 206
    }
  },
  {
    "d": {
      "key": "string18",
      "value": 318
    },
    "f": {
      "fkey": "string18",
      "value": 109
    }
  },
  {
    "d": {
      "key": "string18",
      "value": 318
    },
    "f": {
      "fkey": "string18",
      "value": 209
    }
  },
  {
    "d": {
      "key": "string24",
      "value": 324
    },
    "f": {
      "fkey": "string24",
      "value": 112
    }
  },
  {
    "d": {
      "key": "string24",
      "value": 324
    },
    "f": {
      "fkey": "string24",
      "value": 212
    }
  },
  {
    "d": {
      "key": "string30",
      "value": 330
    },
    "f": {
      "fkey": "string30",
      "value": 115
    }
  },
  {
    "d": {
      "key": "string30",
      "value": 330
    },
    "f": {
      "fkey": "string30",
      "value": 215
    }
  },
  {
    "d": {
      "key": "string36",
      "value": 336
    },
    "f": {
      "fkey": "string36",
      "value": 118
    }
  },
  {
    "d": {
      "key": "string36",
      "value": 336
    },
    "f": {
      "fkey": "string36",
      "value": 218
    }
  },
  {
    "d": {
      "key": "string6",
      "value": 306
    },
    "f": {
      "fkey": "string6",
      "value": 103
    }
  },
  {
    "d": {
      "key": "string6",
      "value": 306
    },
    "f": {
      "fkey": "string6",
      "value": 203
    }
  }
]

;//------------------- TEST-CASE -----------------


// join a computed dimension-table that will be temp'd
read(dataFd)
  -> transform { fkey: strcat('string',$.fkey), $.value }
  -> twoPassJoin( fn(f) f.fkey, 
                  read(dimFd) -> filter $.value < 325, 
                  fn(d) d.key, 
                  fn(f,d) { f, d } )
  -> sort by [$]
;

;//--------------------- RESULT ------------------

[
  {
    "d": {
      "key": "string12",
      "value": 312
    },
    "f": {
      "fkey": "string12",
      "value": 106
    }
  },
  {
    "d": {
      "key": "string12",
      "value": 312
    },
    "f": {
      "fkey": "string12",
      "value": 206
    }
  },
  {
    "d": {
      "key": "string18",
      "value": 318
    },
    "f": {
      "fkey": "string18",
      "value": 109
    }
  },
  {
    "d": {
      "key": "string18",
      "value": 318
    },
    "f": {
      "fkey": "string18",
      "value": 209
    }
  },
  {
    "d": {
      "key": "string24",
      "value": 324
    },
    "f": {
      "fkey": "string24",
      "value": 112
    }
  },
  {
    "d": {
      "key": "string24",
      "value": 324
    },
    "f": {
      "fkey": "string24",
      "value": 212
    }
  },
  {
    "d": {
      "key": "string6",
      "value": 306
    },
    "f": {
      "fkey": "string6",
      "value": 103
    }
  },
  {
    "d": {
      "key": "string6",
      "value": 306
    },
    "f": {
      "fkey": "string6",
      "value": 203
    }
  }
]

;//------------------- TEST-CASE -----------------


// some dimension-table-like data with long key
dimFd = hdfs('S');
;//------------------- TEST-CASE -----------------

read(inFd)
  -> transform { key: $, value: $+300 }
  -> write(dimFd)
;

;//--------------------- RESULT ------------------

{
  "location": "S",
  "type": "hdfs"
}

;//------------------- TEST-CASE -----------------


// join dimension-table with long key in a file
read(dataFd)
  -> twoPassJoinLongFromFile( fn(f) f.fkey, dimFd, fn(d) d.key, fn(f,d) { f, d } )
  -> sort by [$]
;

;//--------------------- RESULT ------------------

[
  {
    "d": {
      "key": 6,
      "value": 306
    },
    "f": {
      "fkey": 6,
      "value": 103
    }
  },
  {
    "d": {
      "key": 6,
      "value": 306
    },
    "f": {
      "fkey": 6,
      "value": 203
    }
  },
  {
    "d": {
      "key": 12,
      "value": 312
    },
    "f": {
      "fkey": 12,
      "value": 106
    }
  },
  {
    "d": {
      "key": 12,
      "value": 312
    },
    "f": {
      "fkey": 12,
      "value": 206
    }
  },
  {
    "d": {
      "key": 18,
      "value": 318
    },
    "f": {
      "fkey": 18,
      "value": 109
    }
  },
  {
    "d": {
      "key": 18,
      "value": 318
    },
    "f": {
      "fkey": 18,
      "value": 209
    }
  },
  {
    "d": {
      "key": 24,
      "value": 324
    },
    "f": {
      "fkey": 24,
      "value": 112
    }
  },
  {
    "d": {
      "key": 24,
      "value": 324
    },
    "f": {
      "fkey": 24,
      "value": 212
    }
  },
  {
    "d": {
      "key": 30,
      "value": 330
    },
    "f": {
      "fkey": 30,
      "value": 115
    }
  },
  {
    "d": {
      "key": 30,
      "value": 330
    },
    "f": {
      "fkey": 30,
      "value": 215
    }
  },
  {
    "d": {
      "key": 36,
      "value": 336
    },
    "f": {
      "fkey": 36,
      "value": 118
    }
  },
  {
    "d": {
      "key": 36,
      "value": 336
    },
    "f": {
      "fkey": 36,
      "value": 218
    }
  }
]

;//------------------- TEST-CASE -----------------


// join a computed dimension-table with long key that will be temp'd
read(dataFd)
  -> twoPassLongJoin( fn(f) f.fkey, 
                      read(dimFd) -> filter $.value < 325, 
                      fn(d) d.key, 
                      fn(f,d) { f, d } )
  -> sort by [$]
;

;//--------------------- RESULT ------------------

[
  {
    "d": {
      "key": 6,
      "value": 306
    },
    "f": {
      "fkey": 6,
      "value": 103
    }
  },
  {
    "d": {
      "key": 6,
      "value": 306
    },
    "f": {
      "fkey": 6,
      "value": 203
    }
  },
  {
    "d": {
      "key": 12,
      "value": 312
    },
    "f": {
      "fkey": 12,
      "value": 106
    }
  },
  {
    "d": {
      "key": 12,
      "value": 312
    },
    "f": {
      "fkey": 12,
      "value": 206
    }
  },
  {
    "d": {
      "key": 18,
      "value": 318
    },
    "f": {
      "fkey": 18,
      "value": 109
    }
  },
  {
    "d": {
      "key": 18,
      "value": 318
    },
    "f": {
      "fkey": 18,
      "value": 209
    }
  },
  {
    "d": {
      "key": 24,
      "value": 324
    },
    "f": {
      "fkey": 24,
      "value": 112
    }
  },
  {
    "d": {
      "key": 24,
      "value": 324
    },
    "f": {
      "fkey": 24,
      "value": 212
    }
  }
]

;//------------------- TEST-CASE -----------------


daisyChain(1, null);

;//--------------------- RESULT ------------------

1

;//------------------- TEST-CASE -----------------

daisyChain(1, []);

;//--------------------- RESULT ------------------

1

;//------------------- TEST-CASE -----------------

daisyChain(1, [fn(x) x+10, fn(x) x+100, fn(x) x+1000]);

;//--------------------- RESULT ------------------

1111

;//------------------- TEST-CASE -----------------

range(1,4) -> daisyChain([-> filter $ > 3, -> filter $ > 2, -> filter $ > 1]);

;//--------------------- RESULT ------------------

[
  4
]

;//------------------- TEST-CASE -----------------


null -> powerset();

;//--------------------- RESULT ------------------

[
  []
]

;//------------------- TEST-CASE -----------------

[] -> powerset();

;//--------------------- RESULT ------------------

[
  []
]

;//------------------- TEST-CASE -----------------

[1] -> powerset();

;//--------------------- RESULT ------------------

[
  [],
  [
    1
  ]
]

;//------------------- TEST-CASE -----------------

[1,2] -> powerset();

;//--------------------- RESULT ------------------

[
  [],
  [
    1
  ],
  [
    2
  ],
  [
    1,
    2
  ]
]

;//------------------- TEST-CASE -----------------

[1,2,3] -> powerset();

;//--------------------- RESULT ------------------

[
  [],
  [
    1
  ],
  [
    2
  ],
  [
    1,
    2
  ],
  [
    3
  ],
  [
    1,
    3
  ],
  [
    2,
    3
  ],
  [
    1,
    2,
    3
  ]
]

;//------------------- TEST-CASE -----------------

['a','b','c'] -> powerset();

;//--------------------- RESULT ------------------

[
  [],
  [
    "a"
  ],
  [
    "b"
  ],
  [
    "a",
    "b"
  ],
  [
    "c"
  ],
  [
    "a",
    "c"
  ],
  [
    "b",
    "c"
  ],
  [
    "a",
    "b",
    "c"
  ]
]

;//------------------- TEST-CASE -----------------


quit;
;//------------------- TEST-DONE -----------------
