//-- hdfs write/read expressions --

// write/read Items to SequenceFile: default adapter, default format, no converter
[1,2,3]
-> write({type: 'hdfs', location: 'jaqlTest/test1.dat'});##
{
  "location": "jaqlTest/test1.dat",
  "type": "hdfs"
}


read({type: 'hdfs', location: 'jaqlTest/test1.dat'});##
[
  1,
  2,
  3
]


[1,2,3]
-> write({type: 'hdfs', location: 'jaqlTest/test1alt.dat'})
-> read();##
[
  1,
  2,
  3
]


// write/read Items to SequenceFile: specify FileAdapter, SequenceFileFormat, no converter
[1,2,3]
-> write({location: 'jaqlTest/test2.dat', 
         outoptions: {adapter: 'com.ibm.jaql.io.hadoop.DefaultHadoopOutputAdapter', 
                         format: 'org.apache.hadoop.mapred.SequenceFileOutputFormat',
                         configurator: 'com.ibm.jaql.io.hadoop.FileOutputConfigurator'}});##
{
  "location": "jaqlTest/test2.dat",
  "outoptions": {
    "adapter": "com.ibm.jaql.io.hadoop.DefaultHadoopOutputAdapter",
    "configurator": "com.ibm.jaql.io.hadoop.FileOutputConfigurator",
    "format": "org.apache.hadoop.mapred.SequenceFileOutputFormat"
  }
}


read({location: 'jaqlTest/test2.dat', 
        inoptions: {adapter: 'com.ibm.jaql.io.hadoop.DefaultHadoopInputAdapter', 
                  format: 'org.apache.hadoop.mapred.SequenceFileInputFormat',
                  configurator: 'com.ibm.jaql.io.hadoop.FileInputConfigurator'}});##
[
  1,
  2,
  3
]

                  
[1,2,3]
-> write({location: 'jaqlTest/test2alt.dat', 
                outoptions: {adapter: 'com.ibm.jaql.io.hadoop.DefaultHadoopOutputAdapter', 
                             format: 'org.apache.hadoop.mapred.SequenceFileOutputFormat',
                             configurator: 'com.ibm.jaql.io.hadoop.FileOutputConfigurator'},
                inoptions:  {adapter: 'com.ibm.jaql.io.hadoop.DefaultHadoopInputAdapter', 
                             format: 'org.apache.hadoop.mapred.SequenceFileInputFormat',
                             configurator: 'com.ibm.jaql.io.hadoop.FileInputConfigurator'}})
-> read();##
[
  1,
  2,
  3
]

                        
// write/read Items to Text in a SequenceFile: specify ConverterAdapter, SequenceFileFormat, Item <-> Text (JSON)
// This assumes that the default, FileOutput(Input)Format can be passed a converter.
[{one: '1'}, {two: '2'}, {three: '3'}]
-> write({type: 'hdfs', location: 'jaqlTest/test3.dat', 
         outoptions: {converter: 'com.foobar.store.ToJSONSeqConverter',
                      configurator: 'com.foobar.store.TextFileOutputConfigurator'}});##
{
  "location": "jaqlTest/test3.dat",
  "outoptions": {
    "configurator": "com.foobar.store.TextFileOutputConfigurator",
    "converter": "com.foobar.store.ToJSONSeqConverter"
  },
  "type": "hdfs"
}

              
read({type: 'hdfs', location: 'jaqlTest/test3.dat', 
        inoptions: {converter: 'com.foobar.store.FromJSONSeqConverter'}});##
[
  {
    "one": "1"
  },
  {
    "two": "2"
  },
  {
    "three": "3"
  }
]


[{one: '1'}, {two: '2'}, {three: '3'}]
-> write({type: 'hdfs', location: 'jaqlTest/test3alt.dat', 
               outoptions: {converter: 'com.foobar.store.ToJSONSeqConverter',
                            configurator: 'com.foobar.store.TextFileOutputConfigurator'},
               inoptions: {converter: 'com.foobar.store.FromJSONSeqConverter'}})
-> read();##
[
  {
    "one": "1"
  },
  {
    "two": "2"
  },
  {
    "three": "3"
  }
]


// write/read Items to Text in a TextFile: specify ConverterAdapter, TextFormat, Item <-> Text (JSON)
[{one: '1'}, {two: '2'}, {three: '3'}]
-> write({type: 'hdfs', location: 'jaqlTest/test4.txt', 
         outoptions: {format: 'org.apache.hadoop.mapred.TextOutputFormat',
                      converter: 'com.foobar.store.ToJSONTxtConverter',
                      configurator: 'com.foobar.store.TextFileOutputConfigurator'}});##
{
  "location": "jaqlTest/test4.txt",
  "outoptions": {
    "configurator": "com.foobar.store.TextFileOutputConfigurator",
    "converter": "com.foobar.store.ToJSONTxtConverter",
    "format": "org.apache.hadoop.mapred.TextOutputFormat"
  },
  "type": "hdfs"
}

              
read({type: 'hdfs', location: 'jaqlTest/test4.txt',
	    inoptions: {format: 'org.apache.hadoop.mapred.TextInputFormat',
			  		converter: 'com.foobar.store.FromJSONTxtConverter'}});##
[
  {
    "one": "1"
  },
  {
    "two": "2"
  },
  {
    "three": "3"
  }
]


[{one: '1'}, {two: '2'}, {three: '3'}]
-> write({type: 'hdfs', location: 'jaqlTest/test4alt.txt', 
                outoptions: {format: 'org.apache.hadoop.mapred.TextOutputFormat',
                             converter: 'com.foobar.store.ToJSONTxtConverter',
                             configurator: 'com.foobar.store.TextFileOutputConfigurator'},
                inoptions: {format: 'org.apache.hadoop.mapred.TextInputFormat',
			  		        converter: 'com.foobar.store.FromJSONTxtConverter'}})
-> read();##
[
  {
    "one": "1"
  },
  {
    "two": "2"
  },
  {
    "three": "3"
  }
]


//-- hbase write/read expressions --

// write/read Items to an HBase Table as Items: default adapter, default format, no converter (see hbaseQueries.txt)
// stWrite({type: 'hbase', location: 'jaqlTesttest5'}, [{key: '1', one: '1'}, {key: '2', two: '2'}, {key: '3', three: '3'}]);

// stRead({type: 'hbase', location: 'jaqlTesttest5'});

// stRead(stWrite({type: 'hbase', location: 'jaqlTesttest5alt1'}, [{key: '1', one: '1'}, {key: '2', two: '2'}, {key: '3', three: '3'}]));

// can do the same thing if you pass in the right options to HadoopRead
// stWrite({location: 'jaqlTesttest5alt2', 
//          outoptions: {adapter: 'com.ibm.jaql.io.hadoop.DefaultHadoopOutputAdapter', 
//                       format: 'com.ibm.jaql.io.hbase.JaqlTableOutputFormat',
//                       configurator: 'com.ibm.jaql.io.hbase.TableOutputConfigurator'}}, 
//          [{key: '1', one: '1'}, {key: '2', two: '2'}, {key: '3', three: '3'}]);
                            
// stRead({location: 'jaqlTesttest5alt2',
// 	    inoptions: {adapter: 'com.ibm.jaql.io.hadoop.DefaultHadoopInputAdapter',
// 			  	    format: 'com.ibm.jaql.io.hbase.JaqlTableInputFormat',
// 			  	    configurator: 'com.ibm.jaql.io.hbase.TableInputConfigurator'}});

// stRead(stWrite({location: 'jaqlTesttest5alt3', 
//                 outoptions: {adapter: 'com.ibm.jaql.io.hadoop.DefaultHadoopOutputAdapter', 
//                              format: 'com.ibm.jaql.io.hbase.JaqlTableOutputFormat',
//                              configurator: 'com.ibm.jaql.io.hbase.TableOutputConfigurator'},
//                 inoptions: {adapter: 'com.ibm.jaql.io.hadoop.DefaultHadoopInputAdapter',
// 			  	            format: 'com.ibm.jaql.io.hbase.JaqlTableInputFormat',
// 			  	            configurator: 'com.ibm.jaql.io.hbase.TableInputConfigurator'}}, 
//               [{key: '1', one: '1'}, {key: '2', two: '2'}, {key: '3', three: '3'}]));

//-- local file read/write expressions

// write/read Items to/from a local file
[1,2,3]
-> write({location: 'build/test/cache/mynewfile.txt', 
         outoptions: {adapter : 'com.ibm.jaql.io.stream.FileStreamOutputAdapter',
                      format  : 'com.ibm.jaql.io.stream.converter.JsonOutputStream'}});##
{
  "location": "build/test/cache/mynewfile.txt",
  "outoptions": {
    "adapter": "com.ibm.jaql.io.stream.FileStreamOutputAdapter",
    "format": "com.ibm.jaql.io.stream.converter.JsonOutputStream"
  }
}


read({location: 'build/test/cache/mynewfile.txt', 
        inoptions: {adapter : 'com.ibm.jaql.io.stream.FileStreamInputAdapter',
                    format  : 'com.ibm.jaql.io.stream.converter.JsonInputStream'}});##
[
  1,
  2,
  3
]


[1,2,3]
-> localWrite({type: 'local', location: 'build/test/cache/myotherfile.txt'});##
{
  "location": "build/test/cache/myotherfile.txt",
  "type": "local"
}


read({type: 'local', location: 'build/test/cache/myotherfile.txt'});##
[
  1,
  2,
  3
]


//-- test map/reduce --

// input/output is hdfs sequence file of Items

// note, 'g' is quoted so that it can be used as a key for subsequent hbase tests
[
  { key: 0, g:"0", text: 'zero' },
  { key: 1, g:"1", text: 'one' },
  { key: 2, g:"0", text: 'two' },
  { key: 3, g:"1", text: 'three' },
  { key: 4, g:"0", text: 'four' },
  { key: 5, g:"1", text: 'five' },
  { key: 6, g:"0", text: 'six' },
  { key: 7, g:"1", text: 'seven' },
  { key: 8, g:"0", text: 'eight' },
]
-> write({ type: 'hdfs', location: 'jaqlTest/test6.dat'});##
{
  "location": "jaqlTest/test6.dat",
  "type": "hdfs"
}


read({type: 'hdfs', location: 'jaqlTest/test6.dat'});##
[
  {
    "g": "0",
    "key": 0,
    "text": "zero"
  },
  {
    "g": "1",
    "key": 1,
    "text": "one"
  },
  {
    "g": "0",
    "key": 2,
    "text": "two"
  },
  {
    "g": "1",
    "key": 3,
    "text": "three"
  },
  {
    "g": "0",
    "key": 4,
    "text": "four"
  },
  {
    "g": "1",
    "key": 5,
    "text": "five"
  },
  {
    "g": "0",
    "key": 6,
    "text": "six"
  },
  {
    "g": "1",
    "key": 7,
    "text": "seven"
  },
  {
    "g": "0",
    "key": 8,
    "text": "eight"
  }
]


// input is hdfs sequence file of Items, output is hbase table (note that 'g' renamed to 'key') (see hbaseQueries.txt)
// stWrite({type: 'hbase', location: 'jaqlTesttest6out'}, []);

//stRead( 
//  mapReduce( {
//    'input' : {type: 'hdfs', location: 'jaqlTest/test6.dat'},
//    'map'   : fn($i) [[ $i.g, 1 ]],
//    'reduce': fn($key, $values) [{ key: $key, n: count($values) }],
//    'output': {type: 'hbase', location: 'jaqlTesttest6out'}
//    })
// );
 
// input is hbase table, output is sequence file of Items
// stWrite({ type: 'hbase', location: 'jaqlTesttest7'}, [
//   { key: "0", g:0, text: 'zero' },
//   { key: "1", g:1, text: 'one' },
//   { key: "2", g:0, text: 'two' },
//   { key: "3", g:1, text: 'three' },
//   { key: "4", g:0, text: 'four' },
//   { key: "5", g:1, text: 'five' },
//   { key: "6", g:0, text: 'six' },
//   { key: "7", g:1, text: 'seven' },
//   { key: "8", g:0, text: 'eight' },
// ]);

// stRead( 
//   mapReduce( {
//     'input' : {type: 'hbase', location: 'jaqlTesttest7'},
//     'map'   : fn($i) [[ $i.g, 1 ]],
//     'reduce': fn($key, $values) [{ g: $key, n: count($values) }],
//     'output': {type: 'hdfs', location: 'jaqlTest/test7.dat'}
//     })
//  );
 
// input is a file with JSON text (one record per line), output is a SequenceFile of items
[
  { key: 0, g:0, text: 'zero' },
  { key: 1, g:1, text: 'one' },
  { key: 2, g:0, text: 'two' },
  { key: 3, g:1, text: 'three' },
  { key: 4, g:0, text: 'four' },
  { key: 5, g:1, text: 'five' },
  { key: 6, g:0, text: 'six' },
  { key: 7, g:1, text: 'seven' },
  { key: 8, g:0, text: 'eight' },
]
-> write({type: 'hdfs', location: 'jaqlTest/test8.txt', 
         outoptions: {format: 'org.apache.hadoop.mapred.TextOutputFormat',
                      converter: 'com.foobar.store.ToJSONTxtConverter',
                      configurator: 'com.foobar.store.TextFileOutputConfigurator'}});##
{
  "location": "jaqlTest/test8.txt",
  "outoptions": {
    "configurator": "com.foobar.store.TextFileOutputConfigurator",
    "converter": "com.foobar.store.ToJSONTxtConverter",
    "format": "org.apache.hadoop.mapred.TextOutputFormat"
  },
  "type": "hdfs"
}


read({type: 'hdfs', location: 'jaqlTest/test8.txt',
	    inoptions: {format: 'org.apache.hadoop.mapred.TextInputFormat',
			  		converter: 'com.foobar.store.FromJSONTxtConverter'}});##
[
  {
    "g": 0,
    "key": 0,
    "text": "zero"
  },
  {
    "g": 1,
    "key": 1,
    "text": "one"
  },
  {
    "g": 0,
    "key": 2,
    "text": "two"
  },
  {
    "g": 1,
    "key": 3,
    "text": "three"
  },
  {
    "g": 0,
    "key": 4,
    "text": "four"
  },
  {
    "g": 1,
    "key": 5,
    "text": "five"
  },
  {
    "g": 0,
    "key": 6,
    "text": "six"
  },
  {
    "g": 1,
    "key": 7,
    "text": "seven"
  },
  {
    "g": 0,
    "key": 8,
    "text": "eight"
  }
]

			  		    
mapReduce( {
    'input' : {type: 'hdfs', location: 'jaqlTest/test8.txt', 
               inoptions: {format: 'org.apache.hadoop.mapred.TextInputFormat',
			  		       converter: 'com.foobar.store.FromJSONTxtConverter'}},
    'map'   : fn($) ( $ -> transform [ $.g, 1 ]),
    'reduce': fn($key, $values) [{ g: $key, n: count($values) }],
    'output': {type: 'hdfs', location: 'jaqlTest/test8.dat'}
    })
-> read();##
[
  {
    "g": 0,
    "n": 5
  },
  {
    "g": 1,
    "n": 4
  }
]


//-- test composite input adapter (see hbaseQueries.txt)

// write out an hdfs file
[
  { key: "0", g:0, text: 'zero' },
  { key: "1", g:1, text: 'one' },
  { key: "2", g:0, text: 'two' },
  { key: "3", g:1, text: 'three' },
  { key: "4", g:0, text: 'four' }
]
-> write({ type: 'hdfs', location: 'jaqlTest/test9.dat'});##
{
  "location": "jaqlTest/test9.dat",
  "type": "hdfs"
}


// write out an hbase table
// stWrite({ type: 'hbase', location: 'jaqlTesttest9'}, [
//   { key: "5", g:1, text: 'five' },
//   { key: "6", g:0, text: 'six' },
//   { key: "7", g:1, text: 'seven' },
//   { key: "8", g:0, text: 'eight' },
// ]);

// stRead([{type: 'hdfs', location: 'jaqlTest/test9.dat'}, {type: 'hbase', location: 'jaqlTesttest9'}]);

// stRead(
//   mapReduce( {
//     'input': [ [{type: 'hdfs', location: 'jaqlTest/test9.dat'}, {type: 'hbase', location: 'jaqlTesttest9'}] ],
//      'map'   : fn($i) [[ $i.g, 1 ]],
//      'reduce': fn($key, $values) [{ g: $key, n: count($values) }],
//      'output': {type: 'hdfs', location: 'jaqlTest/test9out.dat'}
//    })
// );

//-- test co-group (see hbaseQueries.txt)

// stRead(mapReduce( {input: [ {type: 'hdfs', location: 'jaqlTest/test9.dat'}, 
//                             {type: 'hbase', location: 'jaqlTesttest9'}],
//                    map: [ fn($i) [[ $i.g, 1 ]], 
//                           fn($i) [[ $i.g, 1 ]] ],
//                    reduce: fn($key, $aVals, $bVals) [{ g: $key, as: count($aVals), bs: count($bVals) }],
//                    output: {type: 'hdfs', location: 'jaqlTest/test10out.dat'}}));

//-- test higher level interfaces

[1,2,3] -> localWrite(file('build/test/cache/test11a.dat'));##
{
  "location": "build/test/cache/test11a.dat",
  "type": "local"
}


read(file('build/test/cache/test11a.dat'));##
[
  1,
  2,
  3
]


[1,2,3] -> localWrite(file('build/test/cache/test11b.dat'));##
{
  "location": "build/test/cache/test11b.dat",
  "type": "local"
}


read(file('build/test/cache/test11b.dat'));##
[
  1,
  2,
  3
]


[1,2,3] -> write(hdfs('jaqlTest/test12a.dat'));##
{
  "location": "jaqlTest/test12a.dat",
  "type": "hdfs"
}


read(hdfs('jaqlTest/test12a.dat'));##
[
  1,
  2,
  3
]


[1,2,3] -> write(hdfs('jaqlTest/test12b.dat'));##
{
  "location": "jaqlTest/test12b.dat",
  "type": "hdfs"
}


read(hdfs('jaqlTest/test12b.dat'));##
[
  1,
  2,
  3
]


$path = "build/test/cache/";##
"$path"


$r1 = read(file($path + "books.json"));##
"$r1"


$r1 -> write(file($path + "b1"));##
{
  "location": "build/test/cache/b1",
  "type": "local"
}


read(file($path + "b1")) == read(file($path + "books.json"));##
true


$r1 -> write(file($path + "b2", {format: "com.ibm.jaql.io.stream.converter.JsonOutputStream"}));##
{
  "location": "build/test/cache/b2",
  "options": {
    "format": "com.ibm.jaql.io.stream.converter.JsonOutputStream"
  },
  "type": "local"
}


$r2 = read(file($path + "b2", {format: "com.ibm.jaql.io.stream.converter.JsonInputStream"}));##
"$r2"


$r1 == $r2;##
true


$r1 -> write(hdfs("jaqlTest/b1"));##
{
  "location": "jaqlTest/b1",
  "type": "hdfs"
}


$r1 == read(hdfs("jaqlTest/b1"));##
true


$v = {someField: "some value"};##
"$v"


[$v] -> write(file($path + "b2", {asArray: false}));##
{
  "location": "build/test/cache/b2",
  "options": {
    "asArray": false
  },
  "type": "local"
}


read(file($path + "b2", {asArray: false}))[0] == $v;##
true


hdfsShell("-copyFromLocal "+$path+"test.json jaqlTest/test") * 0;##
0


$r = read(hdfs("jaqlTest/test", {format: "org.apache.hadoop.mapred.TextInputFormat", converter: "com.ibm.jaql.io.hadoop.converter.FromJsonTextConverter"}));##
"$r"


$l = [
  {
    "a": "foo",
    "b": "bar"
  },
  {
    "a": 1,
    "b": 2,
    "c": 3
  }
];##
"$l"


$l -> write(hdfs("jaqlTest/test2", {format: "org.apache.hadoop.mapred.TextOutputFormat", converter: "com.ibm.jaql.io.hadoop.converter.ToJsonTextConverter"}));##
{
  "location": "jaqlTest/test2",
  "options": {
    "converter": "com.ibm.jaql.io.hadoop.converter.ToJsonTextConverter",
    "format": "org.apache.hadoop.mapred.TextOutputFormat"
  },
  "type": "hdfs"
}


$r -> write(hdfs("jaqlTest/test2", {format: "org.apache.hadoop.mapred.TextOutputFormat", converter: "com.ibm.jaql.io.hadoop.converter.ToJsonTextConverter"}));##
{
  "location": "jaqlTest/test2",
  "options": {
    "converter": "com.ibm.jaql.io.hadoop.converter.ToJsonTextConverter",
    "format": "org.apache.hadoop.mapred.TextOutputFormat"
  },
  "type": "hdfs"
}


$r2 = read(hdfs("jaqlTest/test2", {format: "org.apache.hadoop.mapred.TextInputFormat", converter: "com.ibm.jaql.io.hadoop.converter.FromJsonTextConverter"}));##
"$r2"


($r -> sort by [$]) == ($r2 -> sort by [$]);##
true


//$artist = "The Police";
//$url = "http://api.freebase.com/api/service/mqlread";
//$query = {query: {album: [], name: $artist, type: "/music/artist"}};
//httpGet($url, {query: serialize($query)}, {asArray: false})[0].result.album;
//read(http($url, {query: serialize($query)}))[0].result.album;

// write('hbase', 'jaqlTesttest13a.dat', [{key: '0', a: 'foo'}, {key: '1', b: 'bar'}]);

// read('hbase', 'jaqlTesttest13a.dat');

// hbaseWrite('jaqlTesttest13b.dat', [{key: '0', a: 'foo'}, {key: '1', b: 'bar'}]);

// hbaseRead('jaqlTesttest13b.dat');

//-- test array read
read({type: 'array', inoptions: {array: [1,2,3]}});##
[
  1,
  2,
  3
]


arrayRead([1,2,3]);##
[
  1,
  2,
  3
]


mapReduce( {
    'input': {type: 'array', inoptions: {array: [1,2,3]}},
    'map'   : fn($) ( $ -> transform [ null, $ ] ),
    'output': {type: 'hdfs', location: 'jaqlTest/test14out.dat'}
  })
-> read();##
[
  1,
  2,
  3
]


mapReduce( {
    'input': [ {type: 'array', inoptions: {array: [1,2,3]}},
               {type: 'array', inoptions: {array: [1,3,5]}} ],
    'map'   : [ fn($) ($ -> transform [ null, $ + 10 ]),
                fn($) ($ -> transform [ null, $ + 20 ]) ],
    'output': {type: 'hdfs', location: 'jaqlTest/test14out.dat'}
  })
-> read();##
[
  11,
  12,
  13,
  21,
  23,
  25
]


//-- test rewrites --

// group by over hdfs read
read({type: 'hdfs', location: 'jaqlTest/test6.dat'})
-> group by $a = ($.g)
    into { g:$a, i:count($) }
-> sort by [$.g];##
[
  {
    "g": "0",
    "i": 5
  },
  {
    "g": "1",
    "i": 4
  }
]


// group by over hbase read (see hbaseQueries.txt)
// group( $i in stRead({type: 'hbase', location: 'jaqlTesttest7'}) 
//        by $a = $i.g
//        into $is
//     [ { g:$a, i:count($is) } ];

// for loop over hdfs read
read({type: 'hdfs', location: 'jaqlTest/test6.dat'})
-> transform $.key;##
[
  0,
  1,
  2,
  3,
  4,
  5,
  6,
  7,
  8
]


// for loop over hbase read (see hbaseQueries.txt)
// for( $r in stRead({type: 'hbase', location: 'jaqlTesttest7'}) )
//   [ $r.key ];

// co-group (see hbaseQueries.txt)
// group(
//   $i in stRead({type: 'hdfs', location: 'jaqlTest/test9.dat'}) 
//      by $g = $i.g
//      into $as,
//   $j in stRead({type: 'hbase', location: 'jaqlTesttest9'}) 
//      by $g = $j.g
//      into $bs )
//  [ { g: $g, as: count($as), bs: count($bs) } ];
