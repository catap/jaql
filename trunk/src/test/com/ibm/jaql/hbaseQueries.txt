// the data
$data = [
  { key: "0", g:0, text: 'zero' },
  { key: "1", g:0, text: 'one' },
  { key: "2", g:0, text: 'two' },
  { key: "3", text: 'three' },
  { key: "4", g:0, text: 'four' },
  { key: "5", g:1, text: 'five' },
  { key: "6", g:0, text: 'six' },
  { key: "7", g:1, text: 'seven' },
  { key: "8", g:0, text: 'eight' }
];

// write it out
$data -> hbaseWrite('test1');

// read it in
hbaseRead('test1');

// fetch a value from one record
hbaseFetch('test1', ['5'], ['text']);

// fetch multiple values from one record
hbaseFetch('test1', ['5'], ['text', 'g']);

// fetch entire record
hbaseFetch('test1', ['5']);

// fetch a value from multiple records
hbaseFetch('test1', ['3', '7'], ['text']);

// fetch multiple values from multiple records
hbaseFetch('test1', ['3', '7'], ['text', 'g']);

// fetch multiple entire records
hbaseFetch('test1', ['3', '7']);

// delete a value from one record 
hbaseDelete('test1', ['5'], ['g']);

// fetch the deleted value
hbaseFetch('test1', ['5'], ['g']);

// fetch a record with a deleted value
hbaseFetch('test1', ['5']);

// delete a value from multiple records
hbaseDelete('test1', ['2', '8'], ['text']);

// fetch a deleted value
hbaseFetch('test1', ['2'], ['text']);

// fetch a record with a deleted value
hbaseFetch('test1', ['2']);

// Example 3. Write to an HBase table named 'webcrawl'. (from exampleQueries)
[
    {key: "www.cnn.com", page:'...', rank: 0.9,
     inlinks:[
       {link: 'www.news.com', anchor: 'newsite'},
       {link: 'www.jscript.com', anchor: 'look here'}]},
    {key: "www.json.org", page:'...', rank: 0.8}
]
-> hbaseWrite('webcrawl');
  
// Read it back... (from exampleQueries)
hbaseRead('webcrawl');

// the data (from exampleQueries)

$books = [
    {publisher: 'Scholastic',
     author: 'J. K. Rowling',
     title: 'Deathly Hallows',
     year: 2007},

    {publisher: 'Scholastic',
     author: 'J. K. Rowling',
     title: 'Chamber of Secrets',
     year: 1999, 
     reviews: [
       {rating: 10, user: 'joe', review: 'The best ...'},
       {rating: 6, user: 'mary', review: 'Average ...'}]},

    {publisher: 'Scholastic',
     author: 'J. K. Rowling',
     title: 'Sorcerers Stone',
     year: 1998},

    {publisher: 'Scholastic',
     author: 'R. L. Stine',
     title: 'Monster Blood IV',
     year: 1997, 
     reviews: [
       {rating: 8, user: 'rob', review: 'High on my list...'}, 
       {rating: 2, user: 'mike', review: 'Not worth the paper ...', 
        discussion:
          [{user: 'ben', text: 'This is too harsh...'}, 
           {user: 'jill', text: 'I agree ...'}]}]},

    {publisher: 'Grosset',
     author: 'Carolyn Keene',
     title: 'The Secret of Kane',
     year: 1930}
  ];
  
// (from exampleQueries)
$books
-> write({type:'hdfs', location:'example.jql', 
          outoptions:{format    : 'org.apache.hadoop.mapred.TextOutputFormat',
                      converter : 'com.ibm.jaql.io.hadoop.converter.ToJSONTxtConverter'}});
                              
// (from exampleQueries)
registerAdapter({type     :	'myHDFSFile',
                 inoptions:	{adapter      : 'com.ibm.jaql.io.hadoop.DefaultHadoopInputAdapter', 
                             format       : 'org.apache.hadoop.mapred.TextInputFormat', 
                             converter    : 'com.ibm.jaql.io.hadoop.converter.FromJSONTxtConverter',
                             configurator : 'com.ibm.jaql.io.hadoop.FileInputConfigurator'}});

// (from exampleQueries)       
[] -> hbaseWrite('example');

// (from exampleQueries)
read({type:'myHDFSFile', location:'example.jql'})
-> transform {key: $.publisher, ($.title): $.year}
-> hbaseWrite('example');

//-- hbase write/read expressions --

// write/read Items to an HBase Table as Items: default adapter, default format, no converter (from storageQueries)
[{key: '1', one: '1'}, {key: '2', two: '2'}, {key: '3', three: '3'}]
-> write({type: 'hbase', location: 'jaqlTesttest5'});

read({type: 'hbase', location: 'jaqlTesttest5'});

[{key: '1', one: '1'}, {key: '2', two: '2'}, {key: '3', three: '3'}]
-> write({type: 'hbase', location: 'jaqlTesttest5alt1'})
-> read();

// can do the same thing if you pass in the right options to HadoopRead
[{key: '1', one: '1'}, {key: '2', two: '2'}, {key: '3', three: '3'}]
-> write({location: 'jaqlTesttest5alt2', 
         outoptions: {adapter: 'com.ibm.jaql.io.hadoop.DefaultHadoopOutputAdapter', 
                      format: 'com.ibm.jaql.io.hbase.JaqlTableOutputFormat',
                      configurator: 'com.ibm.jaql.io.hbase.TableOutputConfigurator'}});
                            
read({location: 'jaqlTesttest5alt2',
	    inoptions: {adapter: 'com.ibm.jaql.io.hadoop.DefaultHadoopInputAdapter',
			  	    format: 'com.ibm.jaql.io.hbase.JaqlTableInputFormat',
			  	    configurator: 'com.ibm.jaql.io.hbase.TableInputConfigurator'}});

[{key: '1', one: '1'}, {key: '2', two: '2'}, {key: '3', three: '3'}]
-> write({location: 'jaqlTesttest5alt3', 
                outoptions: {adapter: 'com.ibm.jaql.io.hadoop.DefaultHadoopOutputAdapter', 
                             format: 'com.ibm.jaql.io.hbase.JaqlTableOutputFormat',
                             configurator: 'com.ibm.jaql.io.hbase.TableOutputConfigurator'},
                inoptions: {adapter: 'com.ibm.jaql.io.hadoop.DefaultHadoopInputAdapter',
			  	            format: 'com.ibm.jaql.io.hbase.JaqlTableInputFormat',
			  	            configurator: 'com.ibm.jaql.io.hbase.TableInputConfigurator'}})
-> read();
              
// input is hdfs sequence file of Items, output is hbase table (note that 'g' renamed to 'key') (from storageQueries.txt)
[] -> write({type: 'hbase', location: 'jaqlTesttest6out'});

// note, 'g' is quoted so that it can be used as a key for subsequent hbase tests (from storageQueries.txt)
[
  { key: 0, g:"0", text: 'zero' },
  { key: 1, g:"1", text: 'one' },
  { key: 2, g:"0", text: 'two' },
  { key: 3, g:"1", text: 'three' },
  { key: 4, g:"0", text: 'four' },
  { key: 5, g:"1", text: 'five' },
  { key: 6, g:"0", text: 'six' },
  { key: 7, g:"1", text: 'seven' },
  { key: 8, g:"0", text: 'eight' },
]
-> write({ type: 'hdfs', location: 'jaqlTest/test6.dat'});

mapReduce( {
   'input' : {type: 'hdfs', location: 'jaqlTest/test6.dat'},
   'map'   : fn($) ( $ -> transform [ $.g, 1 ] ),
   'reduce': fn($key, $values) [{ key: $key, n: count($values) }],
   'output': {type: 'hbase', location: 'jaqlTesttest6out'}
   })
-> read();
 
// input is hbase table, output is sequence file of Items
[
  { key: "0", g:0, text: 'zero' },
  { key: "1", g:1, text: 'one' },
  { key: "2", g:0, text: 'two' },
  { key: "3", g:1, text: 'three' },
  { key: "4", g:0, text: 'four' },
  { key: "5", g:1, text: 'five' },
  { key: "6", g:0, text: 'six' },
  { key: "7", g:1, text: 'seven' },
  { key: "8", g:0, text: 'eight' },
]
-> write({ type: 'hbase', location: 'jaqlTesttest7'});

mapReduce( {
    'input' : {type: 'hbase', location: 'jaqlTesttest7'},
    'map'   : fn($) ( $ -> transform [ $.g, 1 ] ),
    'reduce': fn($key, $values) [{ g: $key, n: count($values) }],
    'output': {type: 'hdfs', location: 'jaqlTest/test7.dat'}
    })
-> read();
 
//-- test composite input adapter (from storageQueries.txt)

// write out an hdfs file
[
  { key: "0", g:0, text: 'zero' },
  { key: "1", g:1, text: 'one' },
  { key: "2", g:0, text: 'two' },
  { key: "3", g:1, text: 'three' },
  { key: "4", g:0, text: 'four' }
]
-> write({ type: 'hdfs', location: 'jaqlTest/test9.dat'});

// write out an hbase table
[
  { key: "5", g:1, text: 'five' },
  { key: "6", g:0, text: 'six' },
  { key: "7", g:1, text: 'seven' },
  { key: "8", g:0, text: 'eight' },
]
-> write({ type: 'hbase', location: 'jaqlTesttest9'});

read([{type: 'hdfs', location: 'jaqlTest/test9.dat'}, {type: 'hbase', location: 'jaqlTesttest9'}]);

mapReduce( {
    'input': [ [{type: 'hdfs', location: 'jaqlTest/test9.dat'}, {type: 'hbase', location: 'jaqlTesttest9'}] ],
     'map'   : fn($) ( $ -> transform [ $.g, 1 ] ),
     'reduce': fn($key, $values) [{ g: $key, n: count($values) }],
     'output': {type: 'hdfs', location: 'jaqlTest/test9out.dat'}
   })
-> read();

//-- test co-group (from storageQueries.txt)

mapReduce( {input: [ {type: 'hdfs', location: 'jaqlTest/test9.dat'}, 
                            {type: 'hbase', location: 'jaqlTesttest9'}],
                   map: [ fn($) ( $ -> transform [ $.g, 1 ] ),
                          fn($) ( $ -> transform [ $.g, 1 ] ) ],
                   reduce: fn($key, $aVals, $bVals) [ { g: $key, as: count($aVals), bs: count($bVals) } ],
                   output: {type: 'hdfs', location: 'jaqlTest/test10out.dat'}})
-> read();

// (from storageQueries.txt)
[{key: '0', a: 'foo'}, {key: '1', b: 'bar'}]
-> write({type:'hbase', location:'jaqlTesttest13a.dat'});

// (from storageQueries.txt)
read({type:'hbase', location:'jaqlTesttest13a.dat'});

// (from storageQueries.txt)                   
[{key: '0', a: 'foo'}, {key: '1', b: 'bar'}]
-> hbaseWrite('jaqlTesttest13b.dat');

// (from storageQueries.txt)
hbaseRead('jaqlTesttest13b.dat');

// group by over hbase read (from storageQueries.txt)
read({type: 'hbase', location: 'jaqlTesttest7'}) 
-> group by $a = ($.g)
    into { g:$a, i:count($) };

// for loop over hbase read (from storageQueries.txt)
read({type: 'hbase', location: 'jaqlTesttest7'})
-> transform $.key;

// co-group (from storageQueries.txt)
group
  read({type: 'hdfs', location: 'jaqlTest/test9.dat'}) 
     by $g = ($.g) as $i,
  read({type: 'hbase', location: 'jaqlTesttest9'}) 
     by $g = ($.g) as $j
 into { g: $g, as: count($i), bs: count($j) };
