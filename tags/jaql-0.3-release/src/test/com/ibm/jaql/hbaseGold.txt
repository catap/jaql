// the data
$data = [
  { key: "0", g:0, text: 'zero' },
  { key: "1", g:0, text: 'one' },
  { key: "2", g:0, text: 'two' },
  { key: "3", text: 'three' },
  { key: "4", g:0, text: 'four' },
  { key: "5", g:1, text: 'five' },
  { key: "6", g:0, text: 'six' },
  { key: "7", g:1, text: 'seven' },
  { key: "8", g:0, text: 'eight' }
];##
"$data"


// write it out
hbaseWrite('test1', $data);##
{
  "location": "test1",
  "type": "hbase"
}


// read it in
hbaseRead('test1');##
[
  {
    "g": 0,
    "key": "0",
    "text": "zero"
  },
  {
    "g": 0,
    "key": "1",
    "text": "one"
  },
  {
    "g": 0,
    "key": "2",
    "text": "two"
  },
  {
    "key": "3",
    "text": "three"
  },
  {
    "g": 0,
    "key": "4",
    "text": "four"
  },
  {
    "g": 1,
    "key": "5",
    "text": "five"
  },
  {
    "g": 0,
    "key": "6",
    "text": "six"
  },
  {
    "g": 1,
    "key": "7",
    "text": "seven"
  },
  {
    "g": 0,
    "key": "8",
    "text": "eight"
  }
]


// fetch a value from one record
hbaseFetch('test1', ['5'], ['text']);##
[
  {
    "key": "5",
    "text": "five"
  }
]


// fetch multiple values from one record
hbaseFetch('test1', ['5'], ['text', 'g']);##
[
  {
    "g": 1,
    "key": "5",
    "text": "five"
  }
]


// fetch entire record
hbaseFetch('test1', ['5']);##
[
  {
    "g": 1,
    "key": "5",
    "text": "five"
  }
]


// fetch a value from multiple records
hbaseFetch('test1', ['3', '7'], ['text']);##
[
  {
    "key": "3",
    "text": "three"
  },
  {
    "key": "7",
    "text": "seven"
  }
]


// fetch multiple values from multiple records
hbaseFetch('test1', ['3', '7'], ['text', 'g']);##
[
  {
    "key": "3",
    "text": "three"
  },
  {
    "g": 1,
    "key": "7",
    "text": "seven"
  }
]


// fetch multiple entire records
hbaseFetch('test1', ['3', '7']);##
[
  {
    "key": "3",
    "text": "three"
  },
  {
    "g": 1,
    "key": "7",
    "text": "seven"
  }
]


// delete a value from one record 
hbaseDelete('test1', ['5'], ['g']);##
true


// fetch the deleted value
hbaseFetch('test1', ['5'], ['g']);##
[]


// fetch a record with a deleted value
hbaseFetch('test1', ['5']);##
[
  {
    "key": "5",
    "text": "five"
  }
]


// delete a value from multiple records
hbaseDelete('test1', ['2', '8'], ['text']);##
true


// fetch a deleted value
hbaseFetch('test1', ['2'], ['text']);##
[]


// fetch a record with a deleted value
hbaseFetch('test1', ['2']);##
[
  {
    "g": 0,
    "key": "2"
  }
]


// Example 3. Write to an HBase table named 'webcrawl'. (from exampleQueries)
hbaseWrite('webcrawl', [
    {key: "www.cnn.com", page:'...', rank: 0.9,
     inlinks:[
       {link: 'www.news.com', anchor: 'newsite'},
       {link: 'www.jscript.com', anchor: 'look here'}]},
    {key: "www.json.org", page:'...', rank: 0.8}
  ]);##
{
  "location": "webcrawl",
  "type": "hbase"
}

  
// Read it back... (from exampleQueries)
hbaseRead('webcrawl');##
[
  {
    "inlinks": [
      {
        "anchor": "newsite",
        "link": "www.news.com"
      },
      {
        "anchor": "look here",
        "link": "www.jscript.com"
      }
    ],
    "key": "www.cnn.com",
    "page": "...",
    "rank": 0.9
  },
  {
    "key": "www.json.org",
    "page": "...",
    "rank": 0.8
  }
]


// the data (from exampleQueries)

$books = [
    {publisher: 'Scholastic',
     author: 'J. K. Rowling',
     title: 'Deathly Hallows',
     year: 2007},

    {publisher: 'Scholastic',
     author: 'J. K. Rowling',
     title: 'Chamber of Secrets',
     year: 1999, 
     reviews: [
       {rating: 10, user: 'joe', review: 'The best ...'},
       {rating: 6, user: 'mary', review: 'Average ...'}]},

    {publisher: 'Scholastic',
     author: 'J. K. Rowling',
     title: 'Sorcerers Stone',
     year: 1998},

    {publisher: 'Scholastic',
     author: 'R. L. Stine',
     title: 'Monster Blood IV',
     year: 1997, 
     reviews: [
       {rating: 8, user: 'rob', review: 'High on my list...'}, 
       {rating: 2, user: 'mike', review: 'Not worth the paper ...', 
        discussion:
          [{user: 'ben', text: 'This is too harsh...'}, 
           {user: 'jill', text: 'I agree ...'}]}]},

    {publisher: 'Grosset',
     author: 'Carolyn Keene',
     title: 'The Secret of Kane',
     year: 1930}
  ];##
"$books"

  
// (from exampleQueries)
write('hdfs', 'example.jql', {format    : 'org.apache.hadoop.mapred.TextOutputFormat',
                              converter : 'com.acme.extensions.data.ToJSONTxtConverter'}, $books);##
{
  "location": "example.jql",
  "outoptions": {
    "adapter": "com.ibm.jaql.io.hadoop.DefaultHadoopOutputAdapter",
    "configurator": "com.ibm.jaql.io.hadoop.FileOutputConfigurator",
    "converter": "com.acme.extensions.data.ToJSONTxtConverter",
    "format": "org.apache.hadoop.mapred.TextOutputFormat"
  },
  "type": "hdfs"
}

                              
// (from exampleQueries)
registerAdapter({type     :	'myHDFSFile',
                 inoptions:	{adapter      : 'com.ibm.jaql.io.hadoop.DefaultHadoopInputAdapter', 
                             format       : 'org.apache.hadoop.mapred.TextInputFormat', 
                             converter    : 'com.acme.extensions.data.FromJSONTxtConverter',
                             configurator : 'com.ibm.jaql.io.hadoop.FileInputConfigurator'}});##
{
  "inoptions": {
    "adapter": "com.ibm.jaql.io.hadoop.DefaultHadoopInputAdapter",
    "configurator": "com.ibm.jaql.io.hadoop.FileInputConfigurator",
    "converter": "com.acme.extensions.data.FromJSONTxtConverter",
    "format": "org.apache.hadoop.mapred.TextInputFormat"
  },
  "type": "myHDFSFile"
}


// (from exampleQueries)                             
$q = for( $i in read('myHDFSFile', 'example.jql') )
       [ {key: $i.publisher, ($i.title): $i.year} ];##
"$q"


// (from exampleQueries)       
hbaseWrite('example', []);##
{
  "location": "example",
  "type": "hbase"
}


// (from exampleQueries)
hbaseWrite('example', $q);##
{
  "location": "example",
  "type": "hbase"
}


//-- hbase write/read expressions --

// write/read Items to an HBase Table as Items: default adapter, default format, no converter (from storageQueries)
stWrite({type: 'hbase', location: 'jaqlTesttest5'}, [{key: '1', one: '1'}, {key: '2', two: '2'}, {key: '3', three: '3'}]);##
{
  "location": "jaqlTesttest5",
  "type": "hbase"
}


stRead({type: 'hbase', location: 'jaqlTesttest5'});##
[
  {
    "key": "1",
    "one": "1"
  },
  {
    "key": "2",
    "two": "2"
  },
  {
    "key": "3",
    "three": "3"
  }
]


stRead(stWrite({type: 'hbase', location: 'jaqlTesttest5alt1'}, [{key: '1', one: '1'}, {key: '2', two: '2'}, {key: '3', three: '3'}]));##
[
  {
    "key": "1",
    "one": "1"
  },
  {
    "key": "2",
    "two": "2"
  },
  {
    "key": "3",
    "three": "3"
  }
]


// can do the same thing if you pass in the right options to HadoopRead
stWrite({location: 'jaqlTesttest5alt2', 
         outoptions: {adapter: 'com.ibm.jaql.io.hadoop.DefaultHadoopOutputAdapter', 
                      format: 'com.ibm.jaql.io.hbase.JaqlTableOutputFormat',
                      configurator: 'com.ibm.jaql.io.hbase.TableOutputConfigurator'}}, 
         [{key: '1', one: '1'}, {key: '2', two: '2'}, {key: '3', three: '3'}]);##
{
  "location": "jaqlTesttest5alt2",
  "outoptions": {
    "adapter": "com.ibm.jaql.io.hadoop.DefaultHadoopOutputAdapter",
    "configurator": "com.ibm.jaql.io.hbase.TableOutputConfigurator",
    "format": "com.ibm.jaql.io.hbase.JaqlTableOutputFormat"
  }
}

                            
stRead({location: 'jaqlTesttest5alt2',
	    inoptions: {adapter: 'com.ibm.jaql.io.hadoop.DefaultHadoopInputAdapter',
			  	    format: 'com.ibm.jaql.io.hbase.JaqlTableInputFormat',
			  	    configurator: 'com.ibm.jaql.io.hbase.TableInputConfigurator'}});##
[
  {
    "key": "1",
    "one": "1"
  },
  {
    "key": "2",
    "two": "2"
  },
  {
    "key": "3",
    "three": "3"
  }
]


stRead(stWrite({location: 'jaqlTesttest5alt3', 
                outoptions: {adapter: 'com.ibm.jaql.io.hadoop.DefaultHadoopOutputAdapter', 
                             format: 'com.ibm.jaql.io.hbase.JaqlTableOutputFormat',
                             configurator: 'com.ibm.jaql.io.hbase.TableOutputConfigurator'},
                inoptions: {adapter: 'com.ibm.jaql.io.hadoop.DefaultHadoopInputAdapter',
			  	            format: 'com.ibm.jaql.io.hbase.JaqlTableInputFormat',
			  	            configurator: 'com.ibm.jaql.io.hbase.TableInputConfigurator'}}, 
              [{key: '1', one: '1'}, {key: '2', two: '2'}, {key: '3', three: '3'}]));##
[
  {
    "key": "1",
    "one": "1"
  },
  {
    "key": "2",
    "two": "2"
  },
  {
    "key": "3",
    "three": "3"
  }
]

              
// input is hdfs sequence file of Items, output is hbase table (note that 'g' renamed to 'key') (from storageQueries.txt)
stWrite({type: 'hbase', location: 'jaqlTesttest6out'}, []);##
{
  "location": "jaqlTesttest6out",
  "type": "hbase"
}


// note, 'g' is quoted so that it can be used as a key for subsequent hbase tests (from storageQueries.txt)
stWrite({ type: 'hdfs', location: 'jaqlTest/test6.dat'}, [
  { key: 0, g:"0", text: 'zero' },
  { key: 1, g:"1", text: 'one' },
  { key: 2, g:"0", text: 'two' },
  { key: 3, g:"1", text: 'three' },
  { key: 4, g:"0", text: 'four' },
  { key: 5, g:"1", text: 'five' },
  { key: 6, g:"0", text: 'six' },
  { key: 7, g:"1", text: 'seven' },
  { key: 8, g:"0", text: 'eight' },
]);##
{
  "location": "jaqlTest/test6.dat",
  "type": "hdfs"
}


stRead( 
  mapReduce( {
   'input' : {type: 'hdfs', location: 'jaqlTest/test6.dat'},
   'map'   : fn($i) [ [ $i.g, 1 ] ],
   'reduce': fn($key, $values) [{ key: $key, n: count($values) }],
   'output': {type: 'hbase', location: 'jaqlTesttest6out'}
   })
);##
[
  {
    "key": "0",
    "n": 5
  },
  {
    "key": "1",
    "n": 4
  }
]

 
// input is hbase table, output is sequence file of Items
stWrite({ type: 'hbase', location: 'jaqlTesttest7'}, [
  { key: "0", g:0, text: 'zero' },
  { key: "1", g:1, text: 'one' },
  { key: "2", g:0, text: 'two' },
  { key: "3", g:1, text: 'three' },
  { key: "4", g:0, text: 'four' },
  { key: "5", g:1, text: 'five' },
  { key: "6", g:0, text: 'six' },
  { key: "7", g:1, text: 'seven' },
  { key: "8", g:0, text: 'eight' },
]);##
{
  "location": "jaqlTesttest7",
  "type": "hbase"
}


stRead( 
  mapReduce( {
    'input' : {type: 'hbase', location: 'jaqlTesttest7'},
    'map'   : fn($i) [ [ $i.g, 1 ] ],
    'reduce': fn($key, $values) [{ g: $key, n: count($values) }],
    'output': {type: 'hdfs', location: 'jaqlTest/test7.dat'}
    })
 );##
[
  {
    "g": 0,
    "n": 5
  },
  {
    "g": 1,
    "n": 4
  }
]

 
//-- test composite input adapter (from storageQueries.txt)

// write out an hdfs file
stWrite({ type: 'hdfs', location: 'jaqlTest/test9.dat'}, [
  { key: "0", g:0, text: 'zero' },
  { key: "1", g:1, text: 'one' },
  { key: "2", g:0, text: 'two' },
  { key: "3", g:1, text: 'three' },
  { key: "4", g:0, text: 'four' }
]);##
{
  "location": "jaqlTest/test9.dat",
  "type": "hdfs"
}


// write out an hbase table
stWrite({ type: 'hbase', location: 'jaqlTesttest9'}, [
  { key: "5", g:1, text: 'five' },
  { key: "6", g:0, text: 'six' },
  { key: "7", g:1, text: 'seven' },
  { key: "8", g:0, text: 'eight' },
]);##
{
  "location": "jaqlTesttest9",
  "type": "hbase"
}


stRead([{type: 'hdfs', location: 'jaqlTest/test9.dat'}, {type: 'hbase', location: 'jaqlTesttest9'}]);##
[
  {
    "g": 0,
    "key": "0",
    "text": "zero"
  },
  {
    "g": 1,
    "key": "1",
    "text": "one"
  },
  {
    "g": 0,
    "key": "2",
    "text": "two"
  },
  {
    "g": 1,
    "key": "3",
    "text": "three"
  },
  {
    "g": 0,
    "key": "4",
    "text": "four"
  },
  {
    "g": 1,
    "key": "5",
    "text": "five"
  },
  {
    "g": 0,
    "key": "6",
    "text": "six"
  },
  {
    "g": 1,
    "key": "7",
    "text": "seven"
  },
  {
    "g": 0,
    "key": "8",
    "text": "eight"
  }
]


stRead(
  mapReduce( {
    'input': [ [{type: 'hdfs', location: 'jaqlTest/test9.dat'}, {type: 'hbase', location: 'jaqlTesttest9'}] ],
     'map'   : fn($i) [ [ $i.g, 1 ] ],
     'reduce': fn($key, $values) [{ g: $key, n: count($values) }],
     'output': {type: 'hdfs', location: 'jaqlTest/test9out.dat'}
   })
);##
[
  {
    "g": 0,
    "n": 5
  },
  {
    "g": 1,
    "n": 4
  }
]


//-- test co-group (from storageQueries.txt)

stRead(mapReduce( {input: [ {type: 'hdfs', location: 'jaqlTest/test9.dat'}, 
                            {type: 'hbase', location: 'jaqlTesttest9'}],
                   map: [ fn($i) [ [ $i.g, 1 ] ],
                          fn($i) [ [ $i.g, 1 ] ] ],
                   reduce: fn($key, $aVals, $bVals) [ { g: $key, as: count($aVals), bs: count($bVals) } ],
                   output: {type: 'hdfs', location: 'jaqlTest/test10out.dat'}}));##
[
  {
    "as": 3,
    "bs": 2,
    "g": 0
  },
  {
    "as": 2,
    "bs": 2,
    "g": 1
  }
]


// (from storageQueries.txt)
write('hbase', 'jaqlTesttest13a.dat', [{key: '0', a: 'foo'}, {key: '1', b: 'bar'}]);##
{
  "location": "jaqlTesttest13a.dat",
  "type": "hbase"
}


// (from storageQueries.txt)
read('hbase', 'jaqlTesttest13a.dat');##
[
  {
    "a": "foo",
    "key": "0"
  },
  {
    "b": "bar",
    "key": "1"
  }
]


// (from storageQueries.txt)                   
hbaseWrite('jaqlTesttest13b.dat', [{key: '0', a: 'foo'}, {key: '1', b: 'bar'}]);##
{
  "location": "jaqlTesttest13b.dat",
  "type": "hbase"
}


// (from storageQueries.txt)
hbaseRead('jaqlTesttest13b.dat');##
[
  {
    "a": "foo",
    "key": "0"
  },
  {
    "b": "bar",
    "key": "1"
  }
]


// group by over hbase read (from storageQueries.txt)
group( $i in stRead({type: 'hbase', location: 'jaqlTesttest7'}) 
       by $a = $i.g
       into $is )
 [ { g:$a, i:count($is) } ];##
[
  {
    "g": 0,
    "i": 5
  },
  {
    "g": 1,
    "i": 4
  }
]


// for loop over hbase read (from storageQueries.txt)
for( $r in stRead({type: 'hbase', location: 'jaqlTesttest7'}) )
  [ $r.key ];##
[
  "0",
  "1",
  "2",
  "3",
  "4",
  "5",
  "6",
  "7",
  "8"
]


// co-group (from storageQueries.txt)
group(
  $i in stRead({type: 'hdfs', location: 'jaqlTest/test9.dat'}) 
     by $g = $i.g
     into $as,
  $j in stRead({type: 'hbase', location: 'jaqlTesttest9'}) 
     by $g = $j.g
     into $bs )
 [ { g: $g, as: count($as), bs: count($bs) } ];##
[
  {
    "as": 3,
    "bs": 2,
    "g": 0
  },
  {
    "as": 2,
    "bs": 2,
    "g": 1
  }
]
