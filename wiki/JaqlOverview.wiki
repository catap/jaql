#summary Jaql: A JSON Query Language

= Introduction =

In this document, we introduce Jaql, a query language for 
!JavaScript Object Notation or [http://www.json.org/ JSON].
Although Jaql has been designed specifically for JSON,
we have tried to borrow some of the best features of
[http://en.wikipedia.org/wiki/SQL SQL], [http://www.w3.org/TR/xquery/ XQuery], LISP, and
[http://wiki.apache.org/pig/PigLatin PigLatin].
Our high-level design objectives include:

  * Semi-structured analytics: easy manipulation and analysis of JSON data
  * Parallelism: Jaql queries that process large amounts of data must be able to take advantage of scaled-out architectures
  * Extensibility: users must be able to easily extend Jaql

We begin with an example of JSON data, then go on to describe
the key features of Jaql and show how it can be used to process
JSON data in parallel using Hadoop's map/reduce framework.
Along the way, we also show how Jaql's [Functions function]
and [IO] libraries can be extended. To get started with the examples below,
you can [http://jaql.googlecode.com/files/jaql-0.4_hadoop-0.18.3.tgz download] the most recent release of Jaql and fire up a Jaql interpreter following these [Running instructions].

_Note:_ Jaql is still in early development, 
so beware that it is likely to change over the next few months.
The future development plans are outlined in the [RoadMap roadmap].

= A JSON Example =

Let's start off with an example of an application log: 

{{{
%cat log.json
[
  { from: 101, 
    to: [102],
    ts: 1243361567,
    msg: "Hello, world!" 
  },
  { from: 201, 
    to: [20, 81, 94],
    ts: 1243361567,
    msg: "Hello, world! was interesting, but lets start a new topic please" 
  },
  { from: 81, 
    to: [201, 94 40],
    ts: 1243361567,
    msg: "Agreed, this topic is not for Joe, but more suitable for Ann" 
  },
  { from: 40, 
    to: [201, 81, 94],
    ts: 1243361567,
    msg: "Thanks for including me on this topic about nothing... reminds me of a Seinfeld episode." 
  },
  { from: 20, 
    to: [81, 201, 94],
    ts: 1243361567,
    msg: "Where did the topic go.. hopefully its more than about nothing." 
  }  
]
}}}

and its associated user data:

{{{
% cat user.json
[
  { id: 20,
    name: "Joe Smith",
    zip: 95120
  },
  { id: 40,
    name: "Ann Jones",
    zip: 94114
  },
  { id: 101,
    name: "Alicia Fox",
    zip: 95008
  },
  { id: 201,
    name: "Mike James",
    zip: 94114
  },
  { id: 102,
    name: "Adam Baker",
    zip: 94114
  },
  { id: 81,
    name: "Beth Charles",
    zip: 95008
  },
  { id: 94,
    name: "Charles Dodd",
    zip: 95120
  },
  { id: 103,
    name: "Dan Epstein",
    zip: 95008
  }
]
}}}

Both data sets are assumed to be stored in separate, local files. Each file consists
of an _array_, deliminted by brackets '[]', of JSON objects (or records), delimited by braces '{}'. Objects contain name:value pairs or _members_, where the
value can be an atomic type or a nested value.

_Note:_ while Jaql can consume strict JSON, it can also consume minor variants of JSON. In the examples, for the sake of clarity, we omit double-quotes around field names and use single-quotes in place of double-quotes for strings.

= The Jaql Query Language =

Jaql is a functional query language that provides users with a simple,
declarative syntax to do things like filter, join, and group JSON data.
Jaql also allows user-defined functions to be written and used in expressions. 
Let's begin with some examples using the log and user data presented earlier:
Our first task is to load the locally stored JSON data into Hadoop's file system, HDFS
(We store data in HDFS since it can processed in parallel using map-reduce).

{{{
  // load log data
  read(file("log.json")) -> write(hdfs("log"));

  // load user data
  read(file("user.json")) -> write(hdfs("user"));
}}}

*read* is an example of a _source_ whereas *write* is an example of a sink. In the example, we've constructed a simple pipe to read from a local file and write to an
hdfs file. For a more comprehensive discussion on how sources and sinks can be parameterized and extended, refer to [IO].

Our first query illustrates simple filtering and projection:

{{{
  //
  // Bind to variable
  $log  = read(hdfs("log"));
  $user = read(hdfs("user"));

  //
  // Query 1: filter and transform
  $log
  -> filter $.from == 101
  -> transform { mandatory: $.msg };

  // result ...
  [
    {
      "mandatory": "Hello, world!"
    }
  ]
}}}

First we assign a variable to both the log and user data in hdfs.
Then, we start a new Jaql pipe with the log data. A pipe expects an
array as input and conceptually streams the array's values to its consumer.
In Query 1, the first consumer is a *filter* operator that outputs only those
values for which the predicate evaluates to true. In this case, the predicate
tests the *from* field of each object. The *$* is an implicitly defined variable
that references the _current_ array value. Note that the *.* used in *$.from* assumes
that each array value is an object and not some other type of JSON value.

The second consumer is a *transform* operator that takes as input a JSON value
and outputs a JSON value. In this case, a new object is constructed to project the
*msg* field. The sink for Query1 writes to the screen. For more details on Jaql's core language features, refer to [LanguageCore].

While Query 1 is reasonably straightforward, it is not reusable. For every combination of
source, sink, and id (e.g., from), a new Jaql pipe must be defined. Jaql supports functions in the language in order to increase re-usability. Consider defining Query 1 as a function, then calling it:

{{{
  //
  // Define Query 1 as a function
  $introMessage = 
  fn($input, $id) (
    $input
    -> filter $.from == $id
    -> transform { mandatory: $.msg }
  );

  // Call Query1
  read(hdfs("log")) 
  -> $introMessage(101);

  // result ...
  [
    {
      "mandatory": "Hello, world!"
    }
  ]
}}}

The first statement defines the variable *$introMessage* as a function of two arguments, namely an *$input* and an *$id* to use for the filter. The second statement shows how to call the newly defined function in the context of a pipe. Log data is used as a source and is implicitly bound to the first agument of the $introMessage function ($input). Using functions allows us to parameterize Query1's filter, use any source that may store log data, and use any sink to consume its output. The following example uses an hdfs sink instead of the screen:

{{{
  read(hdfs("log"))
  -> $introMessage(101)
  -> write(hdfs("greeting"));

  // result ...
  [
    {
      "mandatory": "Hello, world!"
    }
  ]
}}}

For Query 2, we show how to compute *word-count*, the canonical map-reduce example, on the log *msg* field. First we introduce the *expand* operators which will be used to create a single array of strings, and distinguish it from the *transform* operator:

{{{
  $example = [ 
    ["first", "example"],
    ["second", "example"]
  ];
  $example -> transform $;
  // results ...
  [
    [
      "first",
      "example"
    ],
    [
      "second",
      "example"
    ]
  ]

  $example -> expand $;
  // results ...
  [
    "first",
    "example",
    "second",
    "example"
  ]
}}}

*$example* is an array of array of strings, exactly what tokenization will produce for each *msg*. The *transform* operators takes each array child as input, in this case a nested array, and places a single JSON value back into the output array. This leaves us with a nested array of strings whereas word-count operates on a single array of strings. The *expand* operator in contrast expects an array as input, in this case the nested array, and places each of the nested array's children into the output array, thus producing a single array of strings.

{{{

  // register the splitArr function
  registerFunction("splitArr", "com.acme.extensions.expr.SplitIterExpr");
  splitArr("something simple with five words", " ");

  // results ...
  [
    "something",
    "simple",
    "with",
    "five",
    "words"
  ]

  // Query 2: word count across all messages
  $log
  -> expand splitArr($.msg, " ")
  -> group by $word = $
      into { $word, num: count($) };

  // results ...
  [
    {
      "num": 1,
      "word": "Agreed,"
    },
    {
      "num": 1,
      "word": "Ann"
    },
    {
      "num": 2,
      "word": "Hello,"
    },
    {
      "num": 1,
      "word": "Joe,"
    },
    {
      "num": 1,
      "word": "Seinfeld"
    },
    ...
  ]
}}}

{{{
  $inList = ["topic", "Seinfeld"];

  // Query 3: word-count for specific words
  $log
  -> expand (splitArr($.msg, " ") -> filter $ in $inList )
  -> group by $word = $
      into { $word, num: count($) }
  -> write(hdfs("filteredWordCount"));
}}}

{{{
  //
  // Query 4: word-count for specific words per sender
  $log
  -> expand each $l
       ( splitArr($l.msg, " ") -> filter $ in $inList
                               -> transform { $l.from, word: $ } )
  -> group by $uword = $
       into { $uword.from, $uword.word, num: count($) }
  -> write(hdfs("senderWordCount"));
}}}

{{{
  // Query 5: segment by zip which requires a join with the user data
  join $senderWordCount, $user
  where $senderWordCount.from == $user.id
    into { $senderWordCount.*, $user.zip }
  -> group by $g = {$.zip, $.word}
       into { $g.zip, $g.word, num: sum($[*].num) }    
  -> write(hdfs("zipWordCount"));
}}}