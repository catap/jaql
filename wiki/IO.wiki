#summary Jaql Input/Output

= Introduction =

Jaql has been designed to flexibly read and write data 
from a variety of data stores and formats. Its core IO
functions are {{{read}}} and {{{write}}} functions that are
parametrized for specific data stores (e.g., file system, database, or a web service) 
and formats (e.g., JSON, XML, or CSV).

With regards to query processing, the IO functions also inform Jaql about whether data can be processed in parallel or must be processed serially. The main criteria for parallel access depends on whether or not [http://hadoop.apache.org/common/docs/r0.20.0/api/index.html InputFormat] (for read) and [http://hadoop.apache.org/common/docs/r0.20.0/api/index.html OutputFormat] (for write) are available. For example, Hadoop's distributed file system ([http://hadoop.apache.org/hdfs/ HDFS]) has several such Input and Output formats available that are readily used by [http://http://hadoop.apache.org/mapreduce/ MapReduce] jobs. Parallel IO is the pre-requisite for parallel processing with Jaql, so it is the most commonly used way to read and write data.

There are times, however, when a data store is needed but it only supports serial access. Examples of such data sources include standard, local file systems and web services. Jaql supports such serial IO but does not attempt to parallelize expressions that use them.

The general form of Jaql's IO functions is: {{{[ T ] read( fd )}}} and {{{write( [T], fd )}}}. That is, {{{read}}} produces an array of type {{{T}}} and {{{write}}} consumes an array of type {{{T}}}. The file descriptor, {{{fd}}} is simply a JSON record that determines whether access is parallel or serial as well as formatting, location, and other options. File descriptors can be specified explicitly or through the use of helper functions that construct appropriate, useful file descriptors. Here, we describe the supported data stores and formats. For further discussion regarding how to plug-in your own data stores and formats, see the discussion of IOArchitecture and IOExtensibility.

  [#Parallel_IO Parallel IO]
    [#Text_Files Text Files] 
    [#Delimited_Files Delimited Files]
    [#JSON_Sequence_Files JSON Sequence Files]
    [#JSON_Sequence_Files_(schema) JSON Sequence Files (schema)]
    [#JSON_Text_Files JSON Text Files]
    [#Task_List Task List]
  [#Serial_IO Serial IO]
    [#File JSON Files]
    [#URI JSON Net Data]

Input and output are handled through read/write expressions.
For example, {{{hdfsRead, hdfsWrite}}} access 
[http://hadoop.apache.org/core/docs/r0.15.3/hdfs_design.html HDFS] files,
{{{hbaseRead, hbaseWrite}}} access [http://hadoop.apache.org/hbase/ HBase] tables,
and {{{localRead, localWrite}}} access locally stored files.
HDFS files and HBase tables can be partitioned and processed using Map/Reduce so
are examples of Jaql's Hadoop IO. Raw HDFS or standard 
filesystem files where raw bytes are expected are examples of Jaql's stream-oriented IO.
We first go over a few examples, then discuss Hadoop IO, stream IO, how to customize
them, and finally, how to add your own data stores.

= Reading and Writing JSON Data =

Using Jaql, JSON data can be stored and retrieved
from a variety of data sources including ordinary files.
Jaql queries can take a _collection_ as input and generate
a new collection as output, where a
collection corresponds to a JSON array.
An example using an ordinary file to store a collection
is as follows:

{{{
  // Example 1. Write to a file named 'hey.dat'.
  [{text: 'Hello World'}] -> write(file('hey.dat'));
	
  // Read it back...
  read(file('hey.dat'));
}}}

Here, a single object with the 'Hello World' string is 
being written to a file called 'hey.dat' in the current directory. 
It is also possible to read and write JSON data to
Hadoop [http://lucene.apache.org/hadoop/hdfs_design.html HDFS] files and
[http://wiki.apache.org/lucene-hadoop/Hbase HBase] tables.
We have made integration with Hadoop a priority by having all data types implement the 
[http://hadoop.apache.org/common/docs/current/api/org/apache/hadoop/io/WritableComparable.html WritableComparable] interface.
By integrating Jaql with HDFS and HBase, we are able to store JSON data
and process it in parallel using Hadoop's map/reduce framework.

Examples using Hadoop are provided below.
Our second example writes to an HDFS 
[http://hadoop.apache.org/common/docs/current/api/org/apache/hadoop/io/SequenceFile.html SequenceFile].
A !SequenceFile is a collection of key-value pairs. 
Jaql's hdfsWrite() function only writes data into the value field, leaving the key field empty.
In example 2, the input data is represented as a literal, but in general the input
can be an expression that is the result of a Jaql query.

{{{
  // Example 2. Write to a Hadoop SequenceFile named: 'orders.dat'.
  [
    {order: 1, cust: 'c1', items: [ 
      {item: 1, qty: 2},
      {item: 3, qty: 6},
      {item: 5, qty: 10}]},
    {order: 2, cust: 'c2', items: [
      {item: 2, qty: 1},
      {item: 5, qty: 2},
      {item: 7, qty: 3}]},
    {order: 3, cust: 'c1', items: [
      {item: 1, qty: 2},
      {item: 7, qty: 14},
      {item: 5, qty: 10}]} 
  ] -> write(hdfs('orders.dat'));
	
  // Read it back...
  read(hdfs('orders.dat'));
}}}

Our third example writes to an HBase table.
An HBase table is a collection of records, with each record containing a 
primary key and a set of column name-value pairs.
Column names in HBase are of type
[http://lucene.apache.org/hadoop/api/org/apache/hadoop/io/Text.html Text],
while column values are simply byte arrays.
HBase uses a two-part naming scheme for columns of the form 'column family:column'.
In Jaql, we use a sharp '#' instead of a colon ':' to separate column families from columns,
since the colon is already used as a separator in JSON.
If a column family is not specified, a special 'default' column family is used, as in the
following example.

{{{
  // Example 3. Write to an HBase table named 'webcrawl'.
  [
    {key: "www.cnn.com", page:'...', rank: 0.9,
     inlinks:[
       {link: 'www.news.com', anchor: 'newsite'},
       {link: 'www.jscript.com', anchor: 'look here'}]},
    {key: "www.json.org", page:'...', rank: 0.8}
  ]
  -> hbaseWrite('webcrawl');
	
  // Read it back...
  hbaseRead('webcrawl');
}}}

In hbaseWrite(), all objects in the input are written to a 
single HBase table.
Each top-level JSON object is stored as an Hbase record with the specified key,
and each name:value pair in the object is stored as a separate column-value pair in
the record.
Values are serialized as a byte array.
Note that only the outermost name:value pairs in top-level objects are 
stored as separate columns in Hbase.
Nested arrays and objects are serialized within these.
In example 3, two HBase records are written.
Each record is used to store the content, rank, and in-links (if any) of a web page.

= Hadoop IO =

In Hadoop's [http://hadoop.apache.org/core/docs/r0.18.3/mapred_tutorial.html map-reduce], data is accessed using [http://hadoop.apache.org/core/docs/r0.18.3/api/org/apache/hadoop/mapred/InputFormat.html InputFormat] and [http://hadoop.apache.org/core/docs/r0.18.3/api/org/apache/hadoop/mapred/OutputFormat.html OutputFormat]. Classes that implement these interfaces provide enough information to map-reduce so that the data can be partitioned and processed in parallel.


Jaql's I/O framework supports any Input(Output)Format to be plugged-in. 
However, Input(Output)Formats work with {{{Writables}}} while Jaql expects {{{Items}}}.
Thus, the framework makes it easy to convert {{{Writables}}} to and from {{{Items}}}.

== Default Input/Output Format ==

In order to make the discussion more concrete, lets look under the hood of the 
{{{read(hdfs('books.jqlb'))}}} expression.
First, {{{hdfs}}} is an expression that constructs a file-handle that is
input to {{{read}}}. The {{{hdfs}}} expression will evaluate to produce the
following expression:

{{{
  read({type: 'hdfs', location: 'books.jqlb'});
}}}

The first argument to the {{{read}}} expression is the name
associated with a type of data store.  Just as names are associated to
function implementations in the function registry, names are
associated to data store types in the storage registry. The second
argument to {{{read}}} is the path of a file stored in HDFS.

For the hdfs data store type, the registry entry specifies default Input(Output)Formats.
The defaults for Jaql are {{{SequenceFileInputFormat}}} and {{{SequenceFileOutputFormat}}}.

=== Using Globbing Patterns to your Advantage ===

Hadoop supports file name [http://hadoop.apache.org/common/docs/current/api/org/apache/hadoop/fs/FileSystem.html#globStatus%28org.apache.hadoop.fs.Path%29 globbing] so Jaql users can take advantage of this feature directly. The value of the {{{location}}} field is passed verbatim to Hadoop's FileSystem API's so all globbing patterns that Hadoop supports are also available to jaql users. 

Consider a directory structure that starts at {{{data}}}, includes sub-directories named by their creation date, that in turn include sub-directories, one per source of data (e.g., database, logger, etc.). Example paths to consider are as follows:

  * data/04_27_2010/logs/machine1.log
  * data/04_27_2010/db/t1.dat
  * data/0.4_28_2010/logs/machine1.log

To read machine1's data for 04/27/2010, simply issue:
{{{
  read(hdfs("data/04_27_2010/logs/machine1.log"));
}}}

To read all data collected on 04/27/2010:
{{{
  read(hdfs("data/04_27_2010/*/*"));
}}}

When piped to further jaql expressions, the above examples allow you to operate on subsets of the data, effectively eliminating data that is not of current interest.
If needed, all data can be processed by using:
{{{
  read(hdfs("data/*/*/*"));
}}}

In some cases, certain paths may be of interest that cannot be easily expressed
using globbing patterns. For example, consider the simple case of needing to
read {{{data/04_27_2010/db/t1.dat}}} and {{{data/0.4_28_2010/logs/machine1.log}}}.
More generally, one can have an arbitrary collection of paths, modeled as an array
(e.g., ["data/04_27_2010/db/t1.dat", "data/0.4_28_2010/logs/machine1.log"]). To convert
this to a globbing pattern appropriate for hadoop, use:

{{{
  makeGlob = fn(ps) (

    strcat("/{", ps 
                 -> transform strcat(".",$) 
                 -> strJoin(","), 
           "}")

  );
}}}

For example:
{{{
  makeGlob(["data/04_27_2010/db/t1.dat", "data/0.4_28_2010/logs/machine1.log"]);
  // returns
  "/{.data/04_27_2010/db/t1.dat,.data/0.4_28_2010/logs/machine1.log}"
}}}

And can be used in conjunction with {{{read}}} as follows:

{{{
  paths = ["data/04_27_2010/db/t1.dat", "data/0.4_28_2010/logs/machine1.log"];
  read(hdfs(makeGlob(paths)));
}}}




= Stream IO =

The Hadoop-based IO is useful when processing large data sets.
However, we expect that reading from an [http://java.sun.com/j2se/1.5.0/docs/api/java/io/InputStream.html InputStream]
or writing to an [http://java.sun.com/j2se/1.5.0/docs/api/java/io/OutputStream.html OutputStream]
will also be needed when manipulating small data sets. 
For this purpose, we provide an additional type of adapter: !StreamAdapters. 
!StreamAdapters open an input or output stream given a URI. 
For example, {{{localRead, localWrite}}} and {{{httpGet}}} expressions
are based on !StreamAdapters. Just as Hadoop adapters allow for conversions between {{{Writables}}} and {{{Items}}}, Stream adapters also provide for converting between bytes and {{{Items}}}.

For example, consider accessing a local file that is formatted as JSON text. The only
class to implement is a converter that can borrow from the previous example:

{{{
  public class FromJSONTxtConverter implements StreamToItem {
    ...
    public void setInputStream(InputStream in) {
      // set the input stream
    }
    
    public boolean read(Item v) throws IOException {
      // parse the input stream to get the next v
    }
    ...
  }
}}}

The new data source is registered and tested as follows:

{{{
  read({type:"local", location:'build/test/cache/books.json', inoptions:{format : 'com.ibm.jaql.io.stream.converter.JSONTextInputStream'}});
}}}

Finally, to work with external data sources, we recently added a
httpGet() function that can retrieve JSON data from a URL.  Below are
two examples of httpGet() that get data from 
[http://www.freebase.com Freebase] and 
[http://developer.yahoo.com/traffic Yahoo! Traffic] (the
latter requires you to supply an 
[http://developer.yahoo.com/faq/#appid application id]).

{{{
  // Get albums recorded by "The Police" using Freebase.
  $artist = "The Police";
  $url = "http://api.freebase.com/api/service/mqlread";
  $query = 
   {query: {album: [], name: $artist, type: "/music/artist"}};
  read(http($url, {query: serialize($query)}))[0].result.album;
      
  // result...
  [ "Outlandos d\'Amour",
    "Reggatta de Blanc",
    "Zenyatta Mondatta",
    "Ghost in the Machine",
    "Synchronicity",
    "Every Breath You Take: The Singles",
    "Greatest Hits",
    "Message in a Box: The Complete Recordings (disc 1)",
    "Message in a Box: The Complete Recordings (disc 2)",
    "Message in a Box: The Complete Recordings (disc 3)",
    "Message in a Box: The Complete Recordings (disc 4)",
    "Live! (disc 1: Orpheum WBCN/Boston Broadcast)",
    "Live! (disc 2: Atlanta/Synchronicity Concert)",
    "Every Breath You Take: The Classics",
    "Their Greatest Hits",
    "Can\'t Stand Losing You",
    "Roxanne \'97 (Puff Daddy remix)",
    "Roxanne \'97"];
  
  // Get traffic incidents from Yahoo!.
  $appid = "YahooDemo"; // Set to your yahoo application ID
   $trafficData = 
      httpGet('http://local.yahooapis.com/MapsService/V1/trafficData',
        { appid:  "YahooDemo",
          street: "701 First Street",
          city:   "Sunnyvale",
          state:  "CA",
          output: "json"
        })[0],
  
  $trafficData.ResultSet.Result[*].title
  
  // result...
  [ "Road construction, on US-101 NB at FAIROAKS AVE TONBTO NB MATHILDA",
    "Road construction, on CA-85 SB at MOFFETT BLVD",
    "Road construction, on CA-237 EB at MATHILDA AVE TOEBTO FAIR OAKS AVE",
    "Road construction, on CA-237 WB at CROSSMAN AVE",
    "Road construction, on I-880 at GATEWAY BLVD"];
}}}