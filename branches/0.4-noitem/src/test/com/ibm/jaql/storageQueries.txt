//-- hdfs write/read expressions --

// write/read Items to SequenceFile: default adapter, default format, no converter
[1,2,3]
-> write({type: 'hdfs', location: 'jaqlTest/test1.dat'});

read({type: 'hdfs', location: 'jaqlTest/test1.dat'});

[1,2,3]
-> write({type: 'hdfs', location: 'jaqlTest/test1alt.dat'})
-> read();

// write/read Items to SequenceFile: specify FileAdapter, SequenceFileFormat, no converter
[1,2,3]
-> write({location: 'jaqlTest/test2.dat', 
         outoptions: {adapter: 'com.ibm.jaql.io.hadoop.DefaultHadoopOutputAdapter', 
                         format: 'org.apache.hadoop.mapred.SequenceFileOutputFormat',
                         configurator: 'com.ibm.jaql.io.hadoop.FileOutputConfigurator'}});

read({location: 'jaqlTest/test2.dat', 
        inoptions: {adapter: 'com.ibm.jaql.io.hadoop.DefaultHadoopInputAdapter', 
                  format: 'org.apache.hadoop.mapred.SequenceFileInputFormat',
                  configurator: 'com.ibm.jaql.io.hadoop.FileInputConfigurator'}});
                  
[1,2,3]
-> write({location: 'jaqlTest/test2alt.dat', 
                outoptions: {adapter: 'com.ibm.jaql.io.hadoop.DefaultHadoopOutputAdapter', 
                             format: 'org.apache.hadoop.mapred.SequenceFileOutputFormat',
                             configurator: 'com.ibm.jaql.io.hadoop.FileOutputConfigurator'},
                inoptions:  {adapter: 'com.ibm.jaql.io.hadoop.DefaultHadoopInputAdapter', 
                             format: 'org.apache.hadoop.mapred.SequenceFileInputFormat',
                             configurator: 'com.ibm.jaql.io.hadoop.FileInputConfigurator'}})
-> read();
                        
// write/read Items to Text in a SequenceFile: specify ConverterAdapter, SequenceFileFormat, Item <-> Text (JSON)
// This assumes that the default, FileOutput(Input)Format can be passed a converter.
[{one: '1'}, {two: '2'}, {three: '3'}]
-> write({type: 'hdfs', location: 'jaqlTest/test3.dat', 
         outoptions: {converter: 'com.foobar.store.ToJSONSeqConverter',
                      configurator: 'com.foobar.store.TextFileOutputConfigurator'}});
              
read({type: 'hdfs', location: 'jaqlTest/test3.dat', 
        inoptions: {converter: 'com.foobar.store.FromJSONSeqConverter'}});

[{one: '1'}, {two: '2'}, {three: '3'}]
-> write({type: 'hdfs', location: 'jaqlTest/test3alt.dat', 
               outoptions: {converter: 'com.foobar.store.ToJSONSeqConverter',
                            configurator: 'com.foobar.store.TextFileOutputConfigurator'},
               inoptions: {converter: 'com.foobar.store.FromJSONSeqConverter'}})
-> read();

// write/read Items to Text in a TextFile: specify ConverterAdapter, TextFormat, Item <-> Text (JSON)
[{one: '1'}, {two: '2'}, {three: '3'}]
-> write({type: 'hdfs', location: 'jaqlTest/test4.txt', 
         outoptions: {format: 'org.apache.hadoop.mapred.TextOutputFormat',
                      converter: 'com.foobar.store.ToJSONTxtConverter',
                      configurator: 'com.foobar.store.TextFileOutputConfigurator'}});
              
read({type: 'hdfs', location: 'jaqlTest/test4.txt',
	    inoptions: {format: 'org.apache.hadoop.mapred.TextInputFormat',
			  		converter: 'com.foobar.store.FromJSONTxtConverter'}});

[{one: '1'}, {two: '2'}, {three: '3'}]
-> write({type: 'hdfs', location: 'jaqlTest/test4alt.txt', 
                outoptions: {format: 'org.apache.hadoop.mapred.TextOutputFormat',
                             converter: 'com.foobar.store.ToJSONTxtConverter',
                             configurator: 'com.foobar.store.TextFileOutputConfigurator'},
                inoptions: {format: 'org.apache.hadoop.mapred.TextInputFormat',
			  		        converter: 'com.foobar.store.FromJSONTxtConverter'}})
-> read();

//-- hbase write/read expressions --

// write/read Items to an HBase Table as Items: default adapter, default format, no converter (see hbaseQueries.txt)
// stWrite({type: 'hbase', location: 'jaqlTesttest5'}, [{key: '1', one: '1'}, {key: '2', two: '2'}, {key: '3', three: '3'}]);

// stRead({type: 'hbase', location: 'jaqlTesttest5'});

// stRead(stWrite({type: 'hbase', location: 'jaqlTesttest5alt1'}, [{key: '1', one: '1'}, {key: '2', two: '2'}, {key: '3', three: '3'}]));

// can do the same thing if you pass in the right options to HadoopRead
// stWrite({location: 'jaqlTesttest5alt2', 
//          outoptions: {adapter: 'com.ibm.jaql.io.hadoop.DefaultHadoopOutputAdapter', 
//                       format: 'com.ibm.jaql.io.hbase.JaqlTableOutputFormat',
//                       configurator: 'com.ibm.jaql.io.hbase.TableOutputConfigurator'}}, 
//          [{key: '1', one: '1'}, {key: '2', two: '2'}, {key: '3', three: '3'}]);
                            
// stRead({location: 'jaqlTesttest5alt2',
// 	    inoptions: {adapter: 'com.ibm.jaql.io.hadoop.DefaultHadoopInputAdapter',
// 			  	    format: 'com.ibm.jaql.io.hbase.JaqlTableInputFormat',
// 			  	    configurator: 'com.ibm.jaql.io.hbase.TableInputConfigurator'}});

// stRead(stWrite({location: 'jaqlTesttest5alt3', 
//                 outoptions: {adapter: 'com.ibm.jaql.io.hadoop.DefaultHadoopOutputAdapter', 
//                              format: 'com.ibm.jaql.io.hbase.JaqlTableOutputFormat',
//                              configurator: 'com.ibm.jaql.io.hbase.TableOutputConfigurator'},
//                 inoptions: {adapter: 'com.ibm.jaql.io.hadoop.DefaultHadoopInputAdapter',
// 			  	            format: 'com.ibm.jaql.io.hbase.JaqlTableInputFormat',
// 			  	            configurator: 'com.ibm.jaql.io.hbase.TableInputConfigurator'}}, 
//               [{key: '1', one: '1'}, {key: '2', two: '2'}, {key: '3', three: '3'}]));

//-- local file read/write expressions

// write/read Items to/from a local file
[1,2,3]
-> write({location: 'build/test/cache/mynewfile.txt', 
         outoptions: {adapter : 'com.ibm.jaql.io.stream.FileStreamOutputAdapter',
                      format  : 'com.ibm.jaql.io.stream.converter.JsonOutputStream'}});

read({location: 'build/test/cache/mynewfile.txt', 
        inoptions: {adapter : 'com.ibm.jaql.io.stream.FileStreamInputAdapter',
                    format  : 'com.ibm.jaql.io.stream.converter.JsonInputStream'}});

[1,2,3]
-> localWrite({type: 'local', location: 'build/test/cache/myotherfile.txt'});

read({type: 'local', location: 'build/test/cache/myotherfile.txt'});

//-- test map/reduce --

// input/output is hdfs sequence file of Items

// note, 'g' is quoted so that it can be used as a key for subsequent hbase tests
[
  { key: 0, g:"0", text: 'zero' },
  { key: 1, g:"1", text: 'one' },
  { key: 2, g:"0", text: 'two' },
  { key: 3, g:"1", text: 'three' },
  { key: 4, g:"0", text: 'four' },
  { key: 5, g:"1", text: 'five' },
  { key: 6, g:"0", text: 'six' },
  { key: 7, g:"1", text: 'seven' },
  { key: 8, g:"0", text: 'eight' },
]
-> write({ type: 'hdfs', location: 'jaqlTest/test6.dat'});

read({type: 'hdfs', location: 'jaqlTest/test6.dat'});

// input is hdfs sequence file of Items, output is hbase table (note that 'g' renamed to 'key') (see hbaseQueries.txt)
// stWrite({type: 'hbase', location: 'jaqlTesttest6out'}, []);

//stRead( 
//  mapReduce( {
//    'input' : {type: 'hdfs', location: 'jaqlTest/test6.dat'},
//    'map'   : fn($i) [[ $i.g, 1 ]],
//    'reduce': fn($key, $values) [{ key: $key, n: count($values) }],
//    'output': {type: 'hbase', location: 'jaqlTesttest6out'}
//    })
// );
 
// input is hbase table, output is sequence file of Items
// stWrite({ type: 'hbase', location: 'jaqlTesttest7'}, [
//   { key: "0", g:0, text: 'zero' },
//   { key: "1", g:1, text: 'one' },
//   { key: "2", g:0, text: 'two' },
//   { key: "3", g:1, text: 'three' },
//   { key: "4", g:0, text: 'four' },
//   { key: "5", g:1, text: 'five' },
//   { key: "6", g:0, text: 'six' },
//   { key: "7", g:1, text: 'seven' },
//   { key: "8", g:0, text: 'eight' },
// ]);

// stRead( 
//   mapReduce( {
//     'input' : {type: 'hbase', location: 'jaqlTesttest7'},
//     'map'   : fn($i) [[ $i.g, 1 ]],
//     'reduce': fn($key, $values) [{ g: $key, n: count($values) }],
//     'output': {type: 'hdfs', location: 'jaqlTest/test7.dat'}
//     })
//  );
 
// input is a file with JSON text (one record per line), output is a SequenceFile of items
[
  { key: 0, g:0, text: 'zero' },
  { key: 1, g:1, text: 'one' },
  { key: 2, g:0, text: 'two' },
  { key: 3, g:1, text: 'three' },
  { key: 4, g:0, text: 'four' },
  { key: 5, g:1, text: 'five' },
  { key: 6, g:0, text: 'six' },
  { key: 7, g:1, text: 'seven' },
  { key: 8, g:0, text: 'eight' },
]
-> write({type: 'hdfs', location: 'jaqlTest/test8.txt', 
         outoptions: {format: 'org.apache.hadoop.mapred.TextOutputFormat',
                      converter: 'com.foobar.store.ToJSONTxtConverter',
                      configurator: 'com.foobar.store.TextFileOutputConfigurator'}});

read({type: 'hdfs', location: 'jaqlTest/test8.txt',
	    inoptions: {format: 'org.apache.hadoop.mapred.TextInputFormat',
			  		converter: 'com.foobar.store.FromJSONTxtConverter'}});
			  		    
mapReduce( {
    'input' : {type: 'hdfs', location: 'jaqlTest/test8.txt', 
               inoptions: {format: 'org.apache.hadoop.mapred.TextInputFormat',
			  		       converter: 'com.foobar.store.FromJSONTxtConverter'}},
    'map'   : fn($) ( $ -> transform [ $.g, 1 ]),
    'reduce': fn($key, $values) [{ g: $key, n: count($values) }],
    'output': {type: 'hdfs', location: 'jaqlTest/test8.dat'}
    })
-> read();

//-- test composite input adapter (see hbaseQueries.txt)

// write out an hdfs file
[
  { key: "0", g:0, text: 'zero' },
  { key: "1", g:1, text: 'one' },
  { key: "2", g:0, text: 'two' },
  { key: "3", g:1, text: 'three' },
  { key: "4", g:0, text: 'four' }
]
-> write({ type: 'hdfs', location: 'jaqlTest/test9.dat'});

// write out an hbase table
// stWrite({ type: 'hbase', location: 'jaqlTesttest9'}, [
//   { key: "5", g:1, text: 'five' },
//   { key: "6", g:0, text: 'six' },
//   { key: "7", g:1, text: 'seven' },
//   { key: "8", g:0, text: 'eight' },
// ]);

// stRead([{type: 'hdfs', location: 'jaqlTest/test9.dat'}, {type: 'hbase', location: 'jaqlTesttest9'}]);

// stRead(
//   mapReduce( {
//     'input': [ [{type: 'hdfs', location: 'jaqlTest/test9.dat'}, {type: 'hbase', location: 'jaqlTesttest9'}] ],
//      'map'   : fn($i) [[ $i.g, 1 ]],
//      'reduce': fn($key, $values) [{ g: $key, n: count($values) }],
//      'output': {type: 'hdfs', location: 'jaqlTest/test9out.dat'}
//    })
// );

//-- test co-group (see hbaseQueries.txt)

// stRead(mapReduce( {input: [ {type: 'hdfs', location: 'jaqlTest/test9.dat'}, 
//                             {type: 'hbase', location: 'jaqlTesttest9'}],
//                    map: [ fn($i) [[ $i.g, 1 ]], 
//                           fn($i) [[ $i.g, 1 ]] ],
//                    reduce: fn($key, $aVals, $bVals) [{ g: $key, as: count($aVals), bs: count($bVals) }],
//                    output: {type: 'hdfs', location: 'jaqlTest/test10out.dat'}}));

//-- test higher level interfaces

[1,2,3] -> localWrite(file('build/test/cache/test11a.dat'));

read(file('build/test/cache/test11a.dat'));

[1,2,3] -> localWrite(file('build/test/cache/test11b.dat'));

read(file('build/test/cache/test11b.dat'));

[1,2,3] -> write(hdfs('jaqlTest/test12a.dat'));

read(hdfs('jaqlTest/test12a.dat'));

[1,2,3] -> write(hdfs('jaqlTest/test12b.dat'));

read(hdfs('jaqlTest/test12b.dat'));

$path = "build/test/cache/";

$r1 = read(file($path + "books.json"));

$r1 -> write(file($path + "b1"));

read(file($path + "b1")) == read(file($path + "books.json"));

$r1 -> write(file($path + "b2", {format: "com.ibm.jaql.io.stream.converter.JsonOutputStream"}));

$r2 = read(file($path + "b2", {format: "com.ibm.jaql.io.stream.converter.JsonInputStream"}));

$r1 == $r2;

$r1 -> write(hdfs("jaqlTest/b1"));

$r1 == read(hdfs("jaqlTest/b1"));

$v = {someField: "some value"};

[$v] -> write(file($path + "b2", {asArray: false}));

read(file($path + "b2", {asArray: false}))[0] == $v;

hdfsShell("-copyFromLocal "+$path+"test.json jaqlTest/test") * 0;

$r = read(hdfs("jaqlTest/test", {format: "org.apache.hadoop.mapred.TextInputFormat", converter: "com.ibm.jaql.io.hadoop.converter.FromJsonTextConverter"}));

$l = [
  {
    "a": "foo",
    "b": "bar"
  },
  {
    "a": 1,
    "b": 2,
    "c": 3
  }
];

$l -> write(hdfs("jaqlTest/test2", {format: "org.apache.hadoop.mapred.TextOutputFormat", converter: "com.ibm.jaql.io.hadoop.converter.ToJsonTextConverter"}));

$r -> write(hdfs("jaqlTest/test2", {format: "org.apache.hadoop.mapred.TextOutputFormat", converter: "com.ibm.jaql.io.hadoop.converter.ToJsonTextConverter"}));

$r2 = read(hdfs("jaqlTest/test2", {format: "org.apache.hadoop.mapred.TextInputFormat", converter: "com.ibm.jaql.io.hadoop.converter.FromJsonTextConverter"}));

($r -> sort by [$]) == ($r2 -> sort by [$]);

//$artist = "The Police";
//$url = "http://api.freebase.com/api/service/mqlread";
//$query = {query: {album: [], name: $artist, type: "/music/artist"}};
//httpGet($url, {query: serialize($query)}, {asArray: false})[0].result.album;
//read(http($url, {query: serialize($query)}))[0].result.album;

// write('hbase', 'jaqlTesttest13a.dat', [{key: '0', a: 'foo'}, {key: '1', b: 'bar'}]);

// read('hbase', 'jaqlTesttest13a.dat');

// hbaseWrite('jaqlTesttest13b.dat', [{key: '0', a: 'foo'}, {key: '1', b: 'bar'}]);

// hbaseRead('jaqlTesttest13b.dat');

//-- test array read
read({type: 'array', inoptions: {array: [1,2,3]}});

arrayRead([1,2,3]);

mapReduce( {
    'input': {type: 'array', inoptions: {array: [1,2,3]}},
    'map'   : fn($) ( $ -> transform [ null, $ ] ),
    'output': {type: 'hdfs', location: 'jaqlTest/test14out.dat'}
  })
-> read();

mapReduce( {
    'input': [ {type: 'array', inoptions: {array: [1,2,3]}},
               {type: 'array', inoptions: {array: [1,3,5]}} ],
    'map'   : [ fn($) ($ -> transform [ null, $ + 10 ]),
                fn($) ($ -> transform [ null, $ + 20 ]) ],
    'output': {type: 'hdfs', location: 'jaqlTest/test14out.dat'}
  })
-> read();

//-- test rewrites --

// group by over hdfs read
read({type: 'hdfs', location: 'jaqlTest/test6.dat'})
-> group by $a = ($.g)
    into { g:$a, i:count($) }
-> sort by [$.g];

// group by over hbase read (see hbaseQueries.txt)
// group( $i in stRead({type: 'hbase', location: 'jaqlTesttest7'}) 
//        by $a = $i.g
//        into $is
//     [ { g:$a, i:count($is) } ];

// for loop over hdfs read
read({type: 'hdfs', location: 'jaqlTest/test6.dat'})
-> transform $.key;

// for loop over hbase read (see hbaseQueries.txt)
// for( $r in stRead({type: 'hbase', location: 'jaqlTesttest7'}) )
//   [ $r.key ];

// co-group (see hbaseQueries.txt)
// group(
//   $i in stRead({type: 'hdfs', location: 'jaqlTest/test9.dat'}) 
//      by $g = $i.g
//      into $as,
//   $j in stRead({type: 'hbase', location: 'jaqlTesttest9'}) 
//      by $g = $j.g
//      into $bs )
//  [ { g: $g, as: count($as), bs: count($bs) } ];
