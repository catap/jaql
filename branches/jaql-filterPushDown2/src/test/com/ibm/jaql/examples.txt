// From JaqlOverview

$books = [
    {publisher: 'Scholastic',
     author: 'J. K. Rowling',
     title: 'Deathly Hallows',
     year: 2007},

    {publisher: 'Scholastic',
     author: 'J. K. Rowling',
     title: 'Chamber of Secrets',
     year: 1999, 
     reviews: [
       {rating: 10, user: 'joe', review: 'The best ...'},
       {rating: 6, user: 'mary', review: 'Average ...'}]},

    {publisher: 'Scholastic',
     author: 'J. K. Rowling',
     title: 'Sorcerers Stone',
     year: 1998},

    {publisher: 'Scholastic',
     author: 'R. L. Stine',
     title: 'Monster Blood IV',
     year: 1997, 
     reviews: [
       {rating: 8, user: 'rob', review: 'High on my list...'}, 
       {rating: 2, user: 'mike', review: 'Not worth the paper ...', 
        discussion:
          [{user: 'ben', text: 'This is too harsh...'}, 
           {user: 'jill', text: 'I agree ...'}]}]},

    {publisher: 'Grosset',
     author: 'Carolyn Keene',
     title: 'The Secret of Kane',
     year: 1930}
  ];
  
// Write the books collection from data above.
$books -> write(hdfs('books'));
  
// Query 1. Return the publisher and title of each book.
read(hdfs('books'))
-> transform {$.publisher, $.title};
  
// Query 2. Find the authors and titles of books that have received 
// a review. 
read(hdfs('books'))
-> filter exists($.reviews)
-> transform {$.author, $.title};

// Query 3a. Project the title from each book using the short-hand
// projection notation. 
read(hdfs('books'))[*].title;

// Query 3a-alt. Or using equivalent the long-hand notation. 
read(hdfs('books'))
-> transform $.title;
  
// Query 3b. Project the user from each review of each book using the short-hand
// projection notation.  The double-stars flattens the contained arrays.
read(hdfs('books'))[*].reviews[*].user -> expand;

// Query 3b-alt. Or using equivalent the long-hand notation.
read(hdfs('books'))
-> expand $.reviews
-> transform $.user;

// Query 4. Find authors, titles, and reviews of books where a review
// prompted a discussion by the user 'ben'. 
read(hdfs('books'))
-> filter 'ben' in ($.reviews[*].discussion[*].user -> expand)
-> transform { $.author, $.title, $.reviews };

// Query 5. Find the authors and titles of books that had an
// average review rating over 5. 
read(hdfs('books'))
-> filter avg($.reviews[*].rating) > 5
-> transform {$.author, $.title};

// Query 6. Show how many books each publisher has published. 
read(hdfs('books'))
-> group by $p = ($.publisher)
    into {publisher: $p, num: count($)}
-> sort by [$.publisher];
  
// Query 7. Find the publisher who published the most books. 
read(hdfs('books'))
-> group by $p = ($.publisher)
    into {publisher: $p, num: count($)}
-> top 1 by [$.num desc];

// Setup for co-group example 
[
  {a:1, b:1}, 
  {a:1, b:2}, 
  {a:2, b:3}, 
  {a:2, b:4}
]
-> write(hdfs('X'));

[
  {c:2, d:1}, 
  {c:2, d:2}, 
  {c:3, d:3}, 
  {c:3, d:4}
]
-> write(hdfs('Y'));

// Query 8. Co-group X and Y. 
$x = read(hdfs('X'));
$y = read(hdfs('Y'));
group $x by $g = ($.a),
      $y by $g = ($.c)
 into {g: $g, b: $x[*].b, d: $y[*].d}
-> sort by [$];

// Query 9. Join X and Y. : sort to ensure order
join $x, $y
where $x.a == $y.c
into {$x.a, $x.b, $y.c, $y.d}
-> sort by [$];

// Write to an HDFS file called 'sample'.
[
    {x: 0, text: 'zero'},
    {x: 1, text: 'one'},
    {x: 0, text: 'two'},
    {x: 1, text: 'three'},
    {x: 0, text: 'four'},
    {x: 1, text: 'five'},
    {x: 0, text: 'six'},
    {x: 1, text: 'seven'},
    {x: 0, text: 'eight'}
]
-> write(hdfs('sample.dat'));

mapReduce( 
    { input:  {type: 'hdfs', location: 'sample.dat'}, 
      output: {type: 'hdfs', location: 'results.dat'}, 
      map:    fn($v) ( $v -> transform [$.x, 1] ),
      reduce: fn($x, $v) ( $v -> aggregate {x: $x, num: count($)} )
    });
    
read(hdfs('results.dat'));

hdfsShell("-copyFromLocal docs/jaql-overview.html test") * 0;

$lineRdr = read({type: "hdfs", location: "test", inoptions: {format: "org.apache.hadoop.mapred.TextInputFormat", converter: "com.acme.extensions.data.FromLineConverter"}});

registerFunction("splitArr", "com.acme.extensions.expr.SplitIterExpr");

$lineRdr
-> expand splitArr($, " ")
-> transform [$,1]
-> group by $w = ($[0])
    into [$w, sum($[*][1])]
-> count();

$lineRdr
-> expand splitArr($, " ")
-> group by $w = ($)
    into [$w, count($)]
-> count();

registerFunction("split1", "com.acme.extensions.fn.Split1");
$path = '/home/mystuff/stuff';

split1($path, "/");

count(split1($path, "/"));

split1($path, "/")[1]; 

$median = fn($items) (
$sorted = $items -> sort by [$],

$sorted[int(count($sorted)/2)]);

$median( [ 1, 4, 5, 3, 2 ] );

$var = fn($items) (
  $init = 
     $items
     -> filter not isnull($)
     -> transform { n: 1, s1: $, s2: $*$ },

  $combined =
     $init 
     -> combine( fn($a,$b)
            { n:  $a.n  + $b.n,
             s1: $a.s1 + $b.s1,
             s2: $a.s2 + $b.s2 }),

  $E_X  = $combined.s1 / $combined.n,
  $E_X2 = $combined.s2 / $combined.n,

  $E_X2 - $E_X * $E_X
);

$var( [ 1, 4, 5, 3, 2 ] );

registerFunction("gcd1", "com.acme.extensions.fn.GCD1");
gcd1(null); // null
gcd1([]); // null
gcd1(3); // correctly produces cast error: array expected
gcd1([3]); // 3
gcd1([0,0]); // 0
gcd1([3,0]); // 3
gcd1([0,3]); // 3
gcd1([17,13]); // 1
gcd1([12,18]); // 6
gcd1([36,18]); // 18
gcd1([36,18,12]); // 6
gcd1(range(1000,2000) -> filter mod($,3) == 0 -> transform $ * 31); // 31*3 = 93


registerFunction("gcd2", "com.acme.extensions.fn.GCD2");

gcd2("x","y"); // correctly produces error: numbers expected
gcd2(17,13); // 1
gcd2(12,18); // 6


$gcd = fn($nums) combine($nums, fn($a,$b) gcd2($a,$b));

$gcd(range(1000,2000) -> filter mod($,3) == 0 -> transform $ * 31); // 31*3 = 93


$gcd = fn($nums) ($nums -> combine( fn($a,$b) gcd1( [$a,$b] ) ));

$gcd(range(1000,2000) -> filter mod($,3) == 0 -> transform $ * 31); // 31*3 = 93


range(1,100)
-> expand each $i (
     range(1,100)
     -> transform each $j { a: $i, b: $i * $j }
   )
-> write(hdfs('/temp/nums'));

registerFunction("gcd1", "com.acme.extensions.fn.GCD1");
$gcd = fn($nums) gcd1( $nums );
    
read(hdfs('/temp/nums'))
-> group by $a = ($.a)
    into { a: $a, g: $gcd($[*].b) }
-> sort by [$.a];

registerFunction("gcd2", "com.acme.extensions.fn.GCD2");
$gcd = fn($nums) ( $nums -> combine( fn($a,$b) gcd2( $a,$b ) ) );
    
read(hdfs('/temp/nums'))
-> group by $a = ($.a)
    into { a: $a, g: $gcd($[*].b) }
-> sort by [$.a];

mrAggregate( {
     input: { type: "hdfs", location: "/temp/nums" }, 
     output: HadoopTemp(),
     map: fn ($) ( $ -> transform [ $.a, $ ] ),
     aggregate: fn ($k, $) [ combine($.b, fn($a,$b) gcd2( $a,$b ) ) ],
     final: fn ($k, $aggs) [{ a:$k, g: $aggs[0] }]
 } )
 -> read()
 -> sort by [$.a];
 
  registerFunction("grep", "com.acme.extensions.fn.Grep");
$data = [ "a1bxa2b", "a3bxa4b", "a5bxa6b", null, "a7bxa8b" ];

grep("a\\d*b", $data);
// [ "a1b", "a3b", "a5b", "a7b" ]

grep("a\\d*b", null, $data );
// [ "a1b", "a3b", "a5b", "a7b" ]

grep("a\\d*b", "g", $data );
    
// Example 1. Write to a file named 'hey.dat'. 
[{text: 'Hello World'}] -> localWrite(file('hey.dat'));
	
// Read it back... 
read(file('hey.dat'));

// Example 2. Write to a Hadoop SequenceFile named: 'orders.dat'.
[
    {order: 1, cust: 'c1', items: [ 
      {item: 1, qty: 2},
      {item: 3, qty: 6},
      {item: 5, qty: 10}]},
    {order: 2, cust: 'c2', items: [
      {item: 2, qty: 1},
      {item: 5, qty: 2},
      {item: 7, qty: 3}]},
    {order: 3, cust: 'c1', items: [
      {item: 1, qty: 2},
      {item: 7, qty: 14},
      {item: 5, qty: 10}]} 
] 
-> write(hdfs('orders.dat'));

// Read it back...
read(hdfs('orders.dat'));

// Example 3. Write to an HBase table named 'webcrawl'. (from exampleQueries)
[
    {key: "www.cnn.com", page:'...', rank: 0.9,
     inlinks:[
       {link: 'www.news.com', anchor: 'newsite'},
       {link: 'www.jscript.com', anchor: 'look here'}]},
    {key: "www.json.org", page:'...', rank: 0.8}
]
-> hbaseWrite('webcrawl');
