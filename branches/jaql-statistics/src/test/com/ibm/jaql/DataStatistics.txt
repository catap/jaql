/**
* Examples for collecting data statistics.
* These examples show the use of functions:
*    1- paths(<JsonRecord>)                                                                  //jaql.lang.expr.record.pathsFn
*    2- pathValues(<JsonRecord>)                                                             //jaql.lang.expr.record.pathValues
*    3- FieldSortInfo(<JsonArray>)                                                           //jaql.lang.expr.record.FieldSortInfo
*    4- bucketize(<Sorted JsonArray of atom values>, max_step_size, comparison function)     //jaql.land.util.BucketizeFn
*    5- bloomFilter(<JsonArray of atom values>, distinct_threshold)                          //jaql.lang.util.BloomFilterFn
*    6- jaql function that generates statistics about the files and their splits
*/
$books =
[
    {publisher: 'Scholastic',
     author: 'J. K. Rowling',
     title: 'Deathly Hallows',
     year: 2007,
     format: {cover: 'hard', pages: 1302},
     price: [3.5, 34.0, 1022.22, 3.3]},

    {publisher: 'Scholastic',
     author: 'J. K. Rowling',
     title: 'Chamber of Secrets',
     year: 1999, 
     format: {cover: 'hard', pages: 1302},
     reviews: [
       {rating: 10, user: 'joe', review: 'The best ...'},
       {rating: 6, user: 'mary', review: 'Average ...'}],
     price: [20.1, 25.0]
     },

    {publisher: 'John',
     author: 'J. K. Rowling',
     title: 'Deathly Hallows',
     year: 1998,
      format: {cover: 'soft', pages: 1200},
    price: [120.5, 110.0]},

    {publisher: 'Rose',
     author: 'New Rowling',
     title: 'Chamber of Secrets',
     year: 1998, 
     reviews: [{rating: 10, user: 'joe', review: 'The best ...'}]},

    {publisher: 'Scholastic',
     author: 'J. K. Rowling',
     title: 'Sorcerers Stone',
     year: 1996,
     format: {cover: 'hard', pages: 742},
     price: [11.0, 17.5]},

    {publisher: 'Scholastic',
     author: 'R. L. Stine',
     title: 'Monster Blood IV',
     year: 1995, 
     format: {cover: 'soft', pages: 302},

     reviews: [
       {rating: 8, user: 'rob', review: 'High on my list...'}, 
       {rating: 2, user: 'mike', review: 'Not worth the paper ...', 
        discussion:
          [{user: 'ben', text: 'This is too harsh...', evaluation:[10, 3, 20]}, 
           {user: 'jill', text: 'I agree ...', evaluation:[1, 3, 15]}]}]},

    {publisher: 'Grosset',
     author: 'Carolyn Keene',
     title: 'The Secret of Kane',
     year: 1988},
     
     
     {publisher: 'Scholastic',
          author: 'J. K. Rowling',
          title: 'Deathly Hallows',
          year: 1987},
     
         {publisher: 'Scholastic',
          author: 'J. K. Rowling',
          title: 'Chamber of Secrets',
          year: 1987, 
          reviews: [
            {rating: 10, user: 'joe', review: 'The best ...'},
            {rating: 6, user: 'mary', review: 'Average ...'}]},
     
         {publisher: 'John',
          author: 'J. K. Rowling',
          title: 'Deathly Hallows',
          year: 1986},
     
         {publisher: 'W. Kelly',
          author: 'Old Rowling',
          title: 'Chamber of Secrets',
          year: 1979, 
          reviews: [{rating: 10, user: 'joe', review: 'The best ...'}]},
     
         {publisher: 'Scholastic',
          author: 'J. K. Rowling',
          title: 'Sorcerers Stone',
          year: 1978},
     
         {publisher: 'Scholastic',
          author: 'R. L. Stine',
          title: 'Monster Blood IV',
          year: 1977, 
          reviews: [
            {rating: 8, user: 'rob', review: 'High on my list...'}, 
            {rating: 2, user: 'mike', review: 'Not worth the paper ...', 
             discussion:
               [{user: 'ben', text: 'This is too harsh...', evaluation:[2]}, 
                {user: 'jill', text: 'I agree ...'}]}],
          price: [3.6, 20.5, 41.0]
          },
     
         {publisher: 'Grosset',
          author: 'Carolyn Keene',
          title: 'The Secret of Kane',
          year: 1920,
          format: {cover: 'hard', pages: 66}
}    
];
 
//Report the fields (entire paths) in each record 
$books -> transform paths($);

//Report the distinct fields (entire paths) in the file 
$books -> transform paths($) -> expand -> distinct();

//For each input record, report the existing field names, their values, and data types 
$books -> transform pathValues($);

//For each field name and data type, report the min value and max value appeared in that field as well as the count of values and distinct values.
$books -> transform pathValues($) -> expand -> group by $key = {$.AttrName, $.AttrType} into {AttrName: $key.AttrName, AttrType: $key.AttrType, Min: min($[*].AttrValue), Max: max($[*].AttrValue), CountValues: count($[*].AttrValue), CountDistinct: count(distinct($[*].AttrValue))}; 

//Report the order (sorting) information for each field (attribute) in the given input
$books -> FieldSortInfo();

//Divide the following range into buckets 
//	--The max step size between two adjacent points in the same bucket is 5.0,
//  --Use a default comparison function (depends on the data type)
[1.0, 2.0, 5.0, 10.0,12.0, 100.0, 102.0, 103.0, 104.0, 200.0, 205.0] -> bucketize(5.0, null);


//Divide the following range into buckets 
//	--Compute the max step size automatically,
//  --Use a default comparison function (depends on the data type)
[1.0, 2.0, 5.0, 10.0,12.0, 100.0, 102.0, 103.0, 104.0, 200.0, 205.0] -> bucketize(null, null);


//Divide the following range into buckets 
//	--Compute the max step size automatically,
//  --Use the provided comparison function 
$diff = fn ($val1, $val2) ($val1 - $val2);
[1.0, 2.0, 103.0, 104.0, 5.0,  130.0, 10.0, 12.0, 100.0, 102.0,  205.0] -> sort by [$] -> bucketize(null, $diff);


//Compute a BloomFilter for the given array of values
[1,1,2,3,1,2,2,3,2,1,1,1,1] -> bloomFilter(null);

//Compute a BloomFilter for the given array of values
["xyz","xyz","test","val","xyz","test","test","test","xyz","xyz","xyz","xyz"] -> bloomFilter(null);

//Compute a BloomFilter for the given array of values only if the ratio of the distinct values to the total values <= 0.05
[1,1,2,3,1,2,2,3,2,1,1,1,1] -> bloomFilter(0.05);


//Jaql function that reports statistics about each field in a given file and stores the output in a delimited file
DataStatistics = fn($data, $partition_output_file, $ranges_output_file, $bucketize_step_size, $filter_distinct_threshold)(
	$r1 = $data -> FieldSortInfo(),
	$r2 = $data -> transform pathValues($) -> expand 
			-> group by $key = {$.AttrName, $.AttrType} 
				  into {AttrName: $key.AttrName, 
					AttrType: $key.AttrType, 
					CountValues: count($[*].AttrValue), 
					CountDistinct: count(distinct($[*].AttrValue)), 
					Ranges: bucketize($[*].AttrValue -> sort by [$], $bucketize_step_size, null), 
					BloomFilter: bloomFilter($[*].AttrValue, $filter_distinct_threshold)},
	$r3 = join $r1, $r2 where $r1.AttrName == $r2.AttrName into {$r2.*, $r1.SortingInfo},

	$r3 -> transform {$.AttrName, $.AttrType, $.CountValues, $.CountDistinct, $.SortingInfo, BloomFilter: $.BloomFilter[0]} 
		-> write(del($partition_output_file, {schema: schema{AttrName, AttrType, CountValues, CountDistinct, SortingInfo, BloomFilter}})),

	$r3 -> transform {$.AttrName, $.AttrType, $.Ranges} -> expand unroll $.Ranges 
		-> transform {$.AttrName, $.AttrType, $.Ranges.Min, $.Ranges.Max}
		-> write(del($ranges_output_file, {schema: schema{AttrName, AttrType, Min, Max}}))
);
$books ->DataStatistics('f1.txt', 'f2.txt', null, null);

	
	
	
//Given a file, loop over its splits and report the statistics for each split in a delimited file 
Statistics = fn($file_descriptor, $split_id, $split, $bucketize_step_size, $filter_distinct_threshold)(

	//1- Get the ordering (sorting) information
	$r1 = $split -> FieldSortInfo(),

	//2- Get the statistics for each attribute (e.g., min, max, datatype, count distinct values)
	$r2 = $split -> transform pathValues($) -> expand 
			-> group by $key = {$.AttrName, $.AttrType} 
				  into {AttrName: $key.AttrName, 
					AttrType: $key.AttrType, 
					CountValues: count($[*].AttrValue), 
					CountDistinct: count(distinct($[*].AttrValue)), 
					Ranges: bucketize($[*].AttrValue -> sort by [$], $bucketize_step_size, null), 
					BloomFilter: bloomFilter($[*].AttrValue, $filter_distinct_threshold)},

	//3- Join the results from Steps 1 and 2
	$r3 = join $r1, $r2 where $r1.AttrName == $r2.AttrName into {$r2.*, $r1.SortingInfo},

	//4- Merge the output in one stream
	$partition_level_output = [{Tag: "PartitionLevel", FileName: $file_descriptor.location, PartitionId: $split_id, RecordCount: $split -> count()}], 
	
	$attribute_level_output = $r3 -> transform {Tag: "AttributeLevel", FileName: $file_descriptor.location, PartitionId: $split_id, $.AttrName, $.AttrType, $.CountValues, $.CountDistinct, $.SortingInfo, BloomFilter: $.BloomFilter[0]}, 

	$range_level_output = $r3 -> transform {FileName: $file_descriptor.location, PartitionId: $split_id, $.AttrName, $.AttrType, $.Ranges} -> expand unroll $.Ranges 
						-> transform {Tag: "RangeLevel", $.FileName, $.PartitionId, $.AttrName, $.AttrType, $.Ranges.Min, $.Ranges.Max},

	merge($partition_level_output, $attribute_level_output, $range_level_output)						
);


CollectStatistics = fn($inputFile_descriptor, $output_dir, $bucketize_step_size, $filter_distinct_threshold)(

	//1- Collect the statistics for the given file
	$output = inputSplits($inputFile_descriptor) ->  transform {split: $} -> runningCombine(0, fn(counter,i) counter + 1, fn(counter,i) { i.*, counter }) 
				-> transform each $rec 
					Statistics($inputFile_descriptor, $rec.counter, readSplit($inputFile_descriptor, $rec.split), $bucketize_step_size, $filter_distinct_threshold) -> expand,
	
	$partition_level_output = $output_dir + "_partitionInfo",
	$attribute_level_output = $output_dir + "_attributeInfo",
	$range_level_output = $output_dir + "_rangeInfo",
	
	//2- Write the partition_level output					
	$output -> filter $.Tag == 'PartitionLevel' -> transform {$.FileName, $.PartitionId, $.RecordCount} 
		->write(del($partition_level_output, {schema: schema{FileName, PartitionId, RecordCount}})),

	//3- Write the attribute_level output					
	$output -> filter $.Tag == 'AttributeLevel' -> transform {$.FileName, $.PartitionId, $.AttrName, $.AttrType, $.CountValues, $.CountDistinct, $.SortingInfo, $.BloomFilter} 
		-> write(del($attribute_level_output, {schema: schema{FileName, PartitionId, AttrName, AttrType, CountValues, CountDistinct, SortingInfo, BloomFilter}})),

	//4- Write the range_level output
	$output -> filter $.Tag == 'RangeLevel' -> transform {$.FileName, $.PartitionId, $.AttrName, $.AttrType, $.Min, $.Max}
		-> write(del($range_level_output, {schema: schema{FileName, PartitionId, AttrName, AttrType, Min, Max}}))
);

$books -> write(hdfs('books'));

//CollectStatistics() takes the following parameters:
//     1-<input_file_descriptor>: The input file to coolect statistics about
//     2-<output_directory>: the directory where the output files (three files) will be written
//     3-<bucketize_step_size>: configuration parameter for bucketization (can be null)
//     4-<filter_distinct_threshold>: configuration parameter for BloomFilters (can be null)    
$f = hdfs('books');
CollectStatistics($f, '', null, 0.4);



	